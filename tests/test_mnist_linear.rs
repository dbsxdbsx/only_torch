/*
 * @Author       : è€è‘£
 * @Date         : 2025-12-22
 * @Description  : MNIST Linear é›†æˆæµ‹è¯•ï¼ˆMLP æ¶æ„ï¼‰
 *                 éªŒè¯ï¼šLinear Layer API + batch forward/backward + Adam ä¼˜åŒ–å™¨
 *                 ä½¿ç”¨ linear() æ„å»ºçº¯å…¨è¿æ¥ç½‘ç»œï¼Œå¯¹æ¯” test_mnist_cnn.rs çš„ CNN æ¶æ„
 */

use only_torch::data::MnistDataset;
use only_torch::nn::optimizer::{Adam, Optimizer};
use only_torch::nn::{Graph, GraphError, linear};
use only_torch::tensor::Tensor;
use only_torch::tensor_slice;
use std::time::Instant;

/// MNIST Linear é›†æˆæµ‹è¯•ï¼ˆMLP æ¶æ„ï¼‰
///
/// ä½¿ç”¨ Linear layer æ„å»º MLPï¼ŒéªŒè¯ Layer API çš„æ­£ç¡®æ€§å’Œæ˜“ç”¨æ€§
#[test]
fn test_mnist_linear() -> Result<(), GraphError> {
    let start_time = Instant::now();

    println!("\n{}", "=".repeat(60));
    println!("=== MNIST Linear é›†æˆæµ‹è¯•ï¼ˆMLP æ¶æ„ï¼‰===");
    println!("{}\n", "=".repeat(60));

    // ========== 1. åŠ è½½æ•°æ® ==========
    println!("[1/4] åŠ è½½ MNIST æ•°æ®é›†...");
    let load_start = Instant::now();

    let train_data = MnistDataset::train()
        .expect("åŠ è½½ MNIST è®­ç»ƒé›†å¤±è´¥")
        .flatten();
    let test_data = MnistDataset::test()
        .expect("åŠ è½½ MNIST æµ‹è¯•é›†å¤±è´¥")
        .flatten();

    println!(
        "  âœ“ è®­ç»ƒé›†: {} æ ·æœ¬ï¼Œæµ‹è¯•é›†: {} æ ·æœ¬ï¼Œè€—æ—¶ {:.2}s",
        train_data.len(),
        test_data.len(),
        load_start.elapsed().as_secs_f32()
    );

    // ========== 2. è®­ç»ƒé…ç½® ==========
    let batch_size = 512;
    let train_samples = 5000;
    let test_samples = 1000;
    let max_epochs = 15;
    let num_batches = train_samples / batch_size;
    let learning_rate = 0.008;
    let target_accuracy = 0.90;
    let consecutive_success_required = 2;

    println!("\n[2/4] è®­ç»ƒé…ç½®ï¼š");
    println!("  - Batch Size: {}", batch_size);
    println!(
        "  - è®­ç»ƒæ ·æœ¬: {} (å…± {} ä¸ª batch)",
        train_samples, num_batches
    );
    println!("  - æµ‹è¯•æ ·æœ¬: {}", test_samples);
    println!("  - æœ€å¤§ Epochs: {}", max_epochs);
    println!("  - å­¦ä¹ ç‡: {}", learning_rate);
    println!("  - ç›®æ ‡å‡†ç¡®ç‡: {:.0}%", target_accuracy * 100.0);

    // ========== 3. æ„å»ºç½‘ç»œï¼ˆä½¿ç”¨ Layer APIï¼‰==========
    println!("\n[3/4] ä½¿ç”¨ linear() æ„å»º MLP: 784 -> 128 (Softplus) -> 10...");

    let mut graph = Graph::new_with_seed(42);

    // è¾“å…¥/æ ‡ç­¾èŠ‚ç‚¹
    let x = graph.new_input_node(&[batch_size, 784], Some("x"))?;
    let y = graph.new_input_node(&[batch_size, 10], Some("y"))?;

    // ========== ä½¿ç”¨ linear() æ„å»ºç½‘ç»œï¼ˆBatch-Firstï¼‰==========
    // éšè—å±‚: 784 -> 128ï¼Œä½¿ç”¨ Softplus æ¿€æ´»ï¼ˆæ¯” Sigmoid æ›´é€‚åˆéšè—å±‚ï¼‰
    let fc1 = linear(&mut graph, x, 784, 128, batch_size, Some("fc1"))?;
    let a1 = graph.new_softplus_node(fc1.output, Some("fc1_act"))?;

    // è¾“å‡ºå±‚: 128 -> 10
    let fc2 = linear(&mut graph, a1, 128, 10, batch_size, Some("fc2"))?;
    let logits = fc2.output;

    // æŸå¤±å‡½æ•°
    let loss = graph.new_softmax_cross_entropy_node(logits, y, Some("loss"))?;

    println!("  âœ“ ç½‘ç»œæ„å»ºå®Œæˆï¼š784 -> 128 -> 10ï¼ˆ2å±‚ MLPï¼‰");
    println!("  âœ“ å‚æ•°èŠ‚ç‚¹ï¼šfc1_W, fc1_b, fc2_W, fc2_b");

    // ========== 4. è®­ç»ƒå¾ªç¯ ==========
    println!("\n[4/4] å¼€å§‹è®­ç»ƒ...\n");

    let mut optimizer = Adam::new(&graph, learning_rate, 0.9, 0.999, 1e-8)?;

    // è®¾ç½® ones çŸ©é˜µï¼ˆç”¨äº bias å¹¿æ’­ï¼‰
    let ones_tensor = Tensor::ones(&[batch_size, 1]);
    graph.set_node_value(fc1.ones, Some(&ones_tensor))?;
    graph.set_node_value(fc2.ones, Some(&ones_tensor))?;

    let all_train_images = train_data.images();
    let all_train_labels = train_data.labels();
    let all_test_images = test_data.images();
    let all_test_labels = test_data.labels();

    let mut consecutive_success_count = 0;
    let mut test_passed = false;
    let test_batches = test_samples / batch_size;

    for epoch in 0..max_epochs {
        let epoch_start = Instant::now();
        let mut epoch_loss_sum = 0.0;

        // è®­ç»ƒ
        for batch_idx in 0..num_batches {
            let start = batch_idx * batch_size;
            let end = start + batch_size;

            let batch_images = tensor_slice!(all_train_images, start..end, ..);
            let batch_labels = tensor_slice!(all_train_labels, start..end, ..);

            graph.set_node_value(x, Some(&batch_images))?;
            graph.set_node_value(y, Some(&batch_labels))?;

            optimizer.one_step_batch(&mut graph, loss)?;
            optimizer.update_batch(&mut graph)?;

            let loss_val = graph.get_node_value(loss)?.unwrap()[[0, 0]];
            epoch_loss_sum += loss_val;
        }

        let epoch_avg_loss = epoch_loss_sum / num_batches as f32;

        // æµ‹è¯•ç²¾åº¦
        graph.set_eval_mode();
        let mut correct = 0;

        for batch_idx in 0..test_batches {
            let start = batch_idx * batch_size;
            let end = start + batch_size;

            let batch_images = tensor_slice!(all_test_images, start..end, ..);
            let batch_labels = tensor_slice!(all_test_labels, start..end, ..);

            graph.set_node_value(x, Some(&batch_images))?;
            graph.set_node_value(y, Some(&batch_labels))?;

            graph.forward_batch(loss)?;

            let predictions = graph.get_node_value(logits)?.unwrap();

            for i in 0..batch_size {
                let mut pred_class = 0;
                let mut max_val = f32::NEG_INFINITY;
                for j in 0..10 {
                    let val = predictions[[i, j]];
                    if val > max_val {
                        max_val = val;
                        pred_class = j;
                    }
                }

                let mut true_class = 0;
                for j in 0..10 {
                    if batch_labels[[i, j]] > 0.5 {
                        true_class = j;
                        break;
                    }
                }

                if pred_class == true_class {
                    correct += 1;
                }
            }
        }

        graph.set_train_mode();

        let total_tested = test_batches * batch_size;
        let accuracy = correct as f32 / total_tested as f32;

        println!(
            "Epoch {:2}/{}: loss = {:.4}, å‡†ç¡®ç‡ = {:.1}% ({}/{}), è€—æ—¶ {:.2}s",
            epoch + 1,
            max_epochs,
            epoch_avg_loss,
            accuracy * 100.0,
            correct,
            total_tested,
            epoch_start.elapsed().as_secs_f32()
        );

        if accuracy >= target_accuracy {
            consecutive_success_count += 1;
            if consecutive_success_count >= consecutive_success_required {
                test_passed = true;
                println!(
                    "\nğŸ‰ è¿ç»­ {} æ¬¡è¾¾åˆ° {:.0}% ä»¥ä¸Šå‡†ç¡®ç‡ï¼",
                    consecutive_success_required,
                    target_accuracy * 100.0
                );
                break;
            }
        } else {
            consecutive_success_count = 0;
        }
    }

    let total_duration = start_time.elapsed();
    println!("\næ€»è€—æ—¶: {:.2}s", total_duration.as_secs_f32());

    if test_passed {
        println!("\n{}", "=".repeat(60));
        println!("âœ… MNIST Linear æµ‹è¯•é€šè¿‡ï¼");
        println!("{}\n", "=".repeat(60));
        Ok(())
    } else {
        println!("\n{}", "=".repeat(60));
        println!(
            "âŒ æµ‹è¯•å¤±è´¥ï¼šåœ¨ {} ä¸ª epoch å†…æœªèƒ½è¿ç»­ {} æ¬¡è¾¾åˆ° {:.0}% å‡†ç¡®ç‡",
            max_epochs,
            consecutive_success_required,
            target_accuracy * 100.0
        );
        println!("{}\n", "=".repeat(60));
        Err(GraphError::ComputationError(format!(
            "MNIST Linear æµ‹è¯•å¤±è´¥ï¼šåœ¨ {} ä¸ª epoch å†…æœªèƒ½è¿ç»­ {} æ¬¡è¾¾åˆ° {:.0}% å‡†ç¡®ç‡",
            max_epochs,
            consecutive_success_required,
            target_accuracy * 100.0
        )))
    }
}
