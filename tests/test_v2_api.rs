/*
 * V2 API é›†æˆæµ‹è¯•
 *
 * æµ‹è¯• GraphHandle + Var çš„åŸºæœ¬åŠŸèƒ½ï¼š
 * - åˆ›å»ºèŠ‚ç‚¹
 * - é“¾å¼è°ƒç”¨
 * - ç®—å­é‡è½½
 * - å‰å‘/åå‘ä¼ æ’­
 */

use only_torch::nn::{GraphHandle, Init, VarActivationOps, VarLossOps, VarMatrixOps};
use only_torch::tensor::Tensor;

/// æµ‹è¯•åŸºæœ¬çš„ V2 API åˆ›å»ºå’Œå‰å‘ä¼ æ’­
#[test]
fn test_v2_basic_forward() {
    let graph = GraphHandle::new();

    // åˆ›å»ºè¾“å…¥
    let x = graph
        .input(&Tensor::new(&[1.0, 2.0, 3.0, 4.0], &[1, 4]))
        .unwrap();

    // é“¾å¼æ¿€æ´»å‡½æ•°
    let h = x.relu();

    // å‰å‘ä¼ æ’­
    h.forward().unwrap();

    // éªŒè¯ç»“æœ
    let result = h.value().unwrap().unwrap();
    assert_eq!(result.shape(), &[1, 4]);
}

/// æµ‹è¯•ç®—å­é‡è½½ï¼ˆåŠ æ³•ï¼‰
#[test]
fn test_v2_operator_add() {
    let graph = GraphHandle::new();

    let a = graph.input(&Tensor::new(&[1.0, 2.0], &[1, 2])).unwrap();
    let b = graph.input(&Tensor::new(&[3.0, 4.0], &[1, 2])).unwrap();

    // ä½¿ç”¨ç®—å­é‡è½½
    let c = &a + &b;

    c.forward().unwrap();

    let result = c.value().unwrap().unwrap();
    let data = result.data_as_slice();
    assert_eq!(data, &[4.0, 6.0]);
}

/// æµ‹è¯•ç®—å­é‡è½½ï¼ˆå‡æ³•ï¼‰
#[test]
fn test_v2_operator_sub() {
    let graph = GraphHandle::new();

    let a = graph.input(&Tensor::new(&[5.0, 6.0], &[1, 2])).unwrap();
    let b = graph.input(&Tensor::new(&[1.0, 2.0], &[1, 2])).unwrap();

    // ä½¿ç”¨ç®—å­é‡è½½
    let c = &a - &b;

    c.forward().unwrap();

    let result = c.value().unwrap().unwrap();
    let data = result.data_as_slice();
    assert_eq!(data, &[4.0, 4.0]);
}

/// æµ‹è¯•ç®—å­é‡è½½ï¼ˆä¹˜æ³• - å…ƒç´ çº§ï¼‰
#[test]
fn test_v2_operator_mul() {
    let graph = GraphHandle::new();

    let a = graph.input(&Tensor::new(&[2.0, 3.0], &[1, 2])).unwrap();
    let b = graph.input(&Tensor::new(&[4.0, 5.0], &[1, 2])).unwrap();

    // ä½¿ç”¨ç®—å­é‡è½½
    let c = &a * &b;

    c.forward().unwrap();

    let result = c.value().unwrap().unwrap();
    let data = result.data_as_slice();
    assert_eq!(data, &[8.0, 15.0]);
}

/// æµ‹è¯•é“¾å¼è°ƒç”¨
#[test]
fn test_v2_chain_calls() {
    let graph = GraphHandle::new();

    let x = graph
        .input(&Tensor::new(&[-1.0, 2.0, -3.0, 4.0], &[1, 4]))
        .unwrap();

    // é“¾å¼è°ƒç”¨ï¼šReLU -> Sigmoid
    let y = x.relu().sigmoid();

    y.forward().unwrap();

    let result = y.value().unwrap().unwrap();
    assert_eq!(result.shape(), &[1, 4]);

    // ReLU åï¼š[0, 2, 0, 4]
    // Sigmoid åï¼šçº¦ [0.5, 0.88, 0.5, 0.98]
    let data = result.data_as_slice();
    assert!((data[0] - 0.5).abs() < 0.01); // sigmoid(0) = 0.5
    assert!(data[1] > 0.8); // sigmoid(2) â‰ˆ 0.88
}

/// æµ‹è¯•å‚æ•°åˆå§‹åŒ–
#[test]
fn test_v2_parameter_init() {
    let graph = GraphHandle::new();

    // ä½¿ç”¨ Xavier åˆå§‹åŒ–åˆ›å»ºå‚æ•°
    let w = graph.parameter(&[10, 5], Init::Xavier, "weight").unwrap();

    let val = w.value().unwrap().unwrap();
    assert_eq!(val.shape(), &[10, 5]);

    // éªŒè¯åˆå§‹åŒ–çš„ç»Ÿè®¡ç‰¹æ€§
    let data = val.data_as_slice();
    let mean: f32 = data.iter().sum::<f32>() / data.len() as f32;
    assert!(mean.abs() < 0.1); // å‡å€¼æ¥è¿‘ 0
}

/// æµ‹è¯• MSE Loss å’Œåå‘ä¼ æ’­
#[test]
fn test_v2_mse_backward() {
    let graph = GraphHandle::new();

    // ç®€å•çº¿æ€§æ¨¡å‹ï¼šy = w * x
    let x = graph.input(&Tensor::new(&[1.0, 2.0], &[1, 2])).unwrap();
    let target = graph.input(&Tensor::new(&[2.0, 4.0], &[1, 2])).unwrap();

    // è®¡ç®— MSE loss
    let loss = x.mse_loss(&target).unwrap();

    // åå‘ä¼ æ’­
    let loss_val = loss.backward().unwrap();

    // éªŒè¯ loss å€¼
    // MSE = mean((x - target)^2) = mean((1-2)^2 + (2-4)^2) = mean(1 + 4) = 2.5
    assert!((loss_val - 2.5).abs() < 0.01);
}

/// æµ‹è¯• detach åŠŸèƒ½
#[test]
fn test_v2_detach() {
    let graph = GraphHandle::new();

    let x = graph.input(&Tensor::new(&[1.0, 2.0], &[1, 2])).unwrap();
    let y = x.relu();

    // Detach åçš„èŠ‚ç‚¹ä¸å‚ä¸æ¢¯åº¦è®¡ç®—
    let z = y.detach().unwrap();

    // éªŒè¯ detach è¿”å›çš„æ˜¯åŒä¸€ä¸ªèŠ‚ç‚¹
    assert_eq!(y.node_id(), z.node_id());
}

/// æµ‹è¯•çŸ©é˜µä¹˜æ³•
#[test]
fn test_v2_matmul() {
    let graph = GraphHandle::new();

    // [1, 2] @ [[1], [2]] = [5]
    let a = graph.input(&Tensor::new(&[1.0, 2.0], &[1, 2])).unwrap();
    let b = graph.input(&Tensor::new(&[1.0, 2.0], &[2, 1])).unwrap();

    let c = a.matmul(&b).unwrap();
    c.forward().unwrap();

    let result = c.value().unwrap().unwrap();
    assert_eq!(result.shape(), &[1, 1]);

    let val = result.get_data_number().unwrap();
    assert!((val - 5.0).abs() < 0.001);
}

/// æµ‹è¯• GraphHandle çš„ Clone è¯­ä¹‰
#[test]
fn test_v2_graph_clone() {
    let graph1 = GraphHandle::new();
    let graph2 = graph1.clone();

    // åœ¨ graph1 ä¸Šåˆ›å»ºèŠ‚ç‚¹
    let x = graph1.input(&Tensor::new(&[1.0], &[1, 1])).unwrap();

    // graph2 åº”è¯¥èƒ½çœ‹åˆ°è¿™ä¸ªèŠ‚ç‚¹ï¼ˆå› ä¸ºå®ƒä»¬å…±äº«åŒä¸€ä¸ª GraphInnerï¼‰
    let node_count = graph2.inner().nodes_count();
    assert!(node_count >= 1);
}

/// æµ‹è¯•è´Ÿå·è¿ç®—ç¬¦
#[test]
fn test_v2_operator_neg() {
    let graph = GraphHandle::new();

    let a = graph
        .input(&Tensor::new(&[1.0, -2.0, 3.0], &[1, 3]))
        .unwrap();
    let b = -&a;

    b.forward().unwrap();

    let result = b.value().unwrap().unwrap();
    let data = result.data_as_slice();
    assert_eq!(data, &[-1.0, 2.0, -3.0]);
}

// ==================== XOR å®Œæ•´è®­ç»ƒæµ‹è¯• ====================

/// XORé—®é¢˜ - ä½¿ç”¨ V2 API çš„å®Œæ•´è®­ç»ƒæµç¨‹
///
/// ç½‘ç»œç»“æ„ï¼šInput(2) -> Hidden(4, Tanh) -> Output(1) -> PerceptionLoss
/// è¿™æ˜¯ Phase 1b çš„éªŒæ”¶æµ‹è¯•ï¼Œè¯æ˜ V2 API èƒ½å®Œæˆç«¯åˆ°ç«¯è®­ç»ƒ
#[test]
fn test_v2_xor_training() {
    let start_time = std::time::Instant::now();

    // ========== åˆ›å»ºè®¡ç®—å›¾ ==========
    let graph = GraphHandle::new();

    // è¾“å…¥å˜é‡ï¼ˆPlan Aï¼šå»ºå›¾ä¸€æ¬¡ï¼Œè®­ç»ƒå¾ªç¯ä¸­é€šè¿‡ set_value å–‚æ–°æ•°æ®ï¼‰
    let x = graph.zeros(&[2, 1]).unwrap();
    let label = graph.zeros(&[1, 1]).unwrap();

    // éšè—å±‚å‚æ•°ï¼ˆä½¿ç”¨ç§å­åˆå§‹åŒ–ç¡®ä¿å¯é‡å¤ï¼‰
    let w1 = graph.parameter_seeded(&[4, 2], "w1", 42).unwrap();
    let b1 = graph.parameter_seeded(&[4, 1], "b1", 43).unwrap();

    // è¾“å‡ºå±‚å‚æ•°
    let w2 = graph.parameter_seeded(&[1, 4], "w2", 44).unwrap();
    let b2 = graph.parameter_seeded(&[1, 1], "b2", 45).unwrap();

    // ========== æ„å»ºç½‘ç»œï¼ˆä½¿ç”¨ V2 é“¾å¼è°ƒç”¨å’Œç®—å­é‡è½½ï¼‰==========
    // éšè—å±‚: h = tanh(w1 @ x + b1)
    let wx1 = w1.matmul(&x).unwrap();
    let z1 = &wx1 + &b1;
    let h = z1.tanh();

    // è¾“å‡ºå±‚: output = w2 @ h + b2
    let wx2 = w2.matmul(&h).unwrap();
    let output = &wx2 + &b2;

    // é¢„æµ‹: stepå‡½æ•°
    let predict = output.step();

    // æŸå¤±: PerceptionLoss(label * output)
    let loss_input = label.matmul(&output).unwrap();
    let loss = loss_input.perception_loss();

    // ========== è®­ç»ƒæ•°æ® ==========
    let inputs = vec![
        Tensor::new(&[0.0, 0.0], &[2, 1]),
        Tensor::new(&[0.0, 1.0], &[2, 1]),
        Tensor::new(&[1.0, 0.0], &[2, 1]),
        Tensor::new(&[1.0, 1.0], &[2, 1]),
    ];
    let labels = vec![
        Tensor::new(&[-1.0], &[1, 1]), // XOR(0,0) = 0 -> -1
        Tensor::new(&[1.0], &[1, 1]),  // XOR(0,1) = 1 -> +1
        Tensor::new(&[1.0], &[1, 1]),  // XOR(1,0) = 1 -> +1
        Tensor::new(&[-1.0], &[1, 1]), // XOR(1,1) = 0 -> -1
    ];

    // å­¦ä¹ ç‡
    let learning_rate = 1.0;
    let max_epochs = 500;
    let target_accuracy = 1.0;
    let consecutive_success_required = 10;
    let mut consecutive_success_count = 0;
    let mut test_passed = false;

    // ========== è®­ç»ƒå¾ªç¯ ==========
    for epoch in 0..max_epochs {
        // éå†æ‰€æœ‰æ ·æœ¬
        for (input, lbl) in inputs.iter().zip(labels.iter()) {
            // è®¾ç½®è¾“å…¥ï¼ˆä½¿ç”¨ V2 APIï¼‰
            x.set_value(input).unwrap();
            label.set_value(lbl).unwrap();

            // å‰å‘ + åå‘ä¼ æ’­ï¼ˆV2 çš„ ensure-forward è¯­ä¹‰ï¼‰
            loss.backward().unwrap();

            // æ‰‹åŠ¨æ›´æ–°å‚æ•°ï¼ˆPhase 2 ä¼šæœ‰ Optimizerï¼‰
            // w1 -= lr * grad
            let w1_val = w1.value().unwrap().unwrap();
            let w1_grad = w1.grad().unwrap().unwrap();
            w1.set_value(&(&w1_val - learning_rate * &w1_grad)).unwrap();

            // b1 -= lr * grad
            let b1_val = b1.value().unwrap().unwrap();
            let b1_grad = b1.grad().unwrap().unwrap();
            b1.set_value(&(&b1_val - learning_rate * &b1_grad)).unwrap();

            // w2 -= lr * grad
            let w2_val = w2.value().unwrap().unwrap();
            let w2_grad = w2.grad().unwrap().unwrap();
            w2.set_value(&(&w2_val - learning_rate * &w2_grad)).unwrap();

            // b2 -= lr * grad
            let b2_val = b2.value().unwrap().unwrap();
            let b2_grad = b2.grad().unwrap().unwrap();
            b2.set_value(&(&b2_val - learning_rate * &b2_grad)).unwrap();

            // æ¸…é™¤æ¢¯åº¦
            graph.zero_grad().unwrap();
        }

        // è¯„ä¼°å‡†ç¡®ç‡
        let mut correct = 0;
        for (input, lbl) in inputs.iter().zip(labels.iter()) {
            x.set_value(input).unwrap();
            predict.forward().unwrap();

            let pred = predict.value().unwrap().unwrap();
            let pred_val = pred.get_data_number().unwrap();
            // é¢„æµ‹çš„ 0/1 è½¬æ¢ä¸º -1/+1
            let pred_label = pred_val * 2.0 - 1.0;

            let expected = lbl.get_data_number().unwrap();
            if pred_label == expected {
                correct += 1;
            }
        }

        let accuracy = correct as f32 / 4.0;

        // æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if accuracy >= target_accuracy {
            consecutive_success_count += 1;
            if consecutive_success_count >= consecutive_success_required {
                test_passed = true;
                println!(
                    "ğŸ‰ V2 XOR æµ‹è¯•é€šè¿‡ï¼ç¬¬ {} è½®ï¼Œè¿ç»­ {} æ¬¡è¾¾åˆ° 100% å‡†ç¡®ç‡",
                    epoch + 1,
                    consecutive_success_required
                );
                break;
            }
        } else {
            consecutive_success_count = 0;
        }
    }

    let duration = start_time.elapsed();
    println!("V2 XOR è®­ç»ƒè€—æ—¶: {duration:.2?}");

    assert!(
        test_passed,
        "V2 XOR æµ‹è¯•å¤±è´¥ï¼šæœªèƒ½åœ¨ {} è½®å†…è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡",
        max_epochs
    );
}
