/*
 * @Author       : è€è‘£
 * @Date         : 2025-12-22
 * @Description  : California Housing æˆ¿ä»·å›å½’é›†æˆæµ‹è¯•
 *
 * ä½¿ç”¨çœŸå®æ•°æ®é›†éªŒè¯ MSELoss + MLP å›å½’ä»»åŠ¡
 * ç±»ä¼¼äº MNIST åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„åœ°ä½
 *
 * é‡‡ç”¨ Layer API + Batch æ¨¡å¼ï¼Œä¸ MNIST æµ‹è¯•é£æ ¼ä¸€è‡´
 */

use approx::assert_abs_diff_eq;
use only_torch::data::CaliforniaHousingDataset;
use only_torch::nn::optimizer::{Adam, Optimizer};
use only_torch::nn::{Graph, GraphError, linear};
use only_torch::tensor::Tensor;
use std::fs;
use std::time::Instant;

/// California Housing æˆ¿ä»·å›å½’ï¼ˆLayer API + Batch ç‰ˆæœ¬ï¼‰
///
/// ç½‘ç»œç»“æ„ï¼šInput(8) â†’ Linear(128, Softplus) â†’ Linear(64, Softplus) â†’ Linear(32, Softplus) â†’ Linear(1)
/// ç›®æ ‡ï¼šRÂ² â‰¥ 0.70 (70%)
///
/// è®¾è®¡ç‰¹ç‚¹ï¼š
/// 1. ä½¿ç”¨ `linear()` Layer API æ„å»ºç½‘ç»œï¼ˆç®€æ´ã€å¯ç»´æŠ¤ï¼‰
/// 2. çœŸæ­£çš„ batch `è®­ç»ƒï¼ˆbatch_size=256ï¼Œé«˜æ•ˆ`ï¼‰
/// 3. Softplus æ¿€æ´»ï¼šå¹³æ»‘æ¢¯åº¦ï¼Œæ— æ­»ç¥ç»å…ƒé—®é¢˜
/// 4. Xavier åˆå§‹åŒ–ï¼šé€‚é… Softplus
///
/// æ³¨ï¼šCalifornia Housing + MLP åœ¨ batch æ¨¡å¼ä¸‹ 70% RÂ² æ˜¯åˆç†ç›®æ ‡
/// scikit-learn `MLPRegressor` åœ¨æ­¤æ•°æ®é›†ä¸Šä¹Ÿè¾¾åˆ°ç±»ä¼¼æ°´å¹³
#[test]
fn test_california_housing_regression() -> Result<(), GraphError> {
    let start_time = Instant::now();

    println!("\n{}", "=".repeat(60));
    println!("=== California Housing æˆ¿ä»·å›å½’æµ‹è¯•ï¼ˆLayer API + Batchï¼‰===");
    println!("{}\n", "=".repeat(60));

    // ========== 1. åŠ è½½æ•°æ® ==========
    println!("[1/4] åŠ è½½ California Housing æ•°æ®é›†...");
    let load_start = Instant::now();

    let dataset = CaliforniaHousingDataset::load_default()
        .expect("åŠ è½½ California Housing æ•°æ®é›†å¤±è´¥")
        .standardize();

    let (train_data, test_data) = dataset
        .train_test_split(0.2, Some(42))
        .expect("åˆ’åˆ†æ•°æ®é›†å¤±è´¥");

    println!(
        "  âœ“ è®­ç»ƒé›†: {} æ ·æœ¬ï¼Œæµ‹è¯•é›†: {} æ ·æœ¬ï¼Œè€—æ—¶ {:.2}s",
        train_data.len(),
        test_data.len(),
        load_start.elapsed().as_secs_f32()
    );

    // ========== 2. è®­ç»ƒé…ç½® ==========
    // Batch æ¨¡å¼ï¼šä¸ MNIST æµ‹è¯•é£æ ¼ä¸€è‡´ï¼Œä½¿ç”¨è¾ƒå¤§ batch å’Œç›¸åº”å­¦ä¹ ç‡
    // æ³¨ï¼šbatch MSE æ¢¯åº¦ä¼šè¢« batch_size å¹³å‡ï¼Œéœ€è¦æ›´é«˜å­¦ä¹ ç‡è¡¥å¿
    let batch_size = 256;
    let train_samples = 16000; // ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®
    let test_samples = 1024; // æµ‹è¯•æ ·æœ¬æ•°ï¼ˆbatch æ•´é™¤ï¼‰
    let max_epochs = 30;
    let num_batches = train_samples / batch_size; // 62 batches/epoch
    let learning_rate = 0.01; // batch æ¨¡å¼éœ€è¦æ›´é«˜å­¦ä¹ ç‡
    let target_r2 = 0.70; // California Housing + MLP çš„åˆç†ç›®æ ‡

    println!("\n[2/4] è®­ç»ƒé…ç½®ï¼š");
    println!("  - Batch Size: {batch_size}");
    println!("  - è®­ç»ƒæ ·æœ¬: {train_samples} (å…± {num_batches} ä¸ª batch)");
    println!("  - æµ‹è¯•æ ·æœ¬: {test_samples}");
    println!("  - æœ€å¤§ Epochs: {max_epochs}");
    println!("  - å­¦ä¹ ç‡: {learning_rate}");
    println!("  - ç›®æ ‡ RÂ²: {:.0}%", target_r2 * 100.0);

    // ========== 3. æ„å»ºç½‘ç»œï¼ˆä½¿ç”¨ Layer APIï¼‰==========
    println!("\n[3/4] ä½¿ç”¨ linear() æ„å»º MLP: 8 -> 128 -> 64 -> 32 -> 1...");

    let mut graph = Graph::new_with_seed(42);

    // è¾“å…¥/æ ‡ç­¾èŠ‚ç‚¹ï¼ˆbatch ç»´åº¦ï¼‰
    let x = graph.new_input_node(&[batch_size, 8], Some("x"))?;
    let y_true = graph.new_input_node(&[batch_size, 1], Some("y_true"))?;

    // éšè—å±‚1: 8 -> 128 (Softplus)
    let fc1 = linear(&mut graph, x, 8, 128, batch_size, Some("fc1"))?;
    let a1 = graph.new_softplus_node(fc1.output, Some("fc1_act"))?;

    // éšè—å±‚2: 128 -> 64 (Softplus)
    let fc2 = linear(&mut graph, a1, 128, 64, batch_size, Some("fc2"))?;
    let a2 = graph.new_softplus_node(fc2.output, Some("fc2_act"))?;

    // éšè—å±‚3: 64 -> 32 (Softplus)
    let fc3 = linear(&mut graph, a2, 64, 32, batch_size, Some("fc3"))?;
    let a3 = graph.new_softplus_node(fc3.output, Some("fc3_act"))?;

    // è¾“å‡ºå±‚: 32 -> 1 (çº¿æ€§)
    let fc4 = linear(&mut graph, a3, 32, 1, batch_size, Some("fc4"))?;
    let y_pred = fc4.output;

    // æŸå¤±å‡½æ•°
    let loss = graph.new_mse_loss_node(y_pred, y_true, Some("loss"))?;

    println!("  âœ“ ç½‘ç»œæ„å»ºå®Œæˆï¼š8 -> 128 -> 64 -> 32 -> 1ï¼ˆ4 å±‚ MLPï¼‰");
    println!("  âœ“ å‚æ•°èŠ‚ç‚¹ï¼šfc1_W/b, fc2_W/b, fc3_W/b, fc4_W/b");

    // ä¿å­˜ç½‘ç»œç»“æ„å¯è§†åŒ–ï¼ˆè®­ç»ƒå‰ï¼‰
    let output_dir = "tests/outputs";
    fs::create_dir_all(output_dir).ok();
    graph.save_visualization_grouped(format!("{output_dir}/california_housing"), None)?;
    graph.save_summary(format!("{output_dir}/california_housing_summary.md"))?;
    println!("  âœ“ ç½‘ç»œç»“æ„å·²ä¿å­˜: {output_dir}/california_housing.png");

    // ========== Xavier åˆå§‹åŒ– ==========
    let xavier_init = |fan_in: usize, fan_out: usize, seed: u64| -> Tensor {
        let std = (2.0 / (fan_in + fan_out) as f32).sqrt();
        Tensor::normal_seeded(0.0, std, &[fan_in, fan_out], seed)
    };

    graph.set_node_value(fc1.weights, Some(&xavier_init(8, 128, 42)))?;
    graph.set_node_value(fc2.weights, Some(&xavier_init(128, 64, 43)))?;
    graph.set_node_value(fc3.weights, Some(&xavier_init(64, 32, 44)))?;
    graph.set_node_value(fc4.weights, Some(&xavier_init(32, 1, 45)))?;

    // bias åˆå§‹åŒ–ä¸º 0
    graph.set_node_value(fc1.bias, Some(&Tensor::zeros(&[1, 128])))?;
    graph.set_node_value(fc2.bias, Some(&Tensor::zeros(&[1, 64])))?;
    graph.set_node_value(fc3.bias, Some(&Tensor::zeros(&[1, 32])))?;
    graph.set_node_value(fc4.bias, Some(&Tensor::zeros(&[1, 1])))?;

    // ========== 4. è®­ç»ƒå¾ªç¯ ==========
    println!("\n[4/4] å¼€å§‹è®­ç»ƒ...\n");

    let mut optimizer = Adam::new(&graph, learning_rate, 0.9, 0.999, 1e-8)?;

    // é¢„å…ˆæ„å»º batch æ•°æ®
    let train_batches = build_batches(&train_data, batch_size, num_batches);
    let test_batches_data = build_batches(&test_data, batch_size, test_samples / batch_size);

    let mut best_r2 = f32::NEG_INFINITY;

    for epoch in 0..max_epochs {
        let epoch_start = Instant::now();
        let mut epoch_loss_sum = 0.0;

        // è®­ç»ƒ
        for (batch_x, batch_y) in &train_batches {
            graph.set_node_value(x, Some(batch_x))?;
            graph.set_node_value(y_true, Some(batch_y))?;

            graph.zero_grad()?;
            graph.forward(loss)?;
            let loss_val = graph.backward(loss)?; // backward è¿”å› loss å€¼
            optimizer.step(&mut graph)?;

            epoch_loss_sum += loss_val;
        }

        let epoch_avg_loss = epoch_loss_sum / num_batches as f32;

        // æµ‹è¯•é›†è¯„ä¼°ï¼ˆè®¡ç®— RÂ²ï¼‰
        graph.set_eval_mode();
        let mut predictions: Vec<f32> = Vec::with_capacity(test_samples);
        let mut actuals: Vec<f32> = Vec::with_capacity(test_samples);

        for (batch_x, batch_y) in &test_batches_data {
            graph.set_node_value(x, Some(batch_x))?;
            graph.set_node_value(y_true, Some(batch_y))?;

            graph.forward(y_pred)?;

            let pred_tensor = graph.get_node_value(y_pred)?.unwrap();
            for i in 0..batch_size {
                predictions.push(pred_tensor[[i, 0]]);
                actuals.push(batch_y[[i, 0]]);
            }
        }

        graph.set_train_mode();

        // è®¡ç®— RÂ²
        let r2_score = compute_r2(&predictions, &actuals);
        best_r2 = best_r2.max(r2_score);

        println!(
            "Epoch {:2}/{}: loss = {:.4}, RÂ² = {:.2}% ({:.4}), è€—æ—¶ {:.2}s",
            epoch + 1,
            max_epochs,
            epoch_avg_loss,
            r2_score * 100.0,
            r2_score,
            epoch_start.elapsed().as_secs_f32()
        );

        // æå‰ç»“æŸæ¡ä»¶
        if r2_score >= target_r2 {
            println!("\nğŸ‰ è¾¾åˆ°ç›®æ ‡ RÂ² â‰¥ {:.0}%ï¼", target_r2 * 100.0);
            break;
        }
    }

    let total_duration = start_time.elapsed();
    println!("\næ€»è€—æ—¶: {:.2}s", total_duration.as_secs_f32());

    // æ‰“å°æ¨¡å‹æ‘˜è¦
    println!("\næ¨¡å‹æ‘˜è¦ï¼š");
    graph.summary();

    // æœ€ç»ˆéªŒè¯
    println!("\n{}", "=".repeat(60));
    println!("ç»“æœéªŒè¯:");
    println!("  æœ€ä½³ RÂ²: {:.4} ({:.1}%)", best_r2, best_r2 * 100.0);

    assert!(
        best_r2 >= target_r2,
        "RÂ² åˆ†æ•°åº” â‰¥ {:.0}%ï¼Œå®é™…: {:.2}%",
        target_r2 * 100.0,
        best_r2 * 100.0
    );

    println!("\nâœ… California Housing å›å½’æµ‹è¯•é€šè¿‡ï¼");
    println!("   æ¨¡å‹è§£é‡Šäº† {:.1}% çš„ç›®æ ‡å˜é‡æ–¹å·®", best_r2 * 100.0);
    println!("{}\n", "=".repeat(60));

    if best_r2 >= 0.85 {
        println!("   ğŸ‰ è¾¾åˆ°ä¼˜ç§€æ°´å¹³ (RÂ² â‰¥ 85%)ï¼");
    }

    Ok(())
}

/// æ„å»º batch æ•°æ®
fn build_batches(
    data: &CaliforniaHousingDataset,
    batch_size: usize,
    num_batches: usize,
) -> Vec<(Tensor, Tensor)> {
    let mut batches = Vec::with_capacity(num_batches);

    for batch_idx in 0..num_batches {
        let mut x_data = Vec::with_capacity(batch_size * 8);
        let mut y_data = Vec::with_capacity(batch_size);

        for i in 0..batch_size {
            let idx = batch_idx * batch_size + i;
            if idx < data.len() {
                let (features, target) = data.get(idx).unwrap();
                // ä½¿ç”¨ flatten_view() è·å–æ•°æ®è§†å›¾å¹¶å¤åˆ¶
                x_data.extend(features.flatten_view().iter().copied());
                y_data.push(target[[0]]);
            }
        }

        let x_tensor = Tensor::new(&x_data, &[batch_size, 8]);
        let y_tensor = Tensor::new(&y_data, &[batch_size, 1]);
        batches.push((x_tensor, y_tensor));
    }

    batches
}

/// è®¡ç®— RÂ² åˆ†æ•°
fn compute_r2(predictions: &[f32], actuals: &[f32]) -> f32 {
    let mean_actual: f32 = actuals.iter().sum::<f32>() / actuals.len() as f32;

    let ss_res: f32 = predictions
        .iter()
        .zip(actuals.iter())
        .map(|(pred, actual)| (actual - pred).powi(2))
        .sum();

    let ss_tot: f32 = actuals
        .iter()
        .map(|actual| (actual - mean_actual).powi(2))
        .sum();

    1.0 - (ss_res / ss_tot)
}

/// ç®€å•éªŒè¯æ•°æ®é›†åŠ è½½
#[test]
fn test_california_housing_data_loading() {
    let dataset = CaliforniaHousingDataset::load_default().expect("åŠ è½½æ•°æ®é›†å¤±è´¥");

    // éªŒè¯æ•°æ®é›†å¤§å°
    assert!(
        dataset.len() > 20000,
        "æ•°æ®é›†åº”æœ‰ 20000+ æ ·æœ¬ï¼Œå®é™…: {}",
        dataset.len()
    );

    // éªŒè¯ç‰¹å¾ç»´åº¦
    assert_eq!(dataset.feature_dim(), 8);

    // éªŒè¯å¯ä»¥è·å–å•ä¸ªæ ·æœ¬
    let (features, target) = dataset.get(0).expect("è·å–æ ·æœ¬å¤±è´¥");
    assert_eq!(features.shape(), &[8]);
    assert_eq!(target.shape(), &[1]);

    // éªŒè¯æ ‡å‡†åŒ–
    let standardized = dataset.standardize();
    assert!(standardized.is_standardized());

    println!("âœ… æ•°æ®åŠ è½½æµ‹è¯•é€šè¿‡ï¼");
}

/// éªŒè¯è®­ç»ƒé›†/æµ‹è¯•é›†åˆ’åˆ†
#[test]
fn test_california_housing_train_test_split() {
    let dataset = CaliforniaHousingDataset::load_default()
        .expect("åŠ è½½æ•°æ®é›†å¤±è´¥")
        .standardize();

    let total_len = dataset.len();
    let (train, test) = dataset.train_test_split(0.2, Some(42)).expect("åˆ’åˆ†å¤±è´¥");

    // éªŒè¯åˆ’åˆ†æ¯”ä¾‹
    let expected_test_size = (total_len as f32 * 0.2).round() as usize;
    assert_eq!(test.len(), expected_test_size);
    assert_eq!(train.len(), total_len - expected_test_size);

    // éªŒè¯åˆ’åˆ†ç¡®å®šæ€§
    let dataset2 = CaliforniaHousingDataset::load_default()
        .unwrap()
        .standardize();
    let (train2, _) = dataset2.train_test_split(0.2, Some(42)).unwrap();

    // ç›¸åŒç§å­åº”å¾—åˆ°ç›¸åŒçš„è®­ç»ƒé›†ç¬¬ä¸€ä¸ªæ ·æœ¬
    let (f1, t1) = train.get(0).unwrap();
    let (f2, t2) = train2.get(0).unwrap();

    assert_abs_diff_eq!(f1[[0]], f2[[0]], epsilon = 1e-6);
    assert_abs_diff_eq!(t1[[0]], t2[[0]], epsilon = 1e-6);

    println!("âœ… è®­ç»ƒé›†/æµ‹è¯•é›†åˆ’åˆ†æµ‹è¯•é€šè¿‡ï¼");
}
