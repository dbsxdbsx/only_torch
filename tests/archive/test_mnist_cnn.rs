/*
 * @Author       : è€è‘£
 * @Date         : 2025-12-22
 * @Description  : MNIST CNN é›†æˆæµ‹è¯•ï¼ˆå¯¹åº” MatrixSlow Chapter 8ï¼‰
 *                 éªŒè¯ï¼šConv2d + MaxPool2d + AvgPool2d + Linear Layer + Batch è®­ç»ƒ + Adam ä¼˜åŒ–å™¨
 *                 æž„å»º LeNet é£Žæ ¼ CNN è¿›è¡Œæ‰‹å†™æ•°å­—åˆ†ç±»
 *
 * æž¶æž„è¯´æ˜Žï¼š
 *   æœ¬æµ‹è¯•åŸºäºŽç»å…¸ LeNet-5 æž¶æž„ï¼Œä½†æœ‰ä»¥ä¸‹è°ƒæ•´ï¼š
 *   - LeNet-5 åŽŸå§‹è®¾è®¡ï¼ˆ1989, Yann LeCunï¼‰ä½¿ç”¨ **å¹³å‡æ± åŒ– (AvgPool)**
 *   - çŽ°ä»£ CNN å®žè·µä¸­å¸¸ç”¨ **æœ€å¤§æ± åŒ– (MaxPool)** ä»¥èŽ·å¾—æ›´å¥½çš„ç‰¹å¾æå–
 *   - æœ¬æµ‹è¯•åŒæ—¶ä½¿ç”¨ä¸¤ç§æ± åŒ–ï¼špool1 ç”¨ AvgPoolï¼ˆç»å…¸ï¼‰ï¼Œpool2 ç”¨ MaxPoolï¼ˆçŽ°ä»£ï¼‰
 *   - è¿™æ ·è®¾è®¡æ—¢è‡´æ•¬ç»å…¸ï¼ŒåˆéªŒè¯äº†ä¸¤ç§æ± åŒ–å±‚çš„æ­£ç¡®æ€§
 *
 * LeNet-5 åŽŸå§‹ç»“æž„å‚è€ƒï¼š
 *   C1(6@5x5) â†’ S2(AvgPool 2x2) â†’ C3(16@5x5) â†’ S4(AvgPool 2x2) â†’ FC(120) â†’ FC(84) â†’ Output(10)
 */

use only_torch::data::MnistDataset;
use only_torch::nn::layer::{AvgPool2d, Conv2d, Linear, MaxPool2d};
use only_torch::nn::optimizer::{Adam, Optimizer};
use only_torch::nn::{Graph, GraphError, Module, VarActivationOps, VarLossOps, VarShapeOps};
use only_torch::tensor::Tensor;
use std::fs;
use std::time::Instant;

/// MNIST CNN é›†æˆæµ‹è¯•
///
/// ä½¿ç”¨ Conv2d + AvgPool2d + MaxPool2d + Linear æž„å»º LeNet é£Žæ ¼ CNN
/// éªŒè¯æ‰€æœ‰ CNN Layer API çš„æ­£ç¡®æ€§
///
/// ç½‘ç»œç»“æž„ï¼ˆåŸºäºŽ LeNet-5ï¼ŒåŒæ—¶æµ‹è¯•ä¸¤ç§æ± åŒ–ï¼‰ï¼š
/// ```text
/// Input [batch, 1, 28, 28]
///     â†“
/// conv1 (1â†’8, 5x5, pad=2) â†’ ReLU â†’ [batch, 8, 28, 28]
///     â†“
/// avg_pool1 (2x2, stride=2) â†’ [batch, 8, 14, 14]    â† ç»å…¸ LeNet é£Žæ ¼ (AvgPool)
///     â†“
/// conv2 (8â†’16, 3x3, pad=1) â†’ ReLU â†’ [batch, 16, 14, 14]
///     â†“
/// max_pool2 (2x2, stride=2) â†’ [batch, 16, 7, 7]    â† çŽ°ä»£ CNN é£Žæ ¼ (MaxPool)
///     â†“
/// flatten â†’ [batch, 784]
///     â†“
/// fc1 (784 â†’ 64) â†’ ReLU
///     â†“
/// fc2 (64 â†’ 10) â†’ SoftmaxCrossEntropy
/// ```
#[test]
#[cfg_attr(debug_assertions, ignore)]
fn test_mnist_cnn() -> Result<(), GraphError> {
    let start_time = Instant::now();

    println!("\n{}", "=".repeat(60));
    println!("=== MNIST CNN é›†æˆæµ‹è¯•ï¼ˆLeNet é£Žæ ¼ï¼‰===");
    println!("{}\n", "=".repeat(60));

    // ========== 1. åŠ è½½æ•°æ® ==========
    println!("[1/4] åŠ è½½ MNIST æ•°æ®é›†...");
    let load_start = Instant::now();

    let train_data = MnistDataset::train().expect("åŠ è½½ MNIST è®­ç»ƒé›†å¤±è´¥");
    let test_data = MnistDataset::test().expect("åŠ è½½ MNIST æµ‹è¯•é›†å¤±è´¥");
    // æ³¨æ„ï¼šCNN éœ€è¦ [N, C, H, W] æ ¼å¼ï¼Œä¸ flatten

    println!(
        "  âœ“ è®­ç»ƒé›†: {} æ ·æœ¬ï¼Œæµ‹è¯•é›†: {} æ ·æœ¬ï¼Œè€—æ—¶ {:.2}s",
        train_data.len(),
        test_data.len(),
        load_start.elapsed().as_secs_f32()
    );

    // ========== 2. è®­ç»ƒé…ç½®ï¼ˆä¸Ž test_mnist_batch.rs ä¿æŒä¸€è‡´ï¼‰==========
    let batch_size = 512;
    let train_samples = 5000;
    let test_samples = 1000;
    let max_epochs = 15;
    let num_batches = train_samples / batch_size;
    let learning_rate = 0.008; // çº¿æ€§ç¼©æ”¾ï¼šbatch_size Ã—8ï¼Œlr Ã—8
    let target_accuracy = 0.90; // 90% å‡†ç¡®çŽ‡ç›®æ ‡
    let consecutive_success_required = 2;

    println!("\n[2/4] è®­ç»ƒé…ç½®ï¼š");
    println!("  - Batch Size: {batch_size}");
    println!("  - è®­ç»ƒæ ·æœ¬: {train_samples} (å…± {num_batches} ä¸ª batch)");
    println!("  - æµ‹è¯•æ ·æœ¬: {test_samples}");
    println!("  - æœ€å¤§ Epochs: {max_epochs}");
    println!("  - å­¦ä¹ çŽ‡: {learning_rate}");
    println!("  - ç›®æ ‡å‡†ç¡®çŽ‡: {:.0}%", target_accuracy * 100.0);

    // ========== 3. æž„å»º CNN ç½‘ç»œ ==========
    println!("\n[3/4] æž„å»º LeNet é£Žæ ¼ CNN...");
    let build_start = Instant::now();

    let graph = Graph::new_with_seed(42);

    // è¾“å…¥èŠ‚ç‚¹: [batch, 1, 28, 28]
    let x = graph.zeros(&[batch_size, 1, 28, 28])?;
    // æ ‡ç­¾èŠ‚ç‚¹: [batch, 10]
    let y = graph.zeros(&[batch_size, 10])?;

    // ========== å·ç§¯å±‚ 1 ==========
    // conv1: 1â†’8 é€šé“, 5x5 æ ¸, padding=2 (same padding)
    let conv1 = Conv2d::new_seeded(&graph, 1, 8, (5, 5), (1, 1), (2, 2), true, "conv1", 42)?;
    // conv1 è¾“å‡º: [batch, 8, 28, 28]
    let h1 = conv1.forward(&x).leaky_relu(0.0);

    // pool1: 2x2, stride=2 â€”â€” ä½¿ç”¨ AvgPoolï¼ˆç»å…¸ LeNet-5 é£Žæ ¼ï¼‰
    let pool1 = AvgPool2d::new((2, 2), Some((2, 2)), "avg_pool1");
    // pool1 è¾“å‡º: [batch, 8, 14, 14]
    let h2 = pool1.forward(&h1);

    // ========== å·ç§¯å±‚ 2 ==========
    // conv2: 8â†’16 é€šé“, 3x3 æ ¸, padding=1 (same padding)
    let conv2 = Conv2d::new_seeded(&graph, 8, 16, (3, 3), (1, 1), (1, 1), true, "conv2", 43)?;
    // conv2 è¾“å‡º: [batch, 16, 14, 14]
    let h3 = conv2.forward(&h2).leaky_relu(0.0);

    // pool2: 2x2, stride=2 â€”â€” ä½¿ç”¨ MaxPoolï¼ˆçŽ°ä»£ CNN é£Žæ ¼ï¼‰
    let pool2 = MaxPool2d::new((2, 2), Some((2, 2)), "max_pool2");
    // pool2 è¾“å‡º: [batch, 16, 7, 7]
    let h4 = pool2.forward(&h3);

    // ========== å±•å¹³ + å…¨è¿žæŽ¥å±‚ ==========
    // flatten: [batch, 16, 7, 7] â†’ [batch, 784]
    let flat = h4.flatten()?;

    // fc1: 784 â†’ 64
    let fc1 = Linear::new(&graph, 784, 64, true, "fc1")?;
    let h5 = fc1.forward(&flat).leaky_relu(0.0);

    // fc2: 64 â†’ 10 (è¾“å‡ºå±‚)
    let fc2 = Linear::new(&graph, 64, 10, true, "fc2")?;
    let logits = fc2.forward(&h5);

    // æŸå¤±å‡½æ•°
    let loss = logits.cross_entropy(&y)?;

    // æ”¶é›†å‚æ•°
    let mut params = conv1.parameters();
    params.extend(conv2.parameters());
    params.extend(fc1.parameters());
    params.extend(fc2.parameters());

    println!(
        "  âœ“ CNN æž„å»ºå®Œæˆï¼Œè€—æ—¶ {:.2}s",
        build_start.elapsed().as_secs_f32()
    );
    println!("  ç½‘ç»œç»“æž„ï¼ˆåŸºäºŽ LeNet-5ï¼Œæ··åˆä¸¤ç§æ± åŒ–ï¼‰ï¼š");
    println!("    Input [batch, 1, 28, 28]");
    println!("      â†’ Conv1 (1â†’8, 5x5, bias) â†’ ReLU â†’ AvgPool (2x2)  [ç»å…¸]");
    println!("      â†’ Conv2 (8â†’16, 3x3, bias) â†’ ReLU â†’ MaxPool (2x2) [çŽ°ä»£]");
    println!("      â†’ Flatten â†’ FC1 (784â†’64) â†’ ReLU â†’ FC2 (64â†’10)");
    println!("      â†’ SoftmaxCrossEntropy");

    // ä¿å­˜ç½‘ç»œç»“æž„å¯è§†åŒ–ï¼ˆè®­ç»ƒå‰ï¼‰
    let output_dir = "tests/outputs";
    fs::create_dir_all(output_dir).ok();
    graph
        .inner()
        .save_visualization(format!("{output_dir}/mnist_cnn"), None)?;
    graph
        .inner()
        .save_summary(format!("{output_dir}/mnist_cnn_summary.md"))?;
    println!("  âœ“ ç½‘ç»œç»“æž„å·²ä¿å­˜: {output_dir}/mnist_cnn.png");

    // ========== 4. è®­ç»ƒå¾ªçŽ¯ ==========
    println!("\n[4/4] å¼€å§‹è®­ç»ƒ...\n");

    let mut optimizer = Adam::new(&graph, &params, learning_rate);

    // èŽ·å–å›¾åƒæ•°æ®ï¼ˆä¿æŒ [N, 1, 28, 28] æ ¼å¼ï¼‰
    let all_train_images = train_data.images(); // [N, 1, 28, 28]
    let all_train_labels = train_data.labels(); // [N, 10]
    let all_test_images = test_data.images();
    let all_test_labels = test_data.labels();

    let mut consecutive_success_count = 0;
    let mut test_passed = false;
    let test_batches = test_samples / batch_size;

    for epoch in 0..max_epochs {
        let epoch_start = Instant::now();
        let mut epoch_loss_sum = 0.0;

        // è®­ç»ƒ
        for batch_idx in 0..num_batches {
            let start = batch_idx * batch_size;
            let end = start + batch_size;

            // æå– batch æ•°æ®
            let batch_images = extract_batch_4d(all_train_images, start, end, batch_size);
            let batch_labels = extract_batch_2d(all_train_labels, start, end, batch_size);

            x.set_value(&batch_images)?;
            y.set_value(&batch_labels)?;

            let loss_val = optimizer.minimize(&loss)?;
            epoch_loss_sum += loss_val;
        }

        let epoch_avg_loss = epoch_loss_sum / num_batches as f32;

        // æµ‹è¯•ç²¾åº¦
        graph.eval();
        let mut correct = 0;

        for batch_idx in 0..test_batches {
            let start = batch_idx * batch_size;
            let end = start + batch_size;

            let batch_images = extract_batch_4d(all_test_images, start, end, batch_size);
            let batch_labels = extract_batch_2d(all_test_labels, start, end, batch_size);

            x.set_value(&batch_images)?;
            y.set_value(&batch_labels)?;

            logits.forward()?;

            let predictions = logits.value()?.unwrap();

            for i in 0..batch_size {
                let mut pred_class = 0;
                let mut max_val = f32::NEG_INFINITY;
                for j in 0..10 {
                    let val = predictions[[i, j]];
                    if val > max_val {
                        max_val = val;
                        pred_class = j;
                    }
                }

                let mut true_class = 0;
                for j in 0..10 {
                    if batch_labels[[i, j]] > 0.5 {
                        true_class = j;
                        break;
                    }
                }

                if pred_class == true_class {
                    correct += 1;
                }
            }
        }

        graph.train();

        let total_tested = test_batches * batch_size;
        let accuracy = correct as f32 / total_tested as f32;

        println!(
            "Epoch {:2}/{}: loss = {:.4}, å‡†ç¡®çŽ‡ = {:.1}% ({}/{}), è€—æ—¶ {:.2}s",
            epoch + 1,
            max_epochs,
            epoch_avg_loss,
            accuracy * 100.0,
            correct,
            total_tested,
            epoch_start.elapsed().as_secs_f32()
        );

        if accuracy >= target_accuracy {
            consecutive_success_count += 1;
            if consecutive_success_count >= consecutive_success_required {
                test_passed = true;
                println!(
                    "\nðŸŽ‰ è¿žç»­ {} æ¬¡è¾¾åˆ° {:.0}% ä»¥ä¸Šå‡†ç¡®çŽ‡ï¼",
                    consecutive_success_required,
                    target_accuracy * 100.0
                );
                break;
            }
        } else {
            consecutive_success_count = 0;
        }
    }

    let total_duration = start_time.elapsed();
    println!("\næ€»è€—æ—¶: {:.2}s", total_duration.as_secs_f32());

    // æ‰“å°æ¨¡åž‹æ‘˜è¦
    println!("\næ¨¡åž‹æ‘˜è¦ï¼š");
    graph.inner().summary();

    if test_passed {
        println!("\n{}", "=".repeat(60));
        println!("âœ… MNIST CNN æµ‹è¯•é€šè¿‡ï¼");
        println!("{}\n", "=".repeat(60));
        Ok(())
    } else {
        println!("\n{}", "=".repeat(60));
        println!(
            "âŒ æµ‹è¯•å¤±è´¥ï¼šåœ¨ {} ä¸ª epoch å†…æœªèƒ½è¿žç»­ {} æ¬¡è¾¾åˆ° {:.0}% å‡†ç¡®çŽ‡",
            max_epochs,
            consecutive_success_required,
            target_accuracy * 100.0
        );
        println!("{}\n", "=".repeat(60));
        Err(GraphError::ComputationError(format!(
            "MNIST CNN æµ‹è¯•å¤±è´¥ï¼šåœ¨ {} ä¸ª epoch å†…æœªèƒ½è¿žç»­ {} æ¬¡è¾¾åˆ° {:.0}% å‡†ç¡®çŽ‡",
            max_epochs,
            consecutive_success_required,
            target_accuracy * 100.0
        )))
    }
}

/// ä»Ž 4D å¼ é‡ä¸­æå– batchï¼ˆæ‰‹åŠ¨å®žçŽ°ï¼Œé¿å…å®ä¾èµ–é—®é¢˜ï¼‰
fn extract_batch_4d(tensor: &Tensor, start: usize, end: usize, batch_size: usize) -> Tensor {
    let shape = tensor.shape();
    let c = shape[1];
    let h = shape[2];
    let w = shape[3];

    let mut data = Vec::with_capacity(batch_size * c * h * w);

    for n in start..end {
        for ci in 0..c {
            for hi in 0..h {
                for wi in 0..w {
                    data.push(tensor[[n, ci, hi, wi]]);
                }
            }
        }
    }

    Tensor::new(&data, &[batch_size, c, h, w])
}

/// ä»Ž 2D å¼ é‡ä¸­æå– batch
fn extract_batch_2d(tensor: &Tensor, start: usize, end: usize, batch_size: usize) -> Tensor {
    let shape = tensor.shape();
    let cols = shape[1];

    let mut data = Vec::with_capacity(batch_size * cols);

    for n in start..end {
        for j in 0..cols {
            data.push(tensor[[n, j]]);
        }
    }

    Tensor::new(&data, &[batch_size, cols])
}
