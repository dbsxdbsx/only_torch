/*
 * @Author       : è€è‘£
 * @Date         : 2026-01-17
 * @Description  : MNIST MLP é›†æˆæµ‹è¯•
 *
 * å±•ç¤ºç†æƒ³çš„ PyTorch é£æ ¼ APIï¼š
 * - Graph + Varï¼ˆç”¨æˆ·æ— éœ€æ¥è§¦ NodeIdï¼‰
 * - Linear å±‚ï¼ˆå†…éƒ¨è‡ªåŠ¨å¤„ç† bias å¹¿æ’­ï¼‰
 * - Optimizerï¼ˆPyTorch é£æ ¼ APIï¼‰
 * - é“¾å¼è°ƒç”¨
 */

use only_torch::data::MnistDataset;
use only_torch::nn::layer::Linear;
use only_torch::nn::{Adam, Graph, GraphError, Module, Optimizer, VarActivationOps, VarLossOps};
use only_torch::tensor_slice;
use std::fs;
use std::time::Instant;

/// MNIST MLP é›†æˆæµ‹è¯•
///
/// å±•ç¤ºç†æƒ³çš„ PyTorch é£æ ¼ API ç”¨æ³•
#[test]
fn test_mnist_mlp() -> Result<(), GraphError> {
    let start_time = Instant::now();

    println!("\n{}", "=".repeat(60));
    println!("=== MNIST MLP é›†æˆæµ‹è¯• ===");
    println!("{}\n", "=".repeat(60));

    // ========== 1. åŠ è½½æ•°æ® ==========
    println!("[1/4] åŠ è½½ MNIST æ•°æ®é›†...");
    let load_start = Instant::now();

    let train_data = MnistDataset::train()
        .expect("åŠ è½½ MNIST è®­ç»ƒé›†å¤±è´¥")
        .flatten();
    let test_data = MnistDataset::test()
        .expect("åŠ è½½ MNIST æµ‹è¯•é›†å¤±è´¥")
        .flatten();

    println!(
        "  âœ“ è®­ç»ƒé›†: {} æ ·æœ¬ï¼Œæµ‹è¯•é›†: {} æ ·æœ¬ï¼Œè€—æ—¶ {:.2}s",
        train_data.len(),
        test_data.len(),
        load_start.elapsed().as_secs_f32()
    );

    // ========== 2. è®­ç»ƒé…ç½® ==========
    let batch_size = 512;
    let train_samples = 5000;
    let test_samples = 1000;
    let max_epochs = 15;
    let num_batches = train_samples / batch_size;
    let learning_rate = 0.008;
    let target_accuracy = 0.90;
    let consecutive_success_required = 2;

    println!("\n[2/4] è®­ç»ƒé…ç½®ï¼š");
    println!("  - Batch Size: {batch_size}");
    println!("  - è®­ç»ƒæ ·æœ¬: {train_samples} (å…± {num_batches} ä¸ª batch)");
    println!("  - æµ‹è¯•æ ·æœ¬: {test_samples}");
    println!("  - æœ€å¤§ Epochs: {max_epochs}");
    println!("  - å­¦ä¹ ç‡: {learning_rate}");
    println!("  - ç›®æ ‡å‡†ç¡®ç‡: {:.0}%", target_accuracy * 100.0);

    // ========== 3. æ„å»ºç½‘ç»œ ==========
    println!("\n[3/4] æ„å»º MLP: 784 -> 128 (Softplus) -> 10...");

    let graph = Graph::new_with_seed(42);

    // è¾“å…¥å˜é‡ï¼ˆä½¿ç”¨ zeros å ä½ï¼Œç¨åé€šè¿‡ set_value è®¾ç½®çœŸå®æ•°æ®ï¼‰
    let x = graph.zeros(&[batch_size, 784])?;
    let y = graph.zeros(&[batch_size, 10])?;

    // ========== ä½¿ç”¨ Linear å±‚æ„å»ºç½‘ç»œ ==========
    // éšè—å±‚: 784 -> 128ï¼ˆä½¿ç”¨ seeded ç¡®ä¿å¯é‡å¤æ€§ï¼‰
    let fc1 = Linear::new_seeded(&graph, 784, 128, true, "fc1", 100)?;
    // è¾“å‡ºå±‚: 128 -> 10
    let fc2 = Linear::new_seeded(&graph, 128, 10, true, "fc2", 200)?;

    // å‰å‘ä¼ æ’­é“¾ï¼ˆPyTorch é£æ ¼é“¾å¼è°ƒç”¨ï¼‰
    // fc1: [batch, 784] -> [batch, 128]
    let h1 = fc1.forward(&x);
    let a1 = h1.softplus(); // Softplus æ¿€æ´»

    // fc2: [batch, 128] -> [batch, 10]
    let logits = fc2.forward(&a1);

    // æŸå¤±å‡½æ•°ï¼šSoftmax + Cross Entropyï¼ˆcross_entropy å†…å« Softmaxï¼‰
    let loss = logits.cross_entropy(&y)?;

    println!("  âœ“ ç½‘ç»œæ„å»ºå®Œæˆï¼š784 -> 128 -> 10ï¼ˆ2å±‚ MLPï¼‰");
    println!(
        "  âœ“ å‚æ•°èŠ‚ç‚¹ï¼šfc1_W({} params), fc1_b, fc2_W, fc2_b",
        784 * 128
    );

    // æ”¶é›†æ‰€æœ‰å‚æ•°ï¼ˆä½¿ç”¨ Module traitï¼‰
    let mut all_params: Vec<_> = fc1.parameters();
    all_params.extend(fc2.parameters());
    println!("  âœ“ æ€»å‚æ•°æ•°é‡ï¼š{} ä¸ª Var", all_params.len());

    // ä¿å­˜ç½‘ç»œç»“æ„å¯è§†åŒ–
    let output_dir = "tests/outputs";
    fs::create_dir_all(output_dir).ok();

    // ========== 4. åˆ›å»ºä¼˜åŒ–å™¨ ==========
    let mut optimizer = Adam::new(&graph, &all_params, learning_rate);

    println!("  âœ“ ä¼˜åŒ–å™¨ï¼šAdam (lr={learning_rate})");

    // ========== 5. è®­ç»ƒå¾ªç¯ ==========
    println!("\n[4/4] å¼€å§‹è®­ç»ƒ...\n");

    let all_train_images = train_data.images();
    let all_train_labels = train_data.labels();
    let all_test_images = test_data.images();
    let all_test_labels = test_data.labels();

    let mut consecutive_success_count = 0;
    let mut test_passed = false;
    let test_batches = test_samples / batch_size;

    for epoch in 0..max_epochs {
        let epoch_start = Instant::now();
        let mut epoch_loss_sum = 0.0;

        // è®­ç»ƒ
        for batch_idx in 0..num_batches {
            let start = batch_idx * batch_size;
            let end = start + batch_size;

            let batch_images = tensor_slice!(all_train_images, start..end, ..);
            let batch_labels = tensor_slice!(all_train_labels, start..end, ..);

            // è®¾ç½®è¾“å…¥
            x.set_value(&batch_images)?;
            y.set_value(&batch_labels)?;

            // æ¸…ç©ºæ¢¯åº¦
            optimizer.zero_grad()?;

            // åå‘ä¼ æ’­ï¼ˆbackward ä¼šè‡ªåŠ¨ forwardï¼‰
            let loss_val = loss.backward()?;

            // æ›´æ–°å‚æ•°
            optimizer.step()?;

            epoch_loss_sum += loss_val;
        }

        let epoch_avg_loss = epoch_loss_sum / num_batches as f32;

        // æµ‹è¯•ç²¾åº¦
        let mut correct = 0;

        for batch_idx in 0..test_batches {
            let start = batch_idx * batch_size;
            let end = start + batch_size;

            let batch_images = tensor_slice!(all_test_images, start..end, ..);
            let batch_labels = tensor_slice!(all_test_labels, start..end, ..);

            x.set_value(&batch_images)?;
            y.set_value(&batch_labels)?;

            // å‰å‘ä¼ æ’­
            logits.forward()?;

            let predictions = logits.value()?.unwrap();

            for i in 0..batch_size {
                let mut pred_class = 0;
                let mut max_val = f32::NEG_INFINITY;
                for j in 0..10 {
                    let val = predictions[[i, j]];
                    if val > max_val {
                        max_val = val;
                        pred_class = j;
                    }
                }

                let mut true_class = 0;
                for j in 0..10 {
                    if batch_labels[[i, j]] > 0.5 {
                        true_class = j;
                        break;
                    }
                }

                if pred_class == true_class {
                    correct += 1;
                }
            }
        }

        let total_tested = test_batches * batch_size;
        let accuracy = correct as f32 / total_tested as f32;

        println!(
            "Epoch {:2}/{}: loss = {:.4}, å‡†ç¡®ç‡ = {:.1}% ({}/{}), è€—æ—¶ {:.2}s",
            epoch + 1,
            max_epochs,
            epoch_avg_loss,
            accuracy * 100.0,
            correct,
            total_tested,
            epoch_start.elapsed().as_secs_f32()
        );

        if accuracy >= target_accuracy {
            consecutive_success_count += 1;
            if consecutive_success_count >= consecutive_success_required {
                test_passed = true;
                println!(
                    "\nğŸ‰ è¿ç»­ {} æ¬¡è¾¾åˆ° {:.0}% ä»¥ä¸Šå‡†ç¡®ç‡ï¼",
                    consecutive_success_required,
                    target_accuracy * 100.0
                );
                break;
            }
        } else {
            consecutive_success_count = 0;
        }
    }

    let total_duration = start_time.elapsed();
    println!("\næ€»è€—æ—¶: {:.2}s", total_duration.as_secs_f32());

    if test_passed {
        println!("\n{}", "=".repeat(60));
        println!("âœ… MNIST MLP æµ‹è¯•é€šè¿‡ï¼");
        println!("{}\n", "=".repeat(60));
        Ok(())
    } else {
        println!("\n{}", "=".repeat(60));
        println!(
            "âŒ æµ‹è¯•å¤±è´¥ï¼šåœ¨ {} ä¸ª epoch å†…æœªèƒ½è¿ç»­ {} æ¬¡è¾¾åˆ° {:.0}% å‡†ç¡®ç‡",
            max_epochs,
            consecutive_success_required,
            target_accuracy * 100.0
        );
        println!("{}\n", "=".repeat(60));
        Err(GraphError::ComputationError(format!(
            "MNIST MLP æµ‹è¯•å¤±è´¥ï¼šåœ¨ {} ä¸ª epoch å†…æœªèƒ½è¿ç»­ {} æ¬¡è¾¾åˆ° {:.0}% å‡†ç¡®ç‡",
            max_epochs,
            consecutive_success_required,
            target_accuracy * 100.0
        )))
    }
}
