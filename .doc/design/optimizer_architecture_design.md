# Optimizeræ¶æ„è®¾è®¡

## 1. è®¾è®¡ç›®æ ‡

åŸºäºMatrixSlow Pythonç‰ˆæœ¬çš„optimizerè®¾è®¡ï¼Œä¸ºonly_torché¡¹ç›®è®¾è®¡ä¸€ä¸ªå¯æ‰©å±•ã€å¯ç»´æŠ¤çš„ä¼˜åŒ–å™¨æ¶æ„ï¼Œæ”¯æŒå¤šç§ä¼˜åŒ–ç®—æ³•ï¼ˆSGDã€Momentumã€AdaGradã€RMSPropã€Adamç­‰ï¼‰ã€‚

## 2. æ ¸å¿ƒè®¾è®¡åŸåˆ™

- **å¯æ‰©å±•æ€§**: æ˜“äºæ·»åŠ æ–°çš„ä¼˜åŒ–ç®—æ³•
- **ç±»å‹å®‰å…¨**: åˆ©ç”¨Rustçš„ç±»å‹ç³»ç»Ÿç¡®ä¿å®‰å…¨æ€§
- **æ€§èƒ½ä¼˜åŒ–**: é¿å…ä¸å¿…è¦çš„å†…å­˜åˆ†é…å’Œæ‹·è´
- **APIä¸€è‡´æ€§**: ä¸MatrixSlow Pythonç‰ˆæœ¬ä¿æŒç›¸ä¼¼çš„ä½¿ç”¨æ–¹å¼
- **æ¢¯åº¦ç´¯ç§¯**: æ”¯æŒmini-batchè®­ç»ƒçš„æ¢¯åº¦ç´¯ç§¯æœºåˆ¶

## 3. æ¶æ„æ¦‚è§ˆ

```
Optimizer Trait (ä¼˜åŒ–å™¨ç‰¹å¾)
â”œâ”€â”€ æ ¸å¿ƒæ–¹æ³•:
â”‚   â”œâ”€â”€ one_step()     # å•æ­¥è®­ç»ƒï¼ˆå‰å‘+åå‘ä¼ æ’­+æ¢¯åº¦ç´¯ç§¯ï¼‰
â”‚   â”œâ”€â”€ update()       # å‚æ•°æ›´æ–°ï¼ˆæ‰§è¡Œå…·ä½“ä¼˜åŒ–ç®—æ³•ï¼‰
â”‚   â””â”€â”€ reset()        # é‡ç½®ç´¯ç§¯çŠ¶æ€
â”œâ”€â”€ å…·ä½“å®ç°:
â”‚   â”œâ”€â”€ GradientDescent    # æ¢¯åº¦ä¸‹é™
â”‚   â”œâ”€â”€ Momentum          # åŠ¨é‡æ³•
â”‚   â”œâ”€â”€ AdaGrad           # AdaGrad
â”‚   â”œâ”€â”€ RMSProp           # RMSProp
â”‚   â””â”€â”€ Adam              # Adamä¼˜åŒ–å™¨
â””â”€â”€ è¾…åŠ©ç»“æ„:
    â”œâ”€â”€ OptimizerState    # ä¼˜åŒ–å™¨çŠ¶æ€ç®¡ç†
    â””â”€â”€ GradientAccumulator # æ¢¯åº¦ç´¯ç§¯å™¨
```

## 4. æ ¸å¿ƒæ¥å£è®¾è®¡

### 4.1 Optimizer Trait

```rust
pub trait Optimizer {
    /// æ‰§è¡Œä¸€æ­¥è®­ç»ƒï¼šå‰å‘ä¼ æ’­ + åå‘ä¼ æ’­ + æ¢¯åº¦ç´¯ç§¯
    fn one_step(&mut self, graph: &mut Graph, target_node: NodeId) -> Result<(), GraphError>;

    /// æ›´æ–°å‚æ•°ï¼ˆæ‰§è¡Œå…·ä½“çš„ä¼˜åŒ–ç®—æ³•ï¼‰
    fn update(&mut self, graph: &mut Graph) -> Result<(), GraphError>;

    /// é‡ç½®ç´¯ç§¯çŠ¶æ€
    fn reset(&mut self);

    /// è·å–å­¦ä¹ ç‡
    fn learning_rate(&self) -> f32;

    /// è®¾ç½®å­¦ä¹ ç‡
    fn set_learning_rate(&mut self, lr: f32);
}
```

### 4.2 æ¢¯åº¦ç´¯ç§¯å™¨

```rust
pub struct GradientAccumulator {
    /// ç´¯ç§¯çš„æ¢¯åº¦ï¼šNodeId -> ç´¯ç§¯æ¢¯åº¦
    accumulated_gradients: HashMap<NodeId, Tensor>,
    /// ç´¯ç§¯çš„æ ·æœ¬æ•°é‡
    sample_count: usize,
}

impl GradientAccumulator {
    /// ç´¯ç§¯å•ä¸ªæ ·æœ¬çš„æ¢¯åº¦
    pub fn accumulate(&mut self, node_id: NodeId, gradient: &Tensor) -> Result<(), GraphError>;

    /// è·å–å¹³å‡æ¢¯åº¦
    pub fn get_average_gradient(&self, node_id: NodeId) -> Option<Tensor>;

    /// æ¸…é™¤ç´¯ç§¯çŠ¶æ€
    pub fn clear(&mut self);

    /// è·å–ç´¯ç§¯çš„æ ·æœ¬æ•°é‡
    pub fn sample_count(&self) -> usize;
}
```

### 4.3 ä¼˜åŒ–å™¨çŠ¶æ€ç®¡ç†

```rust
pub struct OptimizerState {
    /// å¯è®­ç»ƒå‚æ•°çš„èŠ‚ç‚¹IDåˆ—è¡¨
    trainable_nodes: Vec<NodeId>,
    /// æ¢¯åº¦ç´¯ç§¯å™¨
    gradient_accumulator: GradientAccumulator,
    /// å­¦ä¹ ç‡
    learning_rate: f32,
}
```

## 5. å…·ä½“ä¼˜åŒ–å™¨å®ç°

### 5.1 æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨

```rust
pub struct GradientDescent {
    state: OptimizerState,
}

impl Optimizer for GradientDescent {
    fn update(&mut self, graph: &mut Graph) -> Result<(), GraphError> {
        for &node_id in &self.state.trainable_nodes {
            if let Some(avg_gradient) = self.state.gradient_accumulator.get_average_gradient(node_id) {
                let current_value = graph.get_node_value(node_id)?.unwrap();
                let new_value = current_value - self.state.learning_rate * &avg_gradient;
                graph.set_node_value(node_id, Some(&new_value))?;
            }
        }
        self.state.gradient_accumulator.clear();
        Ok(())
    }
}
```

### 5.2 Adamä¼˜åŒ–å™¨

```rust
pub struct Adam {
    state: OptimizerState,
    beta1: f32,
    beta2: f32,
    epsilon: f32,
    /// ä¸€é˜¶çŸ©ä¼°è®¡
    m: HashMap<NodeId, Tensor>,
    /// äºŒé˜¶çŸ©ä¼°è®¡
    v: HashMap<NodeId, Tensor>,
    /// æ—¶é—´æ­¥
    t: usize,
}

impl Optimizer for Adam {
    fn update(&mut self, graph: &mut Graph) -> Result<(), GraphError> {
        self.t += 1;

        for &node_id in &self.state.trainable_nodes {
            if let Some(gradient) = self.state.gradient_accumulator.get_average_gradient(node_id) {
                // æ›´æ–°ä¸€é˜¶çŸ©ä¼°è®¡
                let m_t = self.beta1 * self.m.get(&node_id).unwrap_or(&Tensor::zeros(gradient.shape()))
                         + (1.0 - self.beta1) * &gradient;

                // æ›´æ–°äºŒé˜¶çŸ©ä¼°è®¡
                let v_t = self.beta2 * self.v.get(&node_id).unwrap_or(&Tensor::zeros(gradient.shape()))
                         + (1.0 - self.beta2) * &gradient.element_wise_multiply(&gradient);

                // åå·®ä¿®æ­£
                let m_hat = &m_t / (1.0 - self.beta1.powi(self.t as i32));
                let v_hat = &v_t / (1.0 - self.beta2.powi(self.t as i32));

                // å‚æ•°æ›´æ–°
                let current_value = graph.get_node_value(node_id)?.unwrap();
                let denominator = v_hat.element_wise_sqrt() + self.epsilon;
                let new_value = current_value - self.state.learning_rate * &m_hat.element_wise_divide(&denominator);

                graph.set_node_value(node_id, Some(&new_value))?;

                // ä¿å­˜çŠ¶æ€
                self.m.insert(node_id, m_t);
                self.v.insert(node_id, v_t);
            }
        }

        self.state.gradient_accumulator.clear();
        Ok(())
    }
}
```

## 6. ä½¿ç”¨ç¤ºä¾‹

### 6.1 åŸºæœ¬ä½¿ç”¨æ–¹å¼

```rust
// åˆ›å»ºè®¡ç®—å›¾å’Œç½‘ç»œç»“æ„
let mut graph = Graph::new();
let x = graph.new_input_node(&[3, 1], Some("x"))?;
let w = graph.new_parameter_node(&[1, 3], Some("w"))?;
let b = graph.new_parameter_node(&[1, 1], Some("b"))?;
let output = graph.new_add_node(&[graph.new_mat_mul_node(w, x, None)?, b], None)?;
let loss = graph.new_perception_loss_node(output, Some("loss"))?;

// åˆ›å»ºä¼˜åŒ–å™¨
let mut optimizer = Adam::new(&graph, loss, 0.01)?;

// è®­ç»ƒå¾ªç¯
for epoch in 0..50 {
    for (features, label) in train_data {
        // è®¾ç½®è¾“å…¥æ•°æ®
        graph.set_node_value(x, Some(&features))?;
        graph.set_node_value(label_node, Some(&label))?;

        // æ‰§è¡Œä¸€æ­¥è®­ç»ƒï¼ˆå‰å‘+åå‘ä¼ æ’­+æ¢¯åº¦ç´¯ç§¯ï¼‰
        optimizer.one_step(&mut graph, loss)?;
    }

    // æ›´æ–°å‚æ•°
    optimizer.update(&mut graph)?;
}
```

### 6.2 Mini-batchè®­ç»ƒ

```rust
let mini_batch_size = 8;
let mut current_batch_size = 0;

for (features, label) in train_data {
    graph.set_node_value(x, Some(&features))?;
    graph.set_node_value(label_node, Some(&label))?;

    optimizer.one_step(&mut graph, loss)?;
    current_batch_size += 1;

    // å½“ç§¯ç´¯åˆ°ä¸€ä¸ªmini batchæ—¶ï¼Œæ‰§è¡Œå‚æ•°æ›´æ–°
    if current_batch_size == mini_batch_size {
        optimizer.update(&mut graph)?;
        current_batch_size = 0;
    }
}
```

## 7. å®ç°è®¡åˆ’

### é˜¶æ®µ1: åŸºç¡€æ¶æ„ âœ…
- [x] å®ç°`Optimizer` trait
- [x] å®ç°`GradientAccumulator`
- [x] å®ç°`OptimizerState`

### é˜¶æ®µ2: åŸºç¡€ä¼˜åŒ–å™¨ ğŸ”„
- [x] å®ç°`SGD` (é‡å‘½åè‡ªGradientDescent)
- [x] åˆ›å»º`optimizer_example.rs`é›†æˆæµ‹è¯•
- [ ] **ä¿®å¤æ¢¯åº¦è®¡ç®—é—®é¢˜** (å½“å‰æ‰€æœ‰æ¢¯åº¦ä¸º0)

### é˜¶æ®µ3: é«˜çº§ä¼˜åŒ–å™¨ ğŸ”„
- [ ] å®ç°`Momentum`
- [ ] å®ç°`AdaGrad`
- [ ] å®ç°`RMSProp`
- [x] å®ç°`Adam` (æ¡†æ¶å®Œæˆï¼Œéœ€ä¿®å¤æ¢¯åº¦é—®é¢˜)

### é˜¶æ®µ4: ä¼˜åŒ–å’Œæ‰©å±•
- [x] åˆ›å»ºbatchç‰ˆæœ¬æµ‹è¯• (`test_adaline_batch.rs`)
- [ ] ä¿®å¤æ¢¯åº¦è®¡ç®—ï¼Œç¡®ä¿optimizeræ­£å¸¸å·¥ä½œ
- [ ] æ€§èƒ½ä¼˜åŒ–
- [ ] å®Œå–„æ–‡æ¡£å’Œæµ‹è¯•

## 8. å½“å‰é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: æ¢¯åº¦è®¡ç®—è¿”å›0 ğŸš¨
**ç°è±¡**: æ‰€æœ‰å‚æ•°èŠ‚ç‚¹çš„æ¢¯åº¦éƒ½æ˜¯0.0ï¼Œå¯¼è‡´å‚æ•°æ— æ³•æ›´æ–°
**å¯èƒ½åŸå› **:
- æŸå¤±å‡½æ•°è¾“å…¥è®¡ç®—æ–¹å¼ä¸æ­£ç¡®
- åå‘ä¼ æ’­é“¾è·¯æœ‰é—®é¢˜
- æ¢¯åº¦è½¬æ¢é€»è¾‘é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
1. å¯¹æ¯”åŸå§‹å•æ ·æœ¬æµ‹è¯•çš„æŸå¤±å‡½æ•°è®¡ç®—æ–¹å¼
2. æ£€æŸ¥`get_node_grad`æ–¹æ³•çš„å®ç°
3. éªŒè¯åå‘ä¼ æ’­æ˜¯å¦æ­£ç¡®æ‰§è¡Œ

### é—®é¢˜2: ä¼˜åŒ–å™¨ç®—æ³•å‘½å âœ…
**è§£å†³**: å°†`GradientDescent`é‡å‘½åä¸º`SGD`ï¼Œæ›´å‡†ç¡®åœ°åæ˜ å…¶å®ç°

## 8. æ–‡ä»¶ç»“æ„

```
src/nn/
â”œâ”€â”€ mod.rs
â”œâ”€â”€ graph.rs
â”œâ”€â”€ nodes/
â”œâ”€â”€ optimizer/           # æ–°å¢ä¼˜åŒ–å™¨æ¨¡å—
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ base.rs         # Optimizer traitå’ŒåŸºç¡€ç»“æ„
â”‚   â”œâ”€â”€ gradient_descent.rs
â”‚   â”œâ”€â”€ momentum.rs
â”‚   â”œâ”€â”€ adagrad.rs
â”‚   â”œâ”€â”€ rmsprop.rs
â”‚   â””â”€â”€ adam.rs
â””â”€â”€ tests/

tests/
â””â”€â”€ optimizer_example.rs  # é›†æˆæµ‹è¯•
```

è¿™ä¸ªè®¾è®¡ç¡®ä¿äº†ä»£ç çš„å¯æ‰©å±•æ€§å’Œå¯ç»´æŠ¤æ€§ï¼ŒåŒæ—¶ä¸MatrixSlow Pythonç‰ˆæœ¬ä¿æŒAPIä¸€è‡´æ€§ã€‚
