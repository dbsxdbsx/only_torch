# æ¢¯åº¦æµæ§åˆ¶æœºåˆ¶è®¾è®¡

## æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿° only_torch ä¸­æ§åˆ¶æ¢¯åº¦è®¡ç®—å’Œä¼ æ’­çš„ä¸‰ç§æ ¸å¿ƒæœºåˆ¶ï¼š`no_grad`ã€`detach` å’Œ `retain_graph`ã€‚è¿™ä¸‰ç§æœºåˆ¶åœ¨é«˜çº§è®­ç»ƒåœºæ™¯ï¼ˆå¦‚ GANã€å¼ºåŒ–å­¦ä¹ ã€å¤šä»»åŠ¡å­¦ä¹ ï¼‰ä¸­ç»å¸¸ç»„åˆä½¿ç”¨ã€‚

## æœºåˆ¶å¯¹æ¯”æ€»è§ˆ

| æœºåˆ¶ | ä½œç”¨åŸŸ | ç›®çš„ | å½±å“èŒƒå›´ | å…¸å‹åœºæ™¯ |
|------|--------|------|----------|----------|
| `no_grad` | å…¨å±€ä¸Šä¸‹æ–‡ | å®Œå…¨ç¦ç”¨æ¢¯åº¦è¿½è¸ª | æ•´ä¸ªä»£ç å— | æ¨ç†ã€è¯„ä¼°ã€éªŒè¯ |
| `detach` | å•ä¸ªèŠ‚ç‚¹ | æˆªæ–­ç‰¹å®šè·¯å¾„çš„æ¢¯åº¦æµ | å±€éƒ¨è·¯å¾„ | GANã€Actor-Criticã€Target Network |
| `retain_graph` | backward è°ƒç”¨ | ä¿ç•™è®¡ç®—å›¾ä¾›å¤šæ¬¡åå‘ä¼ æ’­ | è®¡ç®—å›¾ç”Ÿå‘½å‘¨æœŸ | å¤š Lossã€é«˜é˜¶å¯¼æ•°ã€TBPTT |

### ç›´è§‚å¯¹æ¯”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è®­ç»ƒæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰                          â”‚
â”‚  x â†’ A â†’ B â†’ C â†’ loss                                          â”‚
â”‚       â†‘   â†‘   â†‘                                                â”‚
â”‚      æ¢¯åº¦æ­£å¸¸æµåŠ¨                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        detach (å±€éƒ¨æˆªæ–­)                         â”‚
â”‚  x â†’ A â†’ B.detach() â†’ C â†’ loss                                 â”‚
â”‚       â†‘       â•³       â†‘                                        â”‚
â”‚      æ— æ¢¯åº¦  æˆªæ–­ç‚¹   æœ‰æ¢¯åº¦                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        no_grad (å…¨å±€ç¦ç”¨)                        â”‚
â”‚  x â†’ A â†’ B â†’ C â†’ output                                        â”‚
â”‚      (æ— è®¡ç®—å›¾æ„å»ºï¼Œçº¯å‰å‘è®¡ç®—)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     retain_graph (ä¿ç•™è®¡ç®—å›¾)                    â”‚
â”‚  x â†’ A â†’ B â†’ C â†’ loss1.backward(retain_graph=True)             â”‚
â”‚       â†‘   â†‘   â†‘                                                â”‚
â”‚      å›¾ä¿ç•™ï¼Œå¯å†æ¬¡ backward                                     â”‚
â”‚              â””â”€â”€â”€â†’ loss2.backward()                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. no_grad ä¸Šä¸‹æ–‡

### 1.1 è®¾è®¡ç›®æ ‡

- **å†…å­˜ä¼˜åŒ–**ï¼šæ¨ç†æ—¶ä¸éœ€è¦å­˜å‚¨ä¸­é—´å€¼ç”¨äºåå‘ä¼ æ’­
- **æ€§èƒ½æå‡**ï¼šè·³è¿‡æ¢¯åº¦è¿½è¸ªç›¸å…³çš„å¼€é”€
- **è¯­ä¹‰æ˜ç¡®**ï¼šæ˜ç¡®æ ‡è¯†"è¿™æ®µä»£ç ä¸éœ€è¦æ¢¯åº¦"

### 1.2 API è®¾è®¡

```rust
impl Graph {
    /// åœ¨ no_grad ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œé—­åŒ…
    /// åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ï¼Œå‰å‘ä¼ æ’­ä¸ä¼šä¸ºåå‘ä¼ æ’­ç¼“å­˜ä¸­é—´å€¼
    pub fn no_grad_scope<F, R>(&mut self, f: F) -> R
    where
        F: FnOnce(&mut Self) -> R,
    {
        let was_train = self.is_train_mode();
        self.set_eval_mode();
        let result = f(self);
        if was_train {
            self.set_train_mode();
        }
        result
    }

    /// æ£€æŸ¥æ˜¯å¦åœ¨ no_grad æ¨¡å¼
    pub fn is_grad_enabled(&self) -> bool {
        self.is_train_mode()
    }
}
```

### 1.3 ä½¿ç”¨ç¤ºä¾‹

```rust
// è®­ç»ƒå¾ªç¯
for epoch in 0..epochs {
    // è®­ç»ƒé˜¶æ®µ
    graph.set_train_mode();
    for batch in train_loader {
        graph.forward_node(loss)?;
        graph.backward_nodes(&[w, b], loss)?;
        optimizer.step(&mut graph)?;
        graph.clear_jacobi()?;
    }

    // éªŒè¯é˜¶æ®µï¼ˆno_gradï¼‰
    graph.no_grad_scope(|g| {
        let mut total_loss = 0.0;
        for batch in val_loader {
            g.forward_node(loss)?;
            total_loss += g.get_node_value(loss)?.unwrap().data()[0];
        }
        println!("Validation loss: {}", total_loss / val_loader.len());
        Ok(())
    })?;
}
```

### 1.4 å®ç°è¦ç‚¹

- ä¸ç°æœ‰ `is_train_mode()` / `set_eval_mode()` é›†æˆ
- `eval_mode` ä¸‹çš„ `forward_node` å¯è·³è¿‡ä¸º backward ç¼“å­˜çš„ä¸­é—´å€¼
- æŸäº›å±‚ï¼ˆå¦‚æœªæ¥çš„ Dropoutã€BatchNormï¼‰åœ¨ eval æ¨¡å¼ä¸‹è¡Œä¸ºä¸åŒ

### 1.5 ä¸ PyTorch çš„å¯¹æ¯”

| æ¡†æ¶ | API | è¡Œä¸º |
|------|-----|------|
| PyTorch | `with torch.no_grad():` | ä¸Šä¸‹æ–‡ç®¡ç†å™¨ |
| only_torch | `graph.no_grad_scope(\|g\| { ... })` | é—­åŒ…é£æ ¼ |

---

## 2. detach æœºåˆ¶

### 2.1 è®¾è®¡ç›®æ ‡

- **é€‰æ‹©æ€§æ¢¯åº¦æˆªæ–­**ï¼šåªé˜»æ­¢ç‰¹å®šè·¯å¾„çš„æ¢¯åº¦æµï¼Œå…¶ä»–è·¯å¾„æ­£å¸¸
- **æ”¯æŒé«˜çº§è®­ç»ƒæ¨¡å¼**ï¼šGANã€Actor-Critic ç­‰éœ€è¦ç²¾ç»†æ§åˆ¶æ¢¯åº¦æµå‘

### 2.2 API è®¾è®¡

```rust
impl Graph {
    /// å°†èŠ‚ç‚¹æ ‡è®°ä¸º detachedï¼Œé˜»æ­¢æ¢¯åº¦å›æµåˆ°å…¶çˆ¶èŠ‚ç‚¹
    pub fn detach_node(&mut self, node_id: NodeId) -> Result<(), GraphError> {
        self.get_node_mut(node_id)?.set_detached(true);
        Ok(())
    }

    /// å–æ¶ˆ detach çŠ¶æ€
    pub fn attach_node(&mut self, node_id: NodeId) -> Result<(), GraphError> {
        self.get_node_mut(node_id)?.set_detached(false);
        Ok(())
    }

    /// æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦è¢« detach
    pub fn is_node_detached(&self, node_id: NodeId) -> Result<bool, GraphError> {
        Ok(self.get_node(node_id)?.is_detached())
    }
}

// NodeHandle æ‰©å±•
impl NodeHandle {
    pub fn is_detached(&self) -> bool {
        self.is_detached
    }

    pub fn set_detached(&mut self, detached: bool) {
        self.is_detached = detached;
    }
}
```

### 2.3 å®ç°æ–¹æ¡ˆ

åœ¨ç°æœ‰ `pass_id` æœºåˆ¶ä¸‹å®ç°ï¼Œä¿®æ”¹ `backward_node_internal`ï¼š

```rust
fn backward_node_internal(
    &mut self,
    target_node_id: NodeId,
    result_node_id: NodeId,
) -> Result<(), GraphError> {
    let target_node = self.get_node(target_node_id)?;

    // ğŸ†• æ£€æŸ¥ detach çŠ¶æ€
    if target_node.is_detached() {
        // è§†ä¸ºå¶å­èŠ‚ç‚¹ï¼Œä¸å‘çˆ¶èŠ‚ç‚¹ä¼ æ’­æ¢¯åº¦
        // å¯é€‰ï¼šè®¾ç½® jacobi ä¸º None æˆ–ä¸è®¾ç½®
        return Ok(());
    }

    // åŸæœ‰é€»è¾‘ä¿æŒä¸å˜...
    let parents_ids = self.get_node_parents(target_node_id)?;
    for parent_id in &parents_ids {
        self.backward_node_internal(*parent_id, result_node_id)?;
    }
    // ...
}
```

### 2.4 ä½¿ç”¨ç¤ºä¾‹

#### GAN è®­ç»ƒ

```rust
// è®­ç»ƒåˆ¤åˆ«å™¨
let fake = graph.forward_node(generator_output)?;
graph.detach_node(fake)?;  // é˜²æ­¢ D çš„ loss æ›´æ–° G
let d_fake = graph.forward_node(discriminator_on_fake)?;
graph.backward_nodes(&[d_weights], d_loss)?;

// è®­ç»ƒç”Ÿæˆå™¨
graph.attach_node(fake)?;  // æ¢å¤æ¢¯åº¦æµ
graph.backward_nodes(&[g_weights], g_loss)?;
```

#### Actor-Critic (å¼ºåŒ–å­¦ä¹ )

```rust
// Critic çš„ value ä¼°è®¡ä¼ ç»™ Actor æ—¶éœ€è¦ detach
let value = graph.forward_node(critic_output)?;
graph.detach_node(value)?;  // Actor çš„ loss ä¸åº”æ›´æ–° Critic
let advantage = compute_advantage(reward, value);
// ... è®¡ç®— actor_loss ...
graph.backward_nodes(&[actor_weights], actor_loss)?;
```

### 2.5 ä¸ `value_version` æœºåˆ¶çš„å…³ç³»

å½’æ¡£æ–‡æ¡£ `graph_execution_refactor.md` æè®®ç”¨ `value_version` æ›¿ä»£ `pass_id`ï¼Œå¹¶å£°ç§°å¯¹ `detach` æ›´å‹å¥½ã€‚

**ç»“è®º**ï¼š`detach` åœ¨å½“å‰ `pass_id` æœºåˆ¶ä¸‹**å®Œå…¨å¯å®ç°**ï¼Œä¸¤ç§æœºåˆ¶åœ¨åŠŸèƒ½ä¸Šç­‰ä»·ï¼š

| å®ç°æ–¹å¼ | detach å¤„ç† |
|----------|-------------|
| `pass_id` + é€’å½’ | é€’å½’æ—¶æ£€æŸ¥ `is_detached` flagï¼Œé‡åˆ°åˆ™åœæ­¢ |
| `value_version` + æ‹“æ‰‘æ’åº | æ„å»ºåå‘å­å›¾æ—¶æ’é™¤ detached åˆ†æ”¯ |

---

## 3. retain_graph æœºåˆ¶

### 3.1 è®¾è®¡ç›®æ ‡

- **æ”¯æŒå¤šæ¬¡åå‘ä¼ æ’­**ï¼šå¤šä¸ª Loss å…±äº«è®¡ç®—è·¯å¾„æ—¶å¿…éœ€
- **æ”¯æŒé«˜é˜¶å¯¼æ•°**ï¼šè®¡ç®—æ¢¯åº¦çš„æ¢¯åº¦éœ€è¦ä¿ç•™è®¡ç®—å›¾
- **å†…å­˜æ§åˆ¶**ï¼šé»˜è®¤é‡Šæ”¾ä»¥èŠ‚çœå†…å­˜ï¼Œéœ€è¦æ—¶æ˜¾å¼ä¿ç•™

### 3.2 API è®¾è®¡

```rust
impl Graph {
    /// åå‘ä¼ æ’­ï¼ˆæ‰©å±•ç‰ˆæœ¬ï¼‰
    pub fn backward_nodes_ex(
        &mut self,
        target_nodes: &[NodeId],
        result_node_id: NodeId,
        retain_graph: bool,
    ) -> Result<(), GraphError> {
        // æ‰§è¡Œåå‘ä¼ æ’­...
        self.backward_nodes_internal(target_nodes, result_node_id)?;

        if !retain_graph {
            // é‡Šæ”¾ä¸­é—´è®¡ç®—å€¼ä»¥èŠ‚çœå†…å­˜
            // ä¿ç•™å¶å­èŠ‚ç‚¹ï¼ˆInput/Parameterï¼‰çš„å€¼
            self.release_intermediate_values()?;
        }
        Ok(())
    }

    /// åŸæœ‰ API ä¿æŒå…¼å®¹ï¼ˆé»˜è®¤ retain_graph = falseï¼‰
    pub fn backward_nodes(
        &mut self,
        target_nodes: &[NodeId],
        result_node_id: NodeId,
    ) -> Result<(), GraphError> {
        self.backward_nodes_ex(target_nodes, result_node_id, false)
    }
}
```

### 3.3 å¿…é¡»ä½¿ç”¨ retain_graph çš„åœºæ™¯

#### åœºæ™¯ 1ï¼šå¤š Loss å…±äº«è®¡ç®—è·¯å¾„

```rust
// å¤šä»»åŠ¡å­¦ä¹ 
let features = graph.forward_node(backbone_output)?;
let cls_loss = graph.forward_node(classification_loss)?;
let reg_loss = graph.forward_node(regression_loss)?;

// ç¬¬ä¸€ä¸ª loss backwardï¼Œä¿ç•™å›¾
graph.backward_nodes_ex(&[cls_weights], cls_loss, true)?;
// ç¬¬äºŒä¸ª loss backward
graph.backward_nodes_ex(&[reg_weights], reg_loss, false)?;
```

#### åœºæ™¯ 2ï¼šå¼ºåŒ–å­¦ä¹ å¤šè¾“å‡ºæ¨¡å‹

```rust
// Actor-Critic å…±äº« backbone
let (actor_out, critic_out) = forward_shared_model(&mut graph)?;

let actor_loss = compute_actor_loss(actor_out, actions, advantages);
let critic_loss = compute_critic_loss(critic_out, returns);

// ä¸¤ä¸ª loss éƒ½éœ€è¦ backward
graph.backward_nodes_ex(&[actor_params], actor_loss, true)?;
graph.backward_nodes_ex(&[critic_params], critic_loss, false)?;
```

#### åœºæ™¯ 3ï¼šé«˜é˜¶å¯¼æ•°

```rust
// è®¡ç®— Hessianï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰
// éœ€è¦ä¿ç•™ä¸€é˜¶æ¢¯åº¦çš„è®¡ç®—å›¾
```

### 3.4 å†…å­˜è€ƒè™‘

| retain_graph | è¡Œä¸º | å†…å­˜ |
|--------------|------|------|
| `false`ï¼ˆé»˜è®¤ï¼‰ | backward åé‡Šæ”¾ä¸­é—´å€¼ | ä½ |
| `true` | ä¿ç•™æ‰€æœ‰ä¸­é—´å€¼ | é«˜ |

---

## 4. ç»„åˆä½¿ç”¨æ¨¡å¼

### 4.1 GAN è®­ç»ƒå®Œæ•´ç¤ºä¾‹

```rust
for epoch in 0..epochs {
    // === è®­ç»ƒåˆ¤åˆ«å™¨ ===
    // çœŸå®æ ·æœ¬
    let d_real = graph.forward_node(discriminator_on_real)?;

    // ç”Ÿæˆæ ·æœ¬ï¼ˆdetach é˜²æ­¢æ›´æ–°ç”Ÿæˆå™¨ï¼‰
    let fake = graph.forward_node(generator_output)?;
    graph.detach_node(fake)?;
    let d_fake = graph.forward_node(discriminator_on_fake)?;

    let d_loss = compute_d_loss(d_real, d_fake);
    graph.backward_nodes(&[d_weights], d_loss)?;
    d_optimizer.step(&mut graph)?;
    graph.clear_jacobi()?;

    // === è®­ç»ƒç”Ÿæˆå™¨ ===
    graph.attach_node(fake)?;  // æ¢å¤æ¢¯åº¦æµ
    let g_loss = compute_g_loss(d_fake);
    graph.backward_nodes(&[g_weights], g_loss)?;
    g_optimizer.step(&mut graph)?;
    graph.clear_jacobi()?;
}
```

### 4.2 Actor-Critic (PPO é£æ ¼)

```rust
for epoch in 0..epochs {
    // æ”¶é›†ç»éªŒæ—¶ä½¿ç”¨ no_grad
    let trajectories = graph.no_grad_scope(|g| {
        collect_trajectories(g, env)
    })?;

    // è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆCritic è¾“å‡º detachï¼‰
    let values = graph.forward_node(critic_output)?;
    graph.detach_node(values)?;
    let advantages = compute_gae(rewards, values);

    // å¤šæ¬¡ PPO æ›´æ–°
    for _ in 0..ppo_epochs {
        let actor_loss = compute_ppo_loss(actions, advantages);
        let critic_loss = compute_value_loss(values, returns);

        // ä¸¤ä¸ª loss å…±äº« backboneï¼Œéœ€è¦ retain_graph
        graph.backward_nodes_ex(&[actor_params], actor_loss, true)?;
        graph.backward_nodes_ex(&[critic_params], critic_loss, false)?;

        optimizer.step(&mut graph)?;
        graph.clear_jacobi()?;
    }
}
```

### 4.3 å¤šä»»åŠ¡å­¦ä¹ 

```rust
// å…±äº« backbone çš„å¤šä»»åŠ¡æ¨¡å‹
let features = graph.forward_node(shared_backbone)?;

// ä»»åŠ¡ 1ï¼šåˆ†ç±»
let cls_out = graph.forward_node(classification_head)?;
let cls_loss = graph.forward_node(ce_loss)?;

// ä»»åŠ¡ 2ï¼šæ£€æµ‹
let det_out = graph.forward_node(detection_head)?;
let det_loss = graph.forward_node(detection_loss)?;

// åå‘ä¼ æ’­ï¼ˆæ³¨æ„ retain_graphï¼‰
graph.backward_nodes_ex(&[backbone, cls_head], cls_loss, true)?;
graph.backward_nodes_ex(&[backbone, det_head], det_loss, false)?;

optimizer.step(&mut graph)?;
graph.clear_jacobi()?;
```

---

## 5. å®ç°ä¼˜å…ˆçº§

| åŠŸèƒ½ | ä¼˜å…ˆçº§ | ä¾èµ– | è§¦å‘æ¡ä»¶ |
|------|--------|------|----------|
| `no_grad` / eval mode å¢å¼º | é«˜ | ç°æœ‰ `is_train_mode` | æ¨ç†/è¯„ä¼°éœ€æ±‚ |
| `detach` | ä¸­ | `pass_id` æœºåˆ¶ | GAN/RL ç¤ºä¾‹ |
| `retain_graph` | ä¸­ | backward å®ç° | å¤š Loss åœºæ™¯ |

---

## 6. ä¸å…¶ä»–æ–‡æ¡£çš„å…³ç³»

| æ–‡æ¡£ | å…³æ³¨ç‚¹ |
|------|--------|
| **æœ¬æ–‡æ¡£** | ç”¨æˆ·çº§æ¢¯åº¦æµæ§åˆ¶ API |
| `gradient_clear_and_accumulation_design.md` | è®­ç»ƒå¾ªç¯ä¸­çš„æ¢¯åº¦ç´¯ç§¯å’Œæ¸…é™¤æ—¶æœº |
| `graph_execution_refactor.md`ï¼ˆå·²å½’æ¡£ï¼‰ | åº•å±‚æ‰§è¡Œæœºåˆ¶ï¼ˆpass_id vs value_versionï¼‰ |

---

## 7. å‚è€ƒèµ„æ–™

- [PyTorch Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)
- [JAX Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)

