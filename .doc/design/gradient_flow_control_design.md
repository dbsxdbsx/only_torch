# æ¢¯åº¦æµæ§åˆ¶æœºåˆ¶è®¾è®¡

## æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿° only_torch ä¸­æ§åˆ¶æ¢¯åº¦è®¡ç®—å’Œä¼ æ’­çš„æ ¸å¿ƒæœºåˆ¶ï¼š`no_grad`ã€`detach`ã€`retain_graph`ï¼Œä»¥åŠ**å¯é€‰çš„** `requires_grad` / å†»ç»“æœºåˆ¶ã€‚è¿™äº›æœºåˆ¶åœ¨é«˜çº§è®­ç»ƒåœºæ™¯ï¼ˆå¦‚ GANã€å¼ºåŒ–å­¦ä¹ ã€å¤šä»»åŠ¡å­¦ä¹ ã€è¿ç§»å­¦ä¹ ï¼‰ä¸­ç»å¸¸ç»„åˆä½¿ç”¨ã€‚

## æœºåˆ¶å¯¹æ¯”æ€»è§ˆ

| æœºåˆ¶ | ä½œç”¨åŸŸ | ç›®çš„ | å½±å“èŒƒå›´ | å…¸å‹åœºæ™¯ |
|------|--------|------|----------|----------|
| `no_grad` | å…¨å±€ä¸Šä¸‹æ–‡ | å®Œå…¨ç¦ç”¨æ¢¯åº¦è¿½è¸ª | æ•´ä¸ªä»£ç å— | æ¨ç†ã€è¯„ä¼°ã€éªŒè¯ |
| `detach` | å•ä¸ªèŠ‚ç‚¹ | æˆªæ–­ç‰¹å®šè·¯å¾„çš„æ¢¯åº¦æµ | å±€éƒ¨è·¯å¾„ | GANã€Actor-Criticã€Target Network |
| `retain_graph` | backward è°ƒç”¨ | ä¿ç•™è®¡ç®—å›¾ä¾›å¤šæ¬¡åå‘ä¼ æ’­ | è®¡ç®—å›¾ç”Ÿå‘½å‘¨æœŸ | å¤š Lossã€é«˜é˜¶å¯¼æ•°ã€TBPTT |
| `requires_grad`* | å‚æ•°èŠ‚ç‚¹ | æ§åˆ¶å‚æ•°æ˜¯å¦å‚ä¸æ¢¯åº¦è®¡ç®— | å•ä¸ªå‚æ•° | è¿ç§»å­¦ä¹ ï¼ˆå†»ç»“å±‚ï¼‰ã€éƒ¨åˆ†å¾®è°ƒ |

> \* `requires_grad` / å†»ç»“æœºåˆ¶ä¸º**å¯é€‰åŠŸèƒ½**ï¼Œè¯¦è§ [é™„å½• B](#é™„å½•-brequires_grad--å†»ç»“æœºåˆ¶å¯é€‰åŠŸèƒ½)ã€‚

### ç›´è§‚å¯¹æ¯”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è®­ç»ƒæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰                          â”‚
â”‚  x â†’ A â†’ B â†’ C â†’ loss                                          â”‚
â”‚       â†‘   â†‘   â†‘                                                â”‚
â”‚      æ¢¯åº¦æ­£å¸¸æµåŠ¨                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        detach (å±€éƒ¨æˆªæ–­)                         â”‚
â”‚  x â†’ A â†’ B.detach() â†’ C â†’ loss                                  â”‚
â”‚       â†‘       â•³       â†‘                                         â”‚
â”‚      æ— æ¢¯åº¦  æˆªæ–­ç‚¹   æœ‰æ¢¯åº¦                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        no_grad (å…¨å±€ç¦ç”¨)                        â”‚
â”‚  x â†’ A â†’ B â†’ C â†’ output                                         â”‚
â”‚      (æ— è®¡ç®—å›¾æ„å»ºï¼Œçº¯å‰å‘è®¡ç®—)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     retain_graph (ä¿ç•™è®¡ç®—å›¾)                    â”‚
â”‚  x â†’ A â†’ B â†’ C â†’ loss1.backward(retain_graph=True)              â”‚
â”‚       â†‘   â†‘   â†‘                                                 â”‚
â”‚      å›¾ä¿ç•™ï¼Œå¯å†æ¬¡ backward                                      â”‚
â”‚              â””â”€â”€â”€â†’ loss2.backward()                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              requires_grad=false (å‚æ•°å†»ç»“) [å¯é€‰åŠŸèƒ½]             â”‚
â”‚                                                                 â”‚
â”‚  [æ•°æ®æµ â†’]  data â†’ w0 â†’ w1[frozen] â†’ w2 â†’ loss                  â”‚
â”‚                    â†‘         â”‚         â†‘                        â”‚
â”‚  [â† æ¢¯åº¦æµ]    w0.grad âœ…  ç©¿è¿‡(ä¸å­˜)   w2.grad âœ…                 â”‚
â”‚                                                                 â”‚
â”‚  å…³é”®ï¼šw1 å†»ç»“ä½†æ¢¯åº¦ç©¿è¿‡ï¼Œæ‰€ä»¥ w0 ä»èƒ½è®­ç»ƒ                          â”‚
â”‚  å¯¹æ¯” detachï¼šå¦‚æœ w1 ä¹‹å detachï¼Œw0 å°±æ”¶ä¸åˆ°æ¢¯åº¦äº†               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. no_grad ä¸Šä¸‹æ–‡

### 1.1 è®¾è®¡ç›®æ ‡

- **å†…å­˜ä¼˜åŒ–**ï¼šæ¨ç†æ—¶ä¸éœ€è¦å­˜å‚¨ä¸­é—´å€¼ç”¨äºåå‘ä¼ æ’­
- **æ€§èƒ½æå‡**ï¼šè·³è¿‡æ¢¯åº¦è¿½è¸ªç›¸å…³çš„å¼€é”€
- **è¯­ä¹‰æ˜ç¡®**ï¼šæ˜ç¡®æ ‡è¯†"è¿™æ®µä»£ç ä¸éœ€è¦æ¢¯åº¦"

### 1.2 API è®¾è®¡

```rust
impl Graph {
    /// åœ¨ no_grad ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œé—­åŒ…
    /// åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ï¼Œå‰å‘ä¼ æ’­ä¸ä¼šä¸ºåå‘ä¼ æ’­ç¼“å­˜ä¸­é—´å€¼
    pub fn no_grad_scope<F, R>(&mut self, f: F) -> R
    where
        F: FnOnce(&mut Self) -> R,
    {
        let was_train = self.is_train_mode();
        self.set_eval_mode();
        let result = f(self);
        if was_train {
            self.set_train_mode();
        }
        result
    }

    /// æ£€æŸ¥æ˜¯å¦åœ¨ no_grad æ¨¡å¼
    pub fn is_grad_enabled(&self) -> bool {
        self.is_train_mode()
    }
}
```

### 1.3 ä½¿ç”¨ç¤ºä¾‹

```rust
// è®­ç»ƒå¾ªç¯
for epoch in 0..epochs {
    // è®­ç»ƒé˜¶æ®µ
    graph.set_train_mode();
    for batch in train_loader {
        graph.forward_node(loss)?;
        graph.backward_nodes(&[w, b], loss)?;
        optimizer.step(&mut graph)?;
        graph.clear_jacobi()?;
    }

    // éªŒè¯é˜¶æ®µï¼ˆno_gradï¼‰
    graph.no_grad_scope(|g| {
        let mut total_loss = 0.0;
        for batch in val_loader {
            g.forward_node(loss)?;
            total_loss += g.get_node_value(loss)?.unwrap().data()[0];
        }
        println!("Validation loss: {}", total_loss / val_loader.len());
        Ok(())
    })?;
}
```

### 1.4 å®ç°è¦ç‚¹

- ä¸ç°æœ‰ `is_train_mode()` / `set_eval_mode()` é›†æˆ
- `eval_mode` ä¸‹çš„ `forward_node` å¯è·³è¿‡ä¸º backward ç¼“å­˜çš„ä¸­é—´å€¼
- æŸäº›å±‚ï¼ˆå¦‚æœªæ¥çš„ Dropoutã€BatchNormï¼‰åœ¨ eval æ¨¡å¼ä¸‹è¡Œä¸ºä¸åŒ

### 1.5 ä¸ PyTorch/tch-rs çš„å¯¹æ¯”

| æ¡†æ¶ | API | è¡Œä¸º |
|------|-----|------|
| PyTorch | `with torch.no_grad():` | ä¸Šä¸‹æ–‡ç®¡ç†å™¨ |
| tch-rs | `tch::no_grad(\|\| { ... })` | é—­åŒ…é£æ ¼ |
| tch-rs | `tch::no_grad_guard()` | Guard é£æ ¼ |
| only_torch | `graph.no_grad_scope(\|g\| { ... })` | é—­åŒ…é£æ ¼ |

### 1.6 ä¸ºä½•æš‚ä¸å¼•å…¥ `no_grad_guard` å½¢å¼

tch-rs æä¾›äº†ä¸¤ç§ APIï¼šé—­åŒ…å½¢å¼å’Œ Guard å½¢å¼ã€‚æˆ‘ä»¬ç›®å‰åªå®ç°é—­åŒ…å½¢å¼ï¼ŒåŸå› å¦‚ä¸‹ï¼š

#### æ¶æ„å·®å¼‚

| æ¡†æ¶ | çŠ¶æ€ç®¡ç† | Guard å¯è¡Œæ€§ |
|------|----------|--------------|
| PyTorch/tch-rs | **å…¨å±€/çº¿ç¨‹å±€éƒ¨çŠ¶æ€** | âœ… Guard è‡ªç„¶é€‚é… |
| only_torch | **å›¾ç»‘å®šçŠ¶æ€** | âš ï¸ Guard ä¼šå¯¼è‡´å€Ÿç”¨å†²çª |

```rust
// PyTorch/tch-rs é£æ ¼ï¼šå…¨å±€çŠ¶æ€
let _guard = tch::no_grad_guard();  // ä¿®æ”¹å…¨å±€çŠ¶æ€
let output = model.forward(&input); // tensor æ“ä½œæ£€æŸ¥å…¨å±€çŠ¶æ€

// only_torch è‹¥å®ç° Guard ä¼šé‡åˆ°é—®é¢˜
let _guard = graph.no_grad_guard();  // å€Ÿç”¨äº† &mut graph
graph.forward_node(output)?;         // âŒ æ— æ³•å†å€Ÿç”¨ graphï¼
```

#### é—­åŒ…å½¢å¼çš„ä¼˜åŠ¿

| æ–¹é¢ | é—­åŒ…å½¢å¼ | Guard å½¢å¼ |
|------|----------|------------|
| ä½œç”¨åŸŸæ§åˆ¶ | âœ… è‡ªåŠ¨ã€æ˜ç¡® | âš ï¸ ä¾èµ–å˜é‡ç”Ÿå‘½å‘¨æœŸ |
| çŠ¶æ€æ¢å¤ | âœ… ä¿è¯æ¢å¤ | âš ï¸ éœ€æ­£ç¡®æŒæœ‰ guard |
| Rust é£æ ¼ | âœ… æ›´ç¬¦åˆ RAII | âš ï¸ éœ€é¢å¤–æ³¨æ„ |
| å€Ÿç”¨å®‰å…¨ | âœ… é—­åŒ…å†… `&mut` æ¸…æ™° | âŒ ä¸å›¾ç»‘å®šæ¶æ„å†²çª |

#### ä½•æ—¶è€ƒè™‘å¼•å…¥ Guard å½¢å¼

å½“æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ä¹‹ä¸€æ—¶ï¼Œå¯è€ƒè™‘å¼•å…¥ï¼š

1. **æ¶æ„æ¼”è¿›ä¸ºå…¨å±€çŠ¶æ€æ¨¡å¼**ï¼šå¦‚æœæœªæ¥é¡¹ç›®é‡‡ç”¨ç±»ä¼¼ PyTorch çš„å…¨å±€/çº¿ç¨‹å±€éƒ¨çŠ¶æ€ç®¡ç†æ¢¯åº¦å¼€å…³ï¼ˆè€Œéç»‘å®šåˆ° `Graph` å®ä¾‹ï¼‰ï¼ŒGuard å½¢å¼å°†è‡ªç„¶é€‚é…

2. **å¤šå›¾ååŒåœºæ™¯**ï¼šè‹¥éœ€è¦è·¨å¤šä¸ª `Graph` å®ä¾‹ç»Ÿä¸€ç¦ç”¨æ¢¯åº¦ï¼Œå…¨å±€ Guard ä¼šæ¯”é€ä¸ªè°ƒç”¨ `no_grad_scope` æ›´ä¾¿æ·

3. **ä¸å¤–éƒ¨ FFI é›†æˆ**ï¼šè‹¥éœ€è¦åœ¨ C/FFI è¾¹ç•Œæ§åˆ¶æ¢¯åº¦çŠ¶æ€ï¼ŒGuard æ¨¡å¼å¯èƒ½æ›´é€‚åˆ

#### å½“å‰ç»“è®º

**é—­åŒ…å½¢å¼ `no_grad_scope` å·²è¶³å¤Ÿæ»¡è¶³éœ€æ±‚**ï¼Œä¸”æ›´ç¬¦åˆ Rust çš„å€Ÿç”¨è§„åˆ™å’Œ RAII åŸåˆ™ã€‚åœ¨å½“å‰å›¾ç»‘å®šæ¶æ„ä¸‹ï¼Œè¿™æ˜¯æ›´å®‰å…¨ã€æ›´è‡ªç„¶çš„é€‰æ‹©ã€‚

### 1.7 no_grad ä¸­è°ƒç”¨ backward çš„è­¦å‘Šæœºåˆ¶

#### ä¸ PyTorch çš„è¡Œä¸ºå·®å¼‚

| æ¡†æ¶ | no_grad å†…è°ƒç”¨ backward | åŸå›  |
|------|------------------------|------|
| PyTorch | âŒ **è¿è¡Œæ—¶é”™è¯¯** | åŠ¨æ€å›¾ï¼šno_grad å†…åˆ›å»ºçš„å¼ é‡æ—  `grad_fn`ï¼Œæ— æ³•å›æº¯ |
| only_torch | âš ï¸ **è­¦å‘Šä½†å…è®¸** | é™æ€å›¾ï¼šå›¾åœ¨èŠ‚ç‚¹åˆ›å»ºæ—¶å·²æ„å»ºï¼Œbackward æŠ€æœ¯ä¸Šå¯è¡Œ |

#### ä¸ºä½•ä¸é˜»æ­¢è€Œæ˜¯è­¦å‘Š

1. **æ¶æ„æœ¬è´¨ä¸åŒ**ï¼šPyTorch çš„é”™è¯¯æ˜¯åŠ¨æ€å›¾çš„è‡ªç„¶ç»“æœï¼Œè€Œéæ˜¾å¼æ£€æŸ¥ã€‚only_torch è‹¥è¦é˜»æ­¢éœ€äººä¸ºæ·»åŠ é™åˆ¶ã€‚

2. **å­˜åœ¨åˆæ³•ç”¨ä¾‹**ï¼ˆçº¦ 20%ï¼‰ï¼š
   ```rust
   // è°ƒè¯•åœºæ™¯ï¼šåœ¨è¯„ä¼°æ—¶æŸ¥çœ‹æ¢¯åº¦ä¿¡æ¯
   graph.no_grad_scope(|g| {
       g.forward_node(output)?;
       g.backward_nodes(&[w], output)?;
       println!("Debug grad: {:?}", g.get_node_jacobi(w));
       Ok(())
   });
   ```

3. **å¤§å¤šæ•°æƒ…å†µæ˜¯è¯¯ç”¨**ï¼ˆçº¦ 80%ï¼‰ï¼šç”¨æˆ·å¯èƒ½å¿˜è®°åœ¨è®­ç»ƒæ¨¡å¼ä¸‹è°ƒç”¨ backwardã€‚

#### å®ç°

åœ¨ `backward_nodes_ex` å’Œ `backward_batch` å¼€å¤´æ·»åŠ è­¦å‘Šï¼š

```rust
if !self.is_train_mode() {
    eprintln!(
        "[only_torch è­¦å‘Š] åœ¨ no_grad/eval æ¨¡å¼ä¸‹è°ƒç”¨ backwardï¼Œè¿™é€šå¸¸æ˜¯è¯¯ç”¨ã€‚\
        å¦‚ç¡®éœ€æ­¤è¡Œä¸ºï¼Œè¯·å¿½ç•¥æ­¤è­¦å‘Šã€‚"
    );
}
```

#### å¯¹ç…§æµ‹è¯•

- Rust æµ‹è¯•: `test_no_grad_scope_backward_still_works`
- PyTorch å¯¹ç…§: `tests/calc_jacobi_by_pytorch/no_grad_scope_behavior.py`

---

## 2. detach æœºåˆ¶

> **è®¾è®¡å†³ç­–**ï¼šä¸ºä½•ç”¨ `detach()` è€Œé `target_params` æ§åˆ¶æ¢¯åº¦æµï¼Ÿè¯¦è§ [é™„å½• A](#é™„å½•-aè®¾è®¡å†³ç­–ä¸ºä»€ä¹ˆç”¨-detach-è€Œé-target_params)ã€‚

### 2.1 è®¾è®¡ç›®æ ‡

- **é€‰æ‹©æ€§æ¢¯åº¦æˆªæ–­**ï¼šåªé˜»æ­¢ç‰¹å®šè·¯å¾„çš„æ¢¯åº¦æµï¼Œå…¶ä»–è·¯å¾„æ­£å¸¸
- **æ”¯æŒé«˜çº§è®­ç»ƒæ¨¡å¼**ï¼šGANã€Actor-Critic ç­‰éœ€è¦ç²¾ç»†æ§åˆ¶æ¢¯åº¦æµå‘

### 2.2 API è®¾è®¡

```rust
impl Graph {
    /// å°†èŠ‚ç‚¹æ ‡è®°ä¸º detachedï¼Œé˜»æ­¢æ¢¯åº¦å›æµåˆ°å…¶çˆ¶èŠ‚ç‚¹
    pub fn detach_node(&mut self, node_id: NodeId) -> Result<(), GraphError> {
        self.get_node_mut(node_id)?.set_detached(true);
        Ok(())
    }

    /// å–æ¶ˆ detach çŠ¶æ€
    pub fn attach_node(&mut self, node_id: NodeId) -> Result<(), GraphError> {
        self.get_node_mut(node_id)?.set_detached(false);
        Ok(())
    }

    /// æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦è¢« detach
    pub fn is_node_detached(&self, node_id: NodeId) -> Result<bool, GraphError> {
        Ok(self.get_node(node_id)?.is_detached())
    }
}

// NodeHandle æ‰©å±•
impl NodeHandle {
    pub fn is_detached(&self) -> bool {
        self.is_detached
    }

    pub fn set_detached(&mut self, detached: bool) {
        self.is_detached = detached;
    }
}
```

### 2.3 å®ç°æ–¹æ¡ˆ

åœ¨ç°æœ‰ `pass_id` æœºåˆ¶ä¸‹å®ç°ï¼Œä¿®æ”¹ `backward_node_internal`ï¼š

```rust
fn backward_node_internal(
    &mut self,
    target_node_id: NodeId,
    result_node_id: NodeId,
) -> Result<(), GraphError> {
    let target_node = self.get_node(target_node_id)?;

    // ğŸ†• æ£€æŸ¥ detach çŠ¶æ€
    if target_node.is_detached() {
        // è§†ä¸ºå¶å­èŠ‚ç‚¹ï¼Œä¸å‘çˆ¶èŠ‚ç‚¹ä¼ æ’­æ¢¯åº¦
        // jacobi ä¸è®¾ç½®ï¼ˆä¿æŒ Noneï¼‰
        return Ok(());
    }

    // åŸæœ‰é€»è¾‘ä¿æŒä¸å˜...
    let parents_ids = self.get_node_parents(target_node_id)?;
    for parent_id in &parents_ids {
        self.backward_node_internal(*parent_id, result_node_id)?;
    }
    // ...
}
```

### 2.4 ä¸ PyTorch `tensor.detach()` çš„è¯­ä¹‰å·®å¼‚

> **é‡è¦**ï¼šonly_torch çš„ `detach_node()`/`attach_node()` é‡‡ç”¨"å¼€å…³å¼"è®¾è®¡ï¼Œä¸ PyTorch çš„ `tensor.detach()` è¯­ä¹‰ä¸åŒã€‚

| æ¡†æ¶ | API | è¯­ä¹‰ |
|------|-----|------|
| **PyTorch** | `y = x.detach()` | è¿”å›**æ–°å¼ é‡** `y`ï¼Œ`x` å’Œ `y` å¯åŒæ—¶å­˜åœ¨äºä¸åŒåˆ†æ”¯ |
| **only_torch** | `graph.detach_node(x)` | å¯¹**åŒä¸€èŠ‚ç‚¹** `x` è®¾ç½®å¼€å…³ï¼Œé˜»æ­¢æ¢¯åº¦å›æµ |

è¿™æ˜¯**æœ‰æ„çš„è®¾è®¡é€‰æ‹©**ï¼šonly_torch æ˜¯é™æ€å›¾æ¡†æ¶ï¼Œ"å¼€å…³å¼"è®¾è®¡æ›´ç¬¦åˆé™æ€å›¾çš„å¿ƒæ™ºæ¨¡å‹ï¼Œä¸”åœ¨åŠŸèƒ½ä¸Šå¯è¾¾åˆ°ç›¸åŒæ•ˆæœã€‚

### 2.5 PyTorch è¯­ä¹‰å…¼å®¹æ€§

**å…³é”®è¡Œä¸º**ï¼šå½“èŠ‚ç‚¹è¢« detach åï¼Œå…¶ä¸Šæ¸¸å‚æ•°èŠ‚ç‚¹çš„ jacobi åº”ä¸º `None`ï¼Œè€Œéé›¶å¼ é‡ã€‚

```
ç½‘ç»œ: x â†’ w1 â†’ h(detached) â†’ w2 â†’ y

backward(y) å:
- w2.jacobi = Some(æ­£å¸¸æ¢¯åº¦)
- h.jacobi = None (è¢« detach)
- w1.jacobi = None (æ¢¯åº¦è¢« h é˜»æ–­ï¼Œç¬¦åˆ PyTorch è¯­ä¹‰)
```

å®ç°ç»†èŠ‚ï¼š
- è‹¥ç›®æ ‡èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹éƒ½æ—  jacobiï¼ˆå›  detach å¯¼è‡´ï¼‰ï¼Œåˆ™æ¸…é™¤è¯¥èŠ‚ç‚¹çš„ jacobi
- è¿™ç¡®ä¿äº†è¢« detach é˜»æ–­çš„ä¸Šæ¸¸èŠ‚ç‚¹ä¸ä¼šæ®‹ç•™é›¶æ¢¯åº¦
```

### 2.6 ä½¿ç”¨ç¤ºä¾‹

#### GAN è®­ç»ƒ

```rust
// è®­ç»ƒåˆ¤åˆ«å™¨
let fake = graph.forward_node(generator_output)?;
graph.detach_node(fake)?;  // é˜²æ­¢ D çš„ loss æ›´æ–° G
let d_fake = graph.forward_node(discriminator_on_fake)?;
graph.backward_nodes(&[d_weights], d_loss)?;

// è®­ç»ƒç”Ÿæˆå™¨
graph.attach_node(fake)?;  // æ¢å¤æ¢¯åº¦æµ
graph.backward_nodes(&[g_weights], g_loss)?;
```

#### Actor-Critic (å¼ºåŒ–å­¦ä¹ )

```rust
// Critic çš„ value ä¼°è®¡ä¼ ç»™ Actor æ—¶éœ€è¦ detach
let value = graph.forward_node(critic_output)?;
graph.detach_node(value)?;  // Actor çš„ loss ä¸åº”æ›´æ–° Critic
let advantage = compute_advantage(reward, value);
// ... è®¡ç®— actor_loss ...
graph.backward_nodes(&[actor_weights], actor_loss)?;
```

### 2.7 ä¸ `value_version` æœºåˆ¶çš„å…³ç³»

å½’æ¡£æ–‡æ¡£ `graph_execution_refactor.md` æè®®ç”¨ `value_version` æ›¿ä»£ `pass_id`ï¼Œå¹¶å£°ç§°å¯¹ `detach` æ›´å‹å¥½ã€‚

**ç»“è®º**ï¼š`detach` åœ¨å½“å‰ `pass_id` æœºåˆ¶ä¸‹**å®Œå…¨å¯å®ç°**ï¼Œä¸¤ç§æœºåˆ¶åœ¨åŠŸèƒ½ä¸Šç­‰ä»·ï¼š

| å®ç°æ–¹å¼ | detach å¤„ç† |
|----------|-------------|
| `pass_id` + é€’å½’ | é€’å½’æ—¶æ£€æŸ¥ `is_detached` flagï¼Œé‡åˆ°åˆ™åœæ­¢ |
| `value_version` + æ‹“æ‰‘æ’åº | æ„å»ºåå‘å­å›¾æ—¶æ’é™¤ detached åˆ†æ”¯ |

---

## 3. retain_graph æœºåˆ¶

### 3.1 è®¾è®¡ç›®æ ‡

- **æ”¯æŒå¤šæ¬¡åå‘ä¼ æ’­**ï¼šå¤šä¸ª Loss å…±äº«è®¡ç®—è·¯å¾„æ—¶å¿…éœ€
- **æ”¯æŒé«˜é˜¶å¯¼æ•°**ï¼šè®¡ç®—æ¢¯åº¦çš„æ¢¯åº¦éœ€è¦ä¿ç•™è®¡ç®—å›¾
- **å†…å­˜æ§åˆ¶**ï¼šé»˜è®¤é‡Šæ”¾ä»¥èŠ‚çœå†…å­˜ï¼Œéœ€è¦æ—¶æ˜¾å¼ä¿ç•™

### 3.2 API è®¾è®¡

```rust
impl Graph {
    /// åå‘ä¼ æ’­ï¼ˆæ‰©å±•ç‰ˆæœ¬ï¼‰
    pub fn backward_nodes_ex(
        &mut self,
        target_nodes: &[NodeId],
        result_node_id: NodeId,
        retain_graph: bool,
    ) -> Result<(), GraphError> {
        // æ‰§è¡Œåå‘ä¼ æ’­...
        self.backward_nodes_internal(target_nodes, result_node_id)?;

        if !retain_graph {
            // é‡Šæ”¾ä¸­é—´è®¡ç®—å€¼ä»¥èŠ‚çœå†…å­˜
            // ä¿ç•™å¶å­èŠ‚ç‚¹ï¼ˆInput/Parameterï¼‰çš„å€¼
            self.release_intermediate_values()?;
        }
        Ok(())
    }

    /// ç®€åŒ–ç‰ˆæœ¬ï¼Œé»˜è®¤ retain_graph = falseï¼ˆä¸ PyTorch ä¸€è‡´ï¼‰
    pub fn backward_nodes(
        &mut self,
        target_nodes: &[NodeId],
        result_node_id: NodeId,
    ) -> Result<(), GraphError> {
        self.backward_nodes_ex(target_nodes, result_node_id, false)
    }
}
```

### 3.3 å¿…é¡»ä½¿ç”¨ retain_graph çš„åœºæ™¯

#### åœºæ™¯ 1ï¼šå¤š Loss å…±äº«è®¡ç®—è·¯å¾„

```rust
// å¤šä»»åŠ¡å­¦ä¹ 
let features = graph.forward_node(backbone_output)?;
let cls_loss = graph.forward_node(classification_loss)?;
let reg_loss = graph.forward_node(regression_loss)?;

// ç¬¬ä¸€ä¸ª loss backwardï¼Œä¿ç•™å›¾
graph.backward_nodes_ex(&[cls_weights], cls_loss, true)?;
// ç¬¬äºŒä¸ª loss backward
graph.backward_nodes_ex(&[reg_weights], reg_loss, false)?;
```

#### åœºæ™¯ 2ï¼šå¼ºåŒ–å­¦ä¹ å¤šè¾“å‡ºæ¨¡å‹ï¼ˆActor-Criticï¼‰

> **æ³¨æ„**ï¼šActor-Critic æœ¬è´¨ä¸Šæ˜¯å¤šä»»åŠ¡å­¦ä¹ çš„ä¸€ç§å½¢å¼ï¼Œç»“æ„ä¸åœºæ™¯ 1 ç›¸åŒã€‚

```rust
// Actor-Critic å…±äº« backboneï¼ˆä¸å¤šä»»åŠ¡å­¦ä¹ ç»“æ„ç›¸åŒï¼‰
let (actor_out, critic_out) = forward_shared_model(&mut graph)?;

let actor_loss = compute_actor_loss(actor_out, actions, advantages);
let critic_loss = compute_critic_loss(critic_out, returns);

// ä¸¤ä¸ª loss éƒ½éœ€è¦ backward
graph.backward_nodes_ex(&[actor_params], actor_loss, true)?;
graph.backward_nodes_ex(&[critic_params], critic_loss, false)?;
```

#### åœºæ™¯ 3ï¼šé«˜é˜¶å¯¼æ•°

```rust
// è®¡ç®— Hessianï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰
// éœ€è¦ä¿ç•™ä¸€é˜¶æ¢¯åº¦çš„è®¡ç®—å›¾
```

### 3.4 å†…å­˜è€ƒè™‘

| retain_graph | è¡Œä¸º | å†…å­˜ |
|--------------|------|------|
| `false`ï¼ˆé»˜è®¤ï¼‰ | backward åé‡Šæ”¾ä¸­é—´å€¼ | ä½ |
| `true` | ä¿ç•™æ‰€æœ‰ä¸­é—´å€¼ | é«˜ |

---

## 4. ç»„åˆä½¿ç”¨æ¨¡å¼

### 4.1 GAN è®­ç»ƒå®Œæ•´ç¤ºä¾‹

```rust
for epoch in 0..epochs {
    // === è®­ç»ƒåˆ¤åˆ«å™¨ ===
    // çœŸå®æ ·æœ¬
    let d_real = graph.forward_node(discriminator_on_real)?;

    // ç”Ÿæˆæ ·æœ¬ï¼ˆdetach é˜²æ­¢æ›´æ–°ç”Ÿæˆå™¨ï¼‰
    let fake = graph.forward_node(generator_output)?;
    graph.detach_node(fake)?;
    let d_fake = graph.forward_node(discriminator_on_fake)?;

    let d_loss = compute_d_loss(d_real, d_fake);
    graph.backward_nodes(&[d_weights], d_loss)?;
    d_optimizer.step(&mut graph)?;
    graph.clear_jacobi()?;

    // === è®­ç»ƒç”Ÿæˆå™¨ ===
    graph.attach_node(fake)?;  // æ¢å¤æ¢¯åº¦æµ
    let g_loss = compute_g_loss(d_fake);
    graph.backward_nodes(&[g_weights], g_loss)?;
    g_optimizer.step(&mut graph)?;
    graph.clear_jacobi()?;
}
```

### 4.2 Actor-Critic (PPO é£æ ¼)

```rust
for epoch in 0..epochs {
    // æ”¶é›†ç»éªŒæ—¶ä½¿ç”¨ no_grad
    let trajectories = graph.no_grad_scope(|g| {
        collect_trajectories(g, env)
    })?;

    // è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆCritic è¾“å‡º detachï¼‰
    let values = graph.forward_node(critic_output)?;
    graph.detach_node(values)?;
    let advantages = compute_gae(rewards, values);

    // å¤šæ¬¡ PPO æ›´æ–°
    for _ in 0..ppo_epochs {
        let actor_loss = compute_ppo_loss(actions, advantages);
        let critic_loss = compute_value_loss(values, returns);

        // ä¸¤ä¸ª loss å…±äº« backboneï¼Œéœ€è¦ retain_graph
        graph.backward_nodes_ex(&[actor_params], actor_loss, true)?;
        graph.backward_nodes_ex(&[critic_params], critic_loss, false)?;

        optimizer.step(&mut graph)?;
        graph.clear_jacobi()?;
    }
}
```

### 4.3 å¤šä»»åŠ¡å­¦ä¹ 

```rust
// å…±äº« backbone çš„å¤šä»»åŠ¡æ¨¡å‹
let features = graph.forward_node(shared_backbone)?;

// ä»»åŠ¡ 1ï¼šåˆ†ç±»
let cls_out = graph.forward_node(classification_head)?;
let cls_loss = graph.forward_node(ce_loss)?;

// ä»»åŠ¡ 2ï¼šæ£€æµ‹
let det_out = graph.forward_node(detection_head)?;
let det_loss = graph.forward_node(detection_loss)?;

// åå‘ä¼ æ’­ï¼ˆæ³¨æ„ retain_graphï¼‰
graph.backward_nodes_ex(&[backbone, cls_head], cls_loss, true)?;
graph.backward_nodes_ex(&[backbone, det_head], det_loss, false)?;

optimizer.step(&mut graph)?;
graph.clear_jacobi()?;
```

---

## 5. å®ç°ä¼˜å…ˆçº§

| åŠŸèƒ½ | ä¼˜å…ˆçº§ | ä¾èµ– | è§¦å‘æ¡ä»¶ |
|------|--------|------|----------|
| `no_grad` / eval mode å¢å¼º | é«˜ | ç°æœ‰ `is_train_mode` | æ¨ç†/è¯„ä¼°éœ€æ±‚ |
| `detach` | ä¸­ | `pass_id` æœºåˆ¶ | GAN/RL ç¤ºä¾‹ |
| `retain_graph` | ä¸­ | backward å®ç° | å¤š Loss åœºæ™¯ |

---

## 6. ä¸å…¶ä»–æ–‡æ¡£çš„å…³ç³»

| æ–‡æ¡£ | å…³æ³¨ç‚¹ |
|------|--------|
| **æœ¬æ–‡æ¡£** | ç”¨æˆ·çº§æ¢¯åº¦æµæ§åˆ¶ API |
| `gradient_clear_and_accumulation_design.md` | è®­ç»ƒå¾ªç¯ä¸­çš„æ¢¯åº¦ç´¯ç§¯å’Œæ¸…é™¤æ—¶æœº |
| `graph_execution_refactor.md`ï¼ˆå·²å½’æ¡£ï¼‰ | åº•å±‚æ‰§è¡Œæœºåˆ¶ï¼ˆpass_id vs value_versionï¼‰ |

---

## 7. å®ç°æ³¨æ„äº‹é¡¹

### 7.1 å¤šæ¬¡ forward åçš„ backward

åœ¨å¤šä»»åŠ¡å­¦ä¹ åœºæ™¯ä¸­ï¼Œå¯èƒ½éœ€è¦å¤šæ¬¡è°ƒç”¨ `forward_node`ï¼š

```rust
graph.forward_node(out1)?;  // forward_pass_id = 1
graph.forward_node(out2)?;  // forward_pass_id = 2
```

**å…³é”®å®ç°ç»†èŠ‚**ï¼šåœ¨ backward æ—¶ï¼Œä¸åº”ä¸¥æ ¼æ£€æŸ¥èŠ‚ç‚¹çš„ `forward_pass_id` æ˜¯å¦ç­‰äºå›¾çš„å½“å‰ `last_forward_pass_id`ã€‚è¿™ä¼šå¯¼è‡´åœ¨å¤šæ¬¡ forward åï¼Œæ—©æœŸ forward çš„èŠ‚ç‚¹è¢«é”™è¯¯è·³è¿‡ã€‚

æ­£ç¡®åšæ³•ï¼šåªè·³è¿‡**ä»æœª forward è¿‡**çš„èŠ‚ç‚¹ï¼ˆ`forward_pass_id == 0`ï¼‰ï¼Œè€Œé id ä¸åŒ¹é…çš„èŠ‚ç‚¹ã€‚

### 7.2 æ¢¯åº¦ç´¯ç§¯è¯­ä¹‰ï¼ˆPyTorch å…¼å®¹ï¼‰

å¤šæ¬¡ backward æ—¶ï¼Œæ¢¯åº¦ç´¯ç§¯éµå¾ª PyTorch è¯­ä¹‰ï¼š

| èŠ‚ç‚¹ç±»å‹ | è¡Œä¸º | è¯´æ˜ |
|----------|------|------|
| **å‚æ•°èŠ‚ç‚¹** | jacobi **ç´¯ç§¯** | æ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼ˆå¦‚å¤šä»»åŠ¡å­¦ä¹ ã€å¤§ batch æ¨¡æ‹Ÿï¼‰ |
| **ä¸­é—´èŠ‚ç‚¹** | jacobi **é‡æ–°è®¡ç®—** | æ¯æ¬¡ backward ç‹¬ç«‹è®¡ç®—ï¼Œä¸ç´¯ç§¯ |

#### æ ¸å¿ƒæœºåˆ¶ï¼šä¼ æ’­ä¿¡å· vs ç´¯åŠ å™¨

ç†è§£å¤šæ¬¡ backward çš„å…³é”®æ˜¯åŒºåˆ†ä¸¤ç§ä¸åŒç”¨é€”çš„æ¢¯åº¦ï¼š

| æ¦‚å¿µ | ç”¨é€” | æ˜¯å¦è·¨ backward ç´¯ç§¯ |
|------|------|---------------------|
| **ä¼ æ’­ä¿¡å·**ï¼ˆupstream gradï¼‰ | é“¾å¼æ³•åˆ™å‘ä¸Šä¼ é€’ | âŒ å¿…é¡»æ˜¯æœ¬æ¬¡ backward æ–°ç®—çš„ |
| **å‚æ•°ç´¯åŠ å™¨**ï¼ˆparam.jacobiï¼‰ | ä¼˜åŒ–å™¨æ›´æ–°ç”¨ | âœ… è·¨ backward ç´¯ç§¯ |

**å…³é”®è§„åˆ™**ï¼š
1. æ¯æ¬¡ backward éƒ½ä» scratch è®¡ç®—ä¸€æ¡"æœ¬æ¬¡æ¢¯åº¦æµ"ï¼ˆä¼ æ’­ä¿¡å·åªç”¨æœ¬æ¬¡çš„ï¼‰
2. å‚æ•°èŠ‚ç‚¹ç»´æŠ¤ä¸€ä¸ªè·¨ backward çš„ç´¯åŠ å™¨ï¼ˆç”¨äºæœ€ç»ˆæ›´æ–°ï¼‰
3. éå‚æ•°èŠ‚ç‚¹ä¸ç»´æŠ¤è·¨ backward çš„ç´¯åŠ å™¨ï¼ˆé»˜è®¤ï¼‰ï¼Œå› ä¸ºå®ƒä¸æ˜¯è¦æ›´æ–°çš„çŠ¶æ€
4. âš ï¸ **é“¾å¼æ³•åˆ™ä¼ æ’­å¿…é¡»ä½¿ç”¨"æœ¬æ¬¡æ–°ç®—çš„æ¢¯åº¦"ï¼Œè€Œéä»»ä½•ç´¯ç§¯åçš„å€¼**ï¼ˆå¦åˆ™ä¼š double countï¼‰

**è§„åˆ™ 4 çš„é‡è¦è¡¥å……**ï¼šå³ä½¿ä¸‹æ¸¸èŠ‚ç‚¹ä¹Ÿæ˜¯éœ€è¦ç´¯ç§¯æ¢¯åº¦çš„å‚æ•°èŠ‚ç‚¹ï¼Œåœ¨è®¡ç®—ä¸Šæ¸¸èŠ‚ç‚¹çš„æ¢¯åº¦æ—¶ï¼Œä¹Ÿå¿…é¡»ä½¿ç”¨ä¸‹æ¸¸èŠ‚ç‚¹**æœ¬æ¬¡ backward æ–°ç®—çš„è´¡çŒ®**ï¼Œè€Œéå…¶ç´¯åŠ å™¨ä¸­çš„ç´¯ç§¯å€¼ã€‚

```
å‡è®¾å­˜åœ¨æ‹“æ‰‘ï¼šu(param) â†’ w(param) â†’ out

ç¬¬ 1 æ¬¡ backward:
  w.jacobi = âˆ‚L1/âˆ‚w
  u.jacobi = âˆ‚L1/âˆ‚w Ã— âˆ‚w/âˆ‚u  â† ä½¿ç”¨æœ¬æ¬¡æ–°ç®—çš„ âˆ‚L1/âˆ‚w

ç¬¬ 2 æ¬¡ backward:
  w.jacobi += âˆ‚L2/âˆ‚w  â†’ ç´¯ç§¯å = âˆ‚L1/âˆ‚w + âˆ‚L2/âˆ‚w
  u.jacobi += âˆ‚L2/âˆ‚w Ã— âˆ‚w/âˆ‚u  â† å¿…é¡»ä½¿ç”¨æœ¬æ¬¡æ–°ç®—çš„ âˆ‚L2/âˆ‚wï¼Œä¸èƒ½ç”¨ç´¯ç§¯åçš„ï¼

æ­£ç¡®ç»“æœï¼šu.jacobi = (âˆ‚L1/âˆ‚w + âˆ‚L2/âˆ‚w) Ã— âˆ‚w/âˆ‚u = âˆ‚(L1+L2)/âˆ‚u âœ“
é”™è¯¯ç»“æœï¼ˆè‹¥ç”¨ç´¯ç§¯å€¼ï¼‰ï¼šu.jacobi = âˆ‚L1/âˆ‚wÃ—âˆ‚w/âˆ‚u + (âˆ‚L1/âˆ‚w+âˆ‚L2/âˆ‚w)Ã—âˆ‚w/âˆ‚u
                                = 2Ã—âˆ‚L1/âˆ‚wÃ—âˆ‚w/âˆ‚u + âˆ‚L2/âˆ‚wÃ—âˆ‚w/âˆ‚u âœ— (L1 è¢«ç®—äº†ä¸¤æ¬¡)
```

#### ä¸ºä»€ä¹ˆä¸­é—´èŠ‚ç‚¹ä¸ç´¯ç§¯ä¸å½±å“å‚æ•°çš„æ­£ç¡®æ€§ï¼Ÿ

```
å¤šä»»åŠ¡å­¦ä¹ ç¤ºä¾‹ï¼š
  x â†’ w_shared â†’ features â†’ w1 â†’ out1 (Loss1)
                    â””â”€â”€â”€â”€â†’ w2 â†’ out2 (Loss2)
```

æ•°å­¦ä¸Šï¼Œæ¯æ¬¡ backward è®¡ç®—çš„æ˜¯**ç‹¬ç«‹çš„æ¢¯åº¦æµ**ï¼š

```
ç¬¬ 1 æ¬¡ backward(out1):
  features.jacobi = âˆ‚L1/âˆ‚features  â† æœ¬æ¬¡æ–°ç®—
  w_shared.jacobi = âˆ‚L1/âˆ‚w_shared  â† ä½¿ç”¨ä¸Šé¢çš„ features.jacobi

ç¬¬ 2 æ¬¡ backward(out2):
  features.jacobi = âˆ‚L2/âˆ‚features  â† æœ¬æ¬¡æ–°ç®—ï¼ˆä¸ä¾èµ–ç¬¬ 1 æ¬¡çš„å€¼ï¼ï¼‰
  w_shared.jacobi += âˆ‚L2/âˆ‚w_shared â† ç´¯ç§¯åˆ°å‚æ•°
```

**å…³é”®æ´å¯Ÿ**ï¼šè®¡ç®— `w_shared` çš„æ¢¯åº¦æ—¶ï¼Œåªéœ€è¦**å½“å‰è¿™æ¬¡ backward** ç®—å‡ºæ¥çš„ `âˆ‚L/âˆ‚features`ï¼Œä¸éœ€è¦ä¸Šä¸€æ¬¡ backward ç•™ä¸‹æ¥çš„å€¼ã€‚æ‰€ä»¥æ¸…é™¤ä¸­é—´èŠ‚ç‚¹çš„ jacobi ä¸ä¼šå½±å“å‚æ•°çš„ç´¯ç§¯æ­£ç¡®æ€§ã€‚

ä»"è´£ä»»"çš„è§’åº¦ç†è§£ï¼š
- **å‚æ•°èŠ‚ç‚¹**ï¼šéœ€è¦çŸ¥é“"æˆ‘å¯¹æ‰€æœ‰ loss è´Ÿå¤šå°‘è´£ä»»" â†’ ç´¯ç§¯
- **ä¸­é—´èŠ‚ç‚¹**ï¼šåªæ˜¯ä¼ é€’æ¢¯åº¦çš„"ç®¡é“"ï¼Œæ¯æ¬¡ backward å¯è§†ä¸ºæ¦‚å¿µä¸Šä¸åŒçš„è·¯å¾„ â†’ ä¸ç´¯ç§¯

#### ç¤ºä¾‹

```
backward(out1, retain_graph=True):
  - w_shared.jacobi = [1,2,3,4,...]  âœ“ ä¿ç•™ï¼ˆç´¯åŠ å™¨ï¼‰
  - features.jacobi = [[1],[1]]      æœ¬æ¬¡ä¼ æ’­ä¿¡å·

backward(out2):
  - w_shared.jacobi = [2,4,6,8,...]  ç´¯ç§¯ = task1 + task2
  - features.jacobi = [[1],[1]]      æœ¬æ¬¡ä¼ æ’­ä¿¡å·ï¼ˆé‡æ–°è®¡ç®—ï¼Œä¸æ˜¯ç´¯ç§¯ï¼ï¼‰
```

#### å®ç°ç»†èŠ‚

**backward å¼€å§‹æ—¶**ï¼šè°ƒç”¨ `reset_intermediate_jacobi()` æ¸…é™¤ä¸­é—´èŠ‚ç‚¹çš„ jacobiï¼Œåªä¿ç•™å‚æ•°èŠ‚ç‚¹çš„ jacobiã€‚è¿™ç¡®ä¿ï¼š
1. ä¼ æ’­ä¿¡å·å§‹ç»ˆæ˜¯"æœ¬æ¬¡æ–°ç®—çš„"
2. å‚æ•°ç´¯åŠ å™¨æ­£ç¡®ç´¯ç§¯å¤šæ¬¡ backward çš„è´¡çŒ®

**backward ç»“æŸæ—¶ï¼ˆ`retain_graph=false`ï¼‰**ï¼šè°ƒç”¨ `release_intermediate_results()` åŒæ—¶é‡Šæ”¾ä¸­é—´èŠ‚ç‚¹çš„**å€¼å’Œæ¢¯åº¦**ï¼š
- å€¼è¢«é‡Šæ”¾ï¼šéœ€è¦é‡æ–° forward æ‰èƒ½å†æ¬¡ backward
- æ¢¯åº¦ä¹Ÿè¢«é‡Šæ”¾ï¼šä¿æŒä¸€è‡´æ€§ï¼Œé¿å…ç”¨æˆ·è¯¯ä»¥ä¸ºä¸­é—´èŠ‚ç‚¹çš„æ¢¯åº¦æ˜¯ç´¯ç§¯çš„

è¿™æ›´æ¥è¿‘ PyTorch çš„è¯­ä¹‰ï¼šä¸­é—´èŠ‚ç‚¹çš„æ¢¯åº¦é»˜è®¤ä¸ä¿ç•™ï¼ˆé™¤éæ˜¾å¼è°ƒç”¨ `retain_grad()`ï¼‰ã€‚

è‹¥éœ€è¦é˜»æ­¢å‚æ•°èŠ‚ç‚¹çš„æ¢¯åº¦ç´¯ç§¯ï¼Œåº”åœ¨ backward ä¹‹é—´è°ƒç”¨ `clear_jacobi()`ã€‚

### 7.3 ä¸ºä½•ä¸å¼•å…¥ `retain_grad` åŠŸèƒ½

PyTorch æä¾›äº† `retain_grad()` æ–¹æ³•ï¼Œå…è®¸ä¸­é—´èŠ‚ç‚¹ï¼ˆéå¶å­èŠ‚ç‚¹ï¼‰åœ¨å¤šæ¬¡ backward æ—¶ç´¯ç§¯æ¢¯åº¦ã€‚ç»è¿‡å¯¹ä¸»æµæ¡†æ¶çš„è°ƒç ”ï¼Œæˆ‘ä»¬å†³å®š**æš‚ä¸å¼•å…¥**æ­¤åŠŸèƒ½ã€‚

#### å„æ¡†æ¶å¯¹ä¸­é—´èŠ‚ç‚¹æ¢¯åº¦çš„å¤„ç†

| æ¡†æ¶ | è®¾è®¡æ¨¡å¼ | ä¸­é—´èŠ‚ç‚¹æ¢¯åº¦ | ç±»ä¼¼ `retain_grad`? |
|------|----------|--------------|---------------------|
| **PyTorch** | åŠ¨æ€å›¾ + å¶å­èŠ‚ç‚¹åŒºåˆ† | é»˜è®¤ä¸ä¿ç•™ï¼Œéœ€æ˜¾å¼ `retain_grad()` | âœ… æœ‰ |
| **JAX** | çº¯å‡½æ•°å¼ | **æ ¹æœ¬ä¸æš´éœ²**ï¼ˆåªè¿”å›è¾“å…¥å‚æ•°çš„æ¢¯åº¦ï¼‰ | âŒ æ— æ­¤æ¦‚å¿µ |
| **TensorFlow/Keras** | GradientTape + watch | åªè®¡ç®—æ˜¾å¼ `watch()` çš„å˜é‡ | âŒ æ—  |
| **MXNet** | `attach_grad()` æ˜¾å¼å£°æ˜ | åªè®¡ç®— `attach_grad()` çš„å˜é‡ | âŒ æ—  |

#### ä¸å¼•å…¥çš„ç†ç”±

1. **å†…å­˜æ•ˆç‡**ï¼šä¸­é—´ç‰¹å¾ï¼ˆå¦‚ CNN çš„ feature mapï¼‰å¯èƒ½éå¸¸å¤§ï¼Œé»˜è®¤ä¿ç•™æ‰€æœ‰æ¢¯åº¦ä¼šæ˜¾è‘—å¢åŠ å†…å­˜å ç”¨
2. **å®ç”¨æ€§ä½**ï¼š99% çš„è®­ç»ƒåœºæ™¯åªéœ€è¦å‚æ•°æ¢¯åº¦ï¼Œ`retain_grad` ä¸»è¦ç”¨äºè°ƒè¯•å’Œç ”ç©¶
3. **å½“å‰èƒ½åŠ›å·²è¶³å¤Ÿ**ï¼šåœ¨ `backward(..., retain_graph=true)` åã€ä¸‹ä¸€æ¬¡ backward å‰ï¼Œä¸­é—´èŠ‚ç‚¹çš„ jacobi æ˜¯å¯ä»¥è®¿é—®çš„ï¼Œæ»¡è¶³å¤§å¤šæ•°è°ƒè¯•éœ€æ±‚
4. **API ç®€æ´æ€§**ï¼šé¿å…å¼•å…¥é¢å¤–æ¦‚å¿µï¼Œé™ä½ç”¨æˆ·å­¦ä¹ æˆæœ¬
5. **YAGNI åŸåˆ™**ï¼šåœ¨æ²¡æœ‰æ˜ç¡®éœ€æ±‚å‰ï¼Œä¸è¿‡æ—©å¼•å…¥å¤æ‚åŠŸèƒ½

#### å½“å‰çš„è°ƒè¯•æ–¹å¼

```rust
// ç¬¬ä¸€æ¬¡ backward åï¼Œå¯ä»¥ç«‹å³è®¿é—®ä¸­é—´èŠ‚ç‚¹çš„ jacobi
graph.backward_nodes_ex(&[w], output, true)?;

// è¿™ä¸ªæ—¶é—´çª—å£å†…ï¼Œä¸­é—´èŠ‚ç‚¹çš„ jacobi æ˜¯å¯è®¿é—®çš„
let features_jacobi = graph.get_node(features_id)?.jacobi();
println!("ä¸­é—´ç‰¹å¾çš„æ¢¯åº¦: {:?}", features_jacobi);

// ä¸‹ä¸€æ¬¡ backward ä¼šé‡ç½®ä¸­é—´èŠ‚ç‚¹çš„ jacobi
graph.backward_nodes_ex(&[w], output2, false)?;
```

#### æœªæ¥æ‰©å±•

å½“å‰è®¾è®¡ä¸é˜»ç¢æœªæ¥æ·»åŠ  `retain_grad` åŠŸèƒ½ã€‚å¦‚æœç¡®æœ‰éœ€æ±‚ï¼Œå¯ä»¥ï¼š
1. åœ¨èŠ‚ç‚¹ä¸Šæ·»åŠ  `retains_grad` æ ‡å¿—
2. ä¿®æ”¹ `reset_intermediate_jacobi()` è·³è¿‡æ ‡è®°ä¸º `retains_grad` çš„èŠ‚ç‚¹

---

## 8. ä¸ä¼˜åŒ–å™¨çš„é…åˆ

æ¢¯åº¦æµæ§åˆ¶æœºåˆ¶é€šå¸¸ä¸ `with_params` ä¼˜åŒ–å™¨é…åˆä½¿ç”¨ï¼ˆå¦‚ GAN è®­ç»ƒä¸­ `detach` + ç‹¬ç«‹ä¼˜åŒ–å™¨ï¼‰ã€‚

è¯¦è§ [ä¼˜åŒ–å™¨æ¶æ„è®¾è®¡](optimizer_architecture_design.md#44-æŒ‡å®šå‚æ•°ä¼˜åŒ–with_params) å’Œ `tests/test_mnist_gan.rs`ã€‚

---

## 9. å‚è€ƒèµ„æ–™

- [PyTorch Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)
- [JAX Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)

### é¡¹ç›®å†…å¯¹ç…§æµ‹è¯•

| Rust æµ‹è¯• | PyTorch å¯¹ç…§è„šæœ¬ |
|-----------|------------------|
| `test_retain_graph_multi_task_learning` | `tests/calc_jacobi_by_pytorch/multi_task_learning_retain_graph.py` |
| `test_mnist_gan` | - (é›†æˆæµ‹è¯•ï¼šéªŒè¯ detach + with_params) |

---

## 10. ä¸é«˜å±‚ API çš„é›†æˆ

æœ¬æ–‡æ¡£æè¿°çš„æ¢¯åº¦æµæ§åˆ¶æœºåˆ¶å·²åœ¨ [æ¶æ„ V2 è®¾è®¡](architecture_v2_design.md) ä¸­çš„é«˜å±‚ API ä¸­å¾—åˆ°æ”¯æŒï¼š

| åº•å±‚ API | é«˜å±‚ API (Var ç‰ˆ) |
|----------|-------------------|
| `graph.detach_node(node_id)` | `graph.detach(var)` |
| `graph.attach_node(node_id)` | `graph.attach(var)` |
| `graph.backward_nodes_ex(&ids, loss, retain)` | `graph.backward_ex(loss, &params, retain)` |
| `graph.no_grad_scope(\|g\| { ... })` | åŒä¸Šï¼ˆæ— å˜åŒ–ï¼‰ |

é«˜å±‚ API çš„è®¾è®¡åŸåˆ™æ˜¯**è–„å°è£…**ï¼š`Var` åªæ˜¯ `NodeId` çš„ç±»å‹å®‰å…¨åŒ…è£…ï¼Œæ‰€æœ‰æ¢¯åº¦æµæ§åˆ¶çš„è¯­ä¹‰ä¸åº•å±‚å®Œå…¨ä¸€è‡´ã€‚

---

## é™„å½• Aï¼šè®¾è®¡å†³ç­–â€”â€”ä¸ºä»€ä¹ˆç”¨ `detach()` è€Œé `target_params`

> æœ¬èŠ‚è§£é‡Šä¸ºä½• only_torch ç§»é™¤äº† `backward(target_params)` å‚æ•°ï¼Œæ”¹ç”¨ `detach()` æ§åˆ¶æ¢¯åº¦æµã€‚

### A.1 é—®é¢˜èƒŒæ™¯

åœ¨ GANã€Actor-Critic ç­‰åœºæ™¯ä¸­ï¼Œéœ€è¦æ§åˆ¶"å“ªäº›å‚æ•°è®¡ç®—æ¢¯åº¦"ã€‚å­˜åœ¨ä¸¤ç§è®¾è®¡æ–¹æ¡ˆï¼š

| æ–¹æ¡ˆ | API å½¢å¼ | æ§åˆ¶å±‚é¢ |
|------|----------|----------|
| **æ–¹æ¡ˆ A: `target_params`** | `loss.backward(target_params=&[w1, w2])` | åå‘ä¼ æ’­æ—¶é€‰æ‹©æ€§è®¡ç®— |
| **æ–¹æ¡ˆ B: `detach()`** | `fake.detach(); loss.backward();` | å‰å‘æ—¶æˆªæ–­è®¡ç®—å›¾æ‹“æ‰‘ |

### A.2 ä¸¤ç§æ–¹æ¡ˆçš„å·¥ä½œåŸç†

```
åœºæ™¯ï¼šGAN è®­ç»ƒåˆ¤åˆ«å™¨ Dï¼ˆä¸æƒ³æ›´æ–°ç”Ÿæˆå™¨ Gï¼‰

è®¡ç®—å›¾ï¼š
z â”€â”€â†’ [G] â”€â”€â†’ fake â”€â”€â†’ [D] â”€â”€â†’ d_loss
       â†‘               â†‘
      G å‚æ•°          D å‚æ•°

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æ–¹æ¡ˆ A: target_params â”€ é€‰æ‹©æ€§è®¡ç®—
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
z â”€â”€â†’ [G] â”€â”€â†’ fake â”€â”€â†’ [D] â”€â”€â†’ d_loss
                              â†“
                    backward(target_params=[D å‚æ•°])

è¡Œä¸ºå–å†³äºå®ç°ï¼š
â”œâ”€ å®ç° 1ï¼šè®¡ç®—æ‰€æœ‰æ¢¯åº¦ï¼Œåªè¿”å› D çš„ï¼ˆæµªè´¹è®¡ç®—ï¼‰
â””â”€ å®ç° 2ï¼šæ™ºèƒ½å‰ªæï¼Œåªè®¡ç®—åˆ°è¾¾ D å‚æ•°çš„è·¯å¾„

é—®é¢˜ï¼šAPI è¯­ä¹‰æ··ä¹±â€”â€”backward() åœ¨åš optimizer è¯¥åšçš„äº‹

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æ–¹æ¡ˆ B: detach() â”€ æ‹“æ‰‘æˆªæ–­
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
z â”€â”€â†’ [G] â”€â”€â†’ fake â”€â”€âœ‚â”€â”€â†’ [D] â”€â”€â†’ d_loss
                     â†‘
              detach() æˆªæ–­ç‚¹

åå‘ä¼ æ’­ï¼šd_loss â†’ D â†’ fakeï¼ˆåœæ­¢ï¼Œå›¾è¢«æˆªæ–­ï¼‰
ç»“æœï¼šG çš„æ¢¯åº¦æ ¹æœ¬ä¸ä¼šè¢«è®¡ç®—ï¼ˆå›¾åœ¨å‰å‘æ—¶å°±æˆªæ–­äº†ï¼‰

ä¼˜åŠ¿ï¼šè¯­ä¹‰æ¸…æ™°ï¼Œæ€§èƒ½æœ€ä¼˜
```

### A.3 ä¸ºä»€ä¹ˆé€‰æ‹© `detach()`

#### 1. è¯­ä¹‰æ›´æ¸…æ™°

| æ–¹å¼ | è¯­ä¹‰ | èŒè´£è¾¹ç•Œ |
|------|------|----------|
| `target_params` | "åªè®¡ç®—è¿™äº›å‚æ•°çš„æ¢¯åº¦" | âš ï¸ `backward()` åœ¨åš optimizer çš„äº‹ |
| `detach()` | "ä»è¿™é‡Œåˆ‡æ–­æ¢¯åº¦æµ" | âœ… æ¸…æ™°çš„å›¾æ‹“æ‰‘æ§åˆ¶ |

PyTorch çš„èŒè´£åˆ†ç¦»ï¼š
```python
# PyTorch é£æ ¼ï¼ˆæˆ‘ä»¬é‡‡ç”¨ï¼‰
fake = G(z)
fake_detached = fake.detach()    # æ§åˆ¶å›¾æ‹“æ‰‘
d_loss = D(fake_detached)
d_loss.backward()                # è®¡ç®—æ‰€æœ‰å¯è¾¾çš„æ¢¯åº¦
d_optimizer.step()               # optimizer å†³å®šæ›´æ–°è°

# target_params é£æ ¼ï¼ˆå·²å¼ƒç”¨ï¼‰
fake = G(z)
d_loss = D(fake)
d_loss.backward(target_params=D.parameters())  # backward è¶Šæƒäº†
```

#### 2. æ€§èƒ½æ›´ä¼˜

| æ–¹å¼ | è®¡ç®—é‡ | åŸå›  |
|------|--------|------|
| `detach()` | âœ… æœ€å°‘ | å‰å‘æ—¶æˆªæ–­ï¼Œè¢«æˆªæ–­éƒ¨åˆ†å®Œå…¨ä¸è®¡ç®— |
| `target_params` (å®ç° 1) | âŒ æµªè´¹ | è®¡ç®—æ‰€æœ‰æ¢¯åº¦å†è¿‡æ»¤ |
| `target_params` (å®ç° 2) | âš ï¸ å¤æ‚ | éœ€è¦æ™ºèƒ½å‰ªæç®—æ³• |

#### 3. ä¸ PyTorch ä¸€è‡´

```python
# PyTorch çš„ backward() ç­¾å
Tensor.backward(
    gradient=None,      # ä¸Šæ¸¸æ¢¯åº¦ï¼ˆç”¨äºéæ ‡é‡ lossï¼‰
    retain_graph=None,  # æ˜¯å¦ä¿ç•™è®¡ç®—å›¾
    create_graph=False  # æ˜¯å¦åˆ›å»ºæ¢¯åº¦çš„è®¡ç®—å›¾ï¼ˆé«˜é˜¶å¯¼æ•°ï¼‰
)
# âŒ æ²¡æœ‰ target_params å‚æ•°ï¼
```

é¡¹ç›®æ„¿æ™¯æ˜¯"åª²ç¾ PyTorch çš„æ˜“ç”¨ä½“éªŒ"ï¼ŒAPI åº”ä¸ PyTorch ä¿æŒä¸€è‡´ã€‚

#### 4. æ›´éš¾å‡º Bug

- `detach()` åœ¨å‰å‘æ—¶å°±æˆªæ–­å›¾ï¼Œå¦‚æœå›¾æ„å»ºæœ‰é—®é¢˜ä¼š**ç«‹å³æš´éœ²**
- `target_params` å¯èƒ½**éšè—**å›¾æ„å»ºé—®é¢˜ï¼ˆåå‘æ—¶æ‰å‘ç°æŸäº›æ¢¯åº¦è®¡ç®—ä¸å¯¹ï¼‰

### A.4 æ˜¯å¦å­˜åœ¨ `detach()` æ— æ³•è¦†ç›–çš„åœºæ™¯ï¼Ÿ

**99% çš„åœºæ™¯å¯ä»¥å®Œå…¨æ›¿ä»£**ã€‚å”¯ä¸€å¯èƒ½çš„ä¾‹å¤–æ˜¯ï¼š

```
å¤æ‚å›¾ï¼ˆå‚æ•°ç»„å…±äº«ä¸­é—´èŠ‚ç‚¹ï¼‰ï¼š

           â”Œâ”€â”€â†’ [A] â”€â”€â†’ out_a â”€â”€â†’ loss_a
           â”‚     â†‘
x â”€â”€â†’ [shared] â”€â”€â”¤
           â”‚     â†“
           â””â”€â”€â†’ [B] â”€â”€â†’ out_b â”€â”€â†’ loss_b

åœºæ™¯ï¼šåªæƒ³è®¡ç®— A çš„æ¢¯åº¦ï¼Œä¸æƒ³è®¡ç®— B çš„æ¢¯åº¦

detach() æ–¹å¼ï¼šåœ¨ shared å’Œ B ä¹‹é—´ detach
  - ä½†è¿™æ · B çš„æ¢¯åº¦æ— æ³•æµå› shared
  - å¦‚æœ shared éœ€è¦ä»ä¸¤æ¡è·¯å¾„è·å¾—æ¢¯åº¦ï¼Œè¿™å°±æœ‰é—®é¢˜

target_params æ–¹å¼ï¼šåªæŒ‡å®š A çš„å‚æ•°
  - ä¸å½±å“å›¾æ‹“æ‰‘
```

**ä½†è¿™ç§åœºæ™¯æå…¶ç½•è§**ã€‚åœ¨å®é™… ML åœºæ™¯ä¸­ï¼š
- GANï¼š`detach()` æ˜¯æ ‡å‡†åšæ³•
- Actor-Criticï¼š`detach()` æ˜¯æ ‡å‡†åšæ³•
- å¤šä»»åŠ¡å­¦ä¹ ï¼šç”¨ `retain_graph` + åˆ†åˆ« backward
- è¿ç§»å­¦ä¹ ï¼ˆå†»ç»“å±‚ï¼‰ï¼šç”¨ `requires_grad=False`

### A.5 è¿ç§»æŒ‡å—

å¦‚æœç°æœ‰ä»£ç ä½¿ç”¨äº† `target_params`ï¼š

```rust
// âŒ æ—§ä»£ç ï¼ˆå·²å¼ƒç”¨ï¼‰
let fake = g.forward(input)?;
let d_loss = d.forward(fake)?;
graph.backward(d_loss, Some(&d_params))?;  // target_params

// âœ… æ–°ä»£ç ï¼ˆPyTorch é£æ ¼ï¼‰
let fake = g.forward(input)?;
graph.detach(fake)?;                        // æˆªæ–­æ¢¯åº¦æµ
let d_loss = d.forward(fake)?;
d_loss.backward()?;                         // è®¡ç®—æ‰€æœ‰å¯è¾¾æ¢¯åº¦
d_optimizer.step()?;                        // optimizer å†³å®šæ›´æ–°è°
```

### A.6 æ€»ç»“

| ç»´åº¦ | `target_params` | `detach()` |
|------|-----------------|------------|
| **è¯­ä¹‰æ¸…æ™°åº¦** | âš ï¸ æ··ä¹± | âœ… æ¸…æ™° |
| **æ€§èƒ½** | âš ï¸ å–å†³äºå®ç° | âœ… æœ€ä¼˜ |
| **PyTorch å…¼å®¹** | âŒ ä¸å…¼å®¹ | âœ… ä¸€è‡´ |
| **é”™è¯¯æš´éœ²** | âš ï¸ å¯èƒ½éšè—é—®é¢˜ | âœ… å°½æ—©æš´éœ² |
| **åœºæ™¯è¦†ç›–** | 100% | 99%+ |

**ç»“è®º**ï¼šé‡‡ç”¨ `detach()` + `optimizer.step()` çš„ PyTorch é£æ ¼ï¼Œç§»é™¤ `target_params` å‚æ•°ã€‚

---

## é™„å½• Bï¼š`requires_grad` / å†»ç»“æœºåˆ¶ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰

> **çŠ¶æ€**ï¼šOptional TODOï¼ˆä¸åœ¨å½“å‰è¿­ä»£èŒƒå›´å†…ï¼‰
>
> æœ¬èŠ‚æè¿° `requires_grad` / å‚æ•°å†»ç»“æœºåˆ¶çš„è®¾è®¡è‰æ¡ˆã€‚æ­¤åŠŸèƒ½åœ¨è¿ç§»å­¦ä¹ ã€éƒ¨åˆ†å¾®è°ƒç­‰åœºæ™¯ä¸­æœ‰ç”¨ï¼Œä½†**å¯¹äºç»å¤§å¤šæ•°è®­ç»ƒåœºæ™¯ä¸æ˜¯å¿…éœ€çš„**ã€‚å½“å‰ `detach` æœºåˆ¶å·²èƒ½è¦†ç›– GANã€Actor-Critic ç­‰å¤æ‚æ¢¯åº¦æµæ§åˆ¶éœ€æ±‚ã€‚

### B.1 é—®é¢˜èƒŒæ™¯

åœ¨æŸäº›è®­ç»ƒåœºæ™¯ä¸­ï¼Œç”¨æˆ·å¸Œæœ›"å†»ç»“"éƒ¨åˆ†å‚æ•°â€”â€”ä¸æ›´æ–°å®ƒä»¬ï¼Œä½†**ä»ç„¶å…è®¸æ¢¯åº¦æµç»è¿™äº›å‚æ•°**åˆ°è¾¾æ›´æ—©çš„å¯è®­ç»ƒå‚æ•°ã€‚

| åœºæ™¯ | éœ€æ±‚ | `detach` èƒ½è§£å†³ï¼Ÿ | `requires_grad` èƒ½è§£å†³ï¼Ÿ |
|------|------|------------------|-------------------------|
| **GAN è®­ç»ƒ D æ—¶ä¸æ›´æ–° G** | æ¢¯åº¦ä¸æµå‘ G | âœ… æ˜¯ | âœ… æ˜¯ |
| **è¿ç§»å­¦ä¹ å†»ç»“ backbone** | backbone ä¸æ›´æ–°ï¼Œä½†æ¢¯åº¦ç©¿è¿‡ | âš ï¸ çœ‹æƒ…å†µ | âœ… æ˜¯ |
| **å†»ç»“ embedding ä½†è®­ç»ƒåç»­å±‚** | embedding ä¸æ›´æ–°ï¼Œæ¢¯åº¦ä¸éœ€è¦ç©¿è¿‡ | âœ… æ˜¯ | âœ… æ˜¯ |
| **å…±äº« backbone + åªå†»ç»“ä¸€ä¸ªåˆ†æ”¯çš„å‚æ•°** | å¤æ‚åœºæ™¯ | âš ï¸ å¯èƒ½å›°éš¾ | âœ… æ˜¯ |

### B.2 `detach` vs `requires_grad` å…³é”®åŒºåˆ«

ç†è§£ä¸¤è€…çš„åŒºåˆ«éœ€è¦æ˜ç¡®ä¸¤ä¸ªæ–¹å‘ï¼š
- **å‰å‘ä¼ æ’­æ–¹å‘**ï¼šæ•°æ®ä» input æµå‘ lossï¼ˆinput â†’ ... â†’ lossï¼‰
- **åå‘ä¼ æ’­æ–¹å‘ï¼ˆæ¢¯åº¦æµæ–¹å‘ï¼‰**ï¼šæ¢¯åº¦ä» loss æµå‘å‚æ•°ï¼ˆloss â†’ ... â†’ parametersï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           detach() è¯­ä¹‰                                â”‚
â”‚                                                                        â”‚
â”‚  [æ•°æ®æµæ–¹å‘ â†’]                                                        â”‚
â”‚   data â”€â”€â–º layer_A[å‚æ•°] â”€â”€â–º detach() â”€â”€â–º layer_B[å‚æ•°] â”€â”€â–º loss       â”‚
â”‚                                â”‚                                      â”‚
â”‚                           æˆªæ–­ç‚¹ï¼šæ¢¯åº¦å®Œå…¨åœæ­¢                          â”‚
â”‚                                                                        â”‚
â”‚  [â† æ¢¯åº¦æµæ–¹å‘]                                                        â”‚
â”‚   loss â†’ layer_B.grad âœ… â†’ åœæ­¢ â•³                                      â”‚
â”‚                                                                        â”‚
â”‚   ç»“æœï¼šlayer_B æœ‰æ¢¯åº¦ï¼Œlayer_A æ— æ¢¯åº¦ï¼ˆè¢«æˆªæ–­ï¼‰                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      requires_grad=false è¯­ä¹‰                          â”‚
â”‚                                                                        â”‚
â”‚  [æ•°æ®æµæ–¹å‘ â†’]                                                        â”‚
â”‚   data â”€â”€â–º layer_A[å‚æ•°] â”€â”€â–º frozen_layer[å†»ç»“] â”€â”€â–º layer_B[å‚æ•°] â”€â”€â–º loss â”‚
â”‚                                     â”‚                                 â”‚
â”‚                            ä¸ç´¯ç§¯è‡ªå·±çš„æ¢¯åº¦ï¼Œä½†æ¢¯åº¦ç»§ç»­ä¼ æ’­               â”‚
â”‚                                                                        â”‚
â”‚  [â† æ¢¯åº¦æµæ–¹å‘]                                                        â”‚
â”‚   loss â†’ layer_B.grad âœ… â†’ frozen_layer.grad=None â†’ layer_A.grad âœ…    â”‚
â”‚                              â†‘ ä¸å­˜å‚¨         â†‘ æ¢¯åº¦ç©¿è¿‡               â”‚
â”‚                                                                        â”‚
â”‚   ç»“æœï¼šlayer_A å’Œ layer_B éƒ½æœ‰æ¢¯åº¦ï¼Œåªæœ‰ frozen_layer æ²¡æœ‰ï¼ˆä½†å®ƒè®©æ¢¯åº¦ç©¿è¿‡ï¼‰â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒåŒºåˆ«ä¸€å¥è¯æ€»ç»“**ï¼š
- `detach()` = æ¢¯åº¦**åˆ°è¿™é‡Œå°±åœ**ï¼ˆé˜»æ–­ä¼ æ’­ï¼‰
- `requires_grad=false` = æ¢¯åº¦**ç©¿è¿‡ä½†ä¸å­˜**ï¼ˆç»§ç»­ä¼ æ’­ï¼Œä½†è¯¥å‚æ•°ä¸ç´¯ç§¯ï¼‰

### B.3 ä½•æ—¶éœ€è¦ `requires_grad`ï¼ˆè€Œé `detach`ï¼‰

**ç»å¤§å¤šæ•°åœºæ™¯ï¼ˆ~99%ï¼‰ä½¿ç”¨ `detach` å³å¯**ã€‚ä»¥ä¸‹æ˜¯å°‘æ•°éœ€è¦ `requires_grad` çš„åœºæ™¯ï¼š

1. **å†»ç»“ä¸­é—´å±‚ï¼Œä½†éœ€è¦è®­ç»ƒå…¶"ä¸Šæ¸¸"å‚æ•°**ï¼š
   ```
   [æ•°æ®æµæ–¹å‘ â†’]
   data â”€â”€â–º encoder[è¦è®­ç»ƒ] â”€â”€â–º adapter[è¦å†»ç»“] â”€â”€â–º head[è¦è®­ç»ƒ] â”€â”€â–º loss
                                    â”‚
                               å¦‚æœç”¨ detach()ï¼š
                               - head æœ‰æ¢¯åº¦ âœ…
                               - adapter æ— æ¢¯åº¦ âœ…
                               - encoder æ— æ¢¯åº¦ âŒ â† è¢«æˆªæ–­äº†ï¼

                               å¦‚æœç”¨ requires_grad=falseï¼š
                               - head æœ‰æ¢¯åº¦ âœ…
                               - adapter æ— æ¢¯åº¦ âœ…ï¼ˆå†»ç»“ï¼‰
                               - encoder æœ‰æ¢¯åº¦ âœ… â† æ¢¯åº¦ç©¿è¿‡äº† adapter
   ```

2. **å¤šä»»åŠ¡å­¦ä¹  + é€‰æ‹©æ€§å†»ç»“åˆ†æ”¯**ï¼š
   ```
                             â”Œâ”€â”€â–º task_A_head[è¦è®­ç»ƒ] â”€â”€â–º loss_A
   data â”€â”€â–º shared_encoder â”€â”€â”¤
                             â””â”€â”€â–º task_B_head[è¦å†»ç»“] â”€â”€â–º loss_B

   éœ€æ±‚ï¼štask_B_head ä¸è®­ç»ƒï¼Œä½† shared_encoder éœ€è¦ä» loss_B è·å¾—æ¢¯åº¦

   å¦‚æœç”¨ detach() æ”¾åœ¨ shared_encoder å’Œ task_B_head ä¹‹é—´ï¼š
   - task_B_head ä¸è®­ç»ƒ âœ…
   - ä½† loss_B çš„æ¢¯åº¦æ— æ³•æµå› shared_encoder âŒ

   å¦‚æœç”¨ requires_grad=false å†»ç»“ task_B_headï¼š
   - task_B_head ä¸è®­ç»ƒ âœ…
   - loss_B çš„æ¢¯åº¦å¯ä»¥ç©¿è¿‡ task_B_head åˆ°è¾¾ shared_encoder âœ…
   ```

**é‡è¦ç»“è®º**ï¼š
- å¦‚æœä½ è¦å†»ç»“çš„å‚æ•°**æ›´é è¿‘ loss ç«¯**ï¼ˆå³ï¼šå†»ç»“ç‚¹å’Œ loss ä¹‹é—´æ²¡æœ‰å…¶ä»–éœ€è¦è®­ç»ƒçš„å‚æ•°ï¼‰ï¼Œç”¨ `detach` å®Œå…¨è¶³å¤Ÿ
- å¦‚æœä½ è¦å†»ç»“çš„å‚æ•°**ä½äºä¸­é—´**ï¼Œä¸”å…¶"ä¸Šæ¸¸"ï¼ˆæ›´é è¿‘ input ç«¯ï¼‰è¿˜æœ‰éœ€è¦è®­ç»ƒçš„å‚æ•°ï¼Œåˆ™éœ€è¦ `requires_grad=false`

### B.4 API è®¾è®¡ï¼ˆè‰æ¡ˆï¼‰

```rust
impl Graph {
    /// å†»ç»“å‚æ•°ï¼ˆä¸ç´¯ç§¯æ¢¯åº¦ï¼Œä½†å…è®¸æ¢¯åº¦æµç»ï¼‰
    pub fn freeze_param(&mut self, param_id: NodeId) -> Result<(), GraphError>;

    /// è§£å†»å‚æ•°
    pub fn unfreeze_param(&mut self, param_id: NodeId) -> Result<(), GraphError>;

    /// æ£€æŸ¥å‚æ•°æ˜¯å¦è¢«å†»ç»“
    pub fn is_param_frozen(&self, param_id: NodeId) -> Result<bool, GraphError>;
}

// Var æ‰©å±•ï¼ˆé«˜å±‚ APIï¼‰
impl Var {
    /// å†»ç»“æ­¤å‚æ•°
    pub fn freeze(&self) -> Result<&Self, GraphError>;

    /// è§£å†»æ­¤å‚æ•°
    pub fn unfreeze(&self) -> Result<&Self, GraphError>;

    /// æ£€æŸ¥æ˜¯å¦è¢«å†»ç»“
    pub fn is_frozen(&self) -> Result<bool, GraphError>;
}

// ä½¿ç”¨ç¤ºä¾‹
let backbone_params = backbone.parameters();
for param in &backbone_params {
    param.freeze()?;  // å†»ç»“ backbone
}

// è®­ç»ƒæ—¶ï¼Œbackbone çš„å‚æ•°ä¸ä¼šè¢«æ›´æ–°ï¼Œä½†æ¢¯åº¦ä¼šç©¿è¿‡å®ƒä»¬
loss.backward()?;
optimizer.step()?;  // åªæ›´æ–°éå†»ç»“å‚æ•°
```

### B.5 å®ç°è¦ç‚¹

1. **åœ¨ `Parameter` èŠ‚ç‚¹ä¸Šå¢åŠ  `requires_grad` æ ‡å¿—**
2. **backward æ—¶**ï¼šå¯¹äº `requires_grad=false` çš„å‚æ•°ï¼Œä¸ç´¯ç§¯å…¶ `.grad`ï¼Œä½†ç»§ç»­å‘å…¶çˆ¶èŠ‚ç‚¹ä¼ æ’­æ¢¯åº¦
3. **optimizer.step() æ—¶**ï¼šè·³è¿‡ `requires_grad=false` çš„å‚æ•°

### B.6 ä¸å…¶ä»–æœºåˆ¶çš„å¯¹æ¯”æ€»ç»“

| æœºåˆ¶ | é˜»æ­¢æ¢¯åº¦æµï¼Ÿ | é˜»æ­¢å‚æ•°æ›´æ–°ï¼Ÿ | é€‚ç”¨åœºæ™¯ |
|------|-------------|---------------|----------|
| `detach()` | âœ… æ˜¯ | âœ… æ˜¯ï¼ˆé—´æ¥ï¼‰ | GANã€Actor-Criticã€æ¢¯åº¦éš”ç¦» |
| `requires_grad=false` | âŒ å¦ | âœ… æ˜¯ | è¿ç§»å­¦ä¹ å†»ç»“ã€éƒ¨åˆ†å¾®è°ƒ |
| `no_grad` | âœ… æ˜¯ï¼ˆå…¨å±€ï¼‰ | âœ… æ˜¯ï¼ˆå…¨å±€ï¼‰ | æ¨ç†ã€è¯„ä¼° |
| `optimizer` ä¸åŒ…å«å‚æ•° | âŒ å¦ | âœ… æ˜¯ | é€‰æ‹©æ€§è®­ç»ƒ |

**é‡è¦ï¼šè¿™ä¸‰ä¸ªç»´åº¦æ˜¯æ­£äº¤çš„**

- `detach` ç®¡"æ¢¯åº¦èƒ½ä¸èƒ½è¿‡å»"ï¼ˆå›¾æ‹“æ‰‘ï¼‰
- `requires_grad` ç®¡"è¿™ä¸ªå‚æ•°è¦ä¸è¦ç´¯ç§¯æ¢¯åº¦"
- `optimizerå‚æ•°åˆ—è¡¨` ç®¡"step æ—¶æ›´æ–°è°"

åœ¨ PyTorch ä¸­ï¼Œå®ƒä»¬å¯ä»¥ç‹¬ç«‹ç»„åˆä½¿ç”¨ã€‚

### B.7 ä¸ºä»€ä¹ˆæ˜¯ Optional TODO

1. **`detach` å·²è¦†ç›– 99% åœºæ™¯**ï¼šGANã€Actor-Criticã€Target Network ç­‰éƒ½ç”¨ `detach`
2. **optimizer é€‰æ‹©æ€§ç»‘å®š**ï¼š`SGD::with_params(&partial_params, lr)` ä¹Ÿèƒ½å®ç°"åªæ›´æ–°éƒ¨åˆ†å‚æ•°"
3. **å®ç°æˆæœ¬**ï¼šéœ€è¦ä¿®æ”¹ backward é€»è¾‘ï¼Œå¢åŠ å¤æ‚åº¦
4. **ç”¨æˆ·å­¦ä¹ æˆæœ¬**ï¼šå¤šä¸€ä¸ªæ¦‚å¿µéœ€è¦ç†è§£

**âš ï¸ é‡è¦æé†’ï¼šoptimizer é€‰æ‹©æ€§ç»‘å®šçš„å±€é™æ€§**

"optimizer ä¸åŒ…å«å‚æ•°"**åªèƒ½æ›¿ä»£"å†»ç»“æ›´æ–°"ï¼Œä¸èƒ½ç­‰ä»·æ›¿ä»£ `requires_grad=false`**ï¼š
1. **æ¢¯åº¦ä»ä¼šè¢«è®¡ç®—/å å†…å­˜**ï¼šå¦‚æœå‚æ•° `requires_grad=true`ï¼ˆé»˜è®¤ï¼‰ï¼Œå³ä½¿ä¸åœ¨ optimizer ä¸­ï¼Œbackward æ—¶ä»ä¼šè®¡ç®—å…¶ `.grad`
2. **`zero_grad()` è¦†ç›–èŒƒå›´ä¸åŒ**ï¼šoptimizer çš„ `zero_grad()` åªæ¸…é™¤å…¶ç®¡ç†çš„å‚æ•°ï¼Œä¸ä¼šæ¸…é™¤"ä¸åœ¨ optimizer ä¸­"çš„å‚æ•°æ¢¯åº¦ï¼Œå¯èƒ½å¯¼è‡´æ¢¯åº¦æ„å¤–ç´¯ç§¯
3. **ç”Ÿæ€å·¥å…·é“¾ä¾èµ– `requires_grad` è¯­ä¹‰**ï¼šå¦‚ `filter(p.requires_grad, params)`ã€æ¢¯åº¦è£å‰ªç­‰

**å»ºè®®**ï¼šä»…å½“æœ‰æ˜ç¡®çš„è¿ç§»å­¦ä¹  / å¤æ‚å†»ç»“éœ€æ±‚æ—¶ï¼Œå†å®ç°æ­¤åŠŸèƒ½ã€‚
