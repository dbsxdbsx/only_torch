# æ¢¯åº¦æµæ§åˆ¶æœºåˆ¶è®¾è®¡

## æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿° only_torch ä¸­æ§åˆ¶æ¢¯åº¦è®¡ç®—å’Œä¼ æ’­çš„ä¸‰ç§æ ¸å¿ƒæœºåˆ¶ï¼š`no_grad`ã€`detach` å’Œ `retain_graph`ã€‚è¿™ä¸‰ç§æœºåˆ¶åœ¨é«˜çº§è®­ç»ƒåœºæ™¯ï¼ˆå¦‚ GANã€å¼ºåŒ–å­¦ä¹ ã€å¤šä»»åŠ¡å­¦ä¹ ï¼‰ä¸­ç»å¸¸ç»„åˆä½¿ç”¨ã€‚

## æœºåˆ¶å¯¹æ¯”æ€»è§ˆ

| æœºåˆ¶ | ä½œç”¨åŸŸ | ç›®çš„ | å½±å“èŒƒå›´ | å…¸å‹åœºæ™¯ |
|------|--------|------|----------|----------|
| `no_grad` | å…¨å±€ä¸Šä¸‹æ–‡ | å®Œå…¨ç¦ç”¨æ¢¯åº¦è¿½è¸ª | æ•´ä¸ªä»£ç å— | æ¨ç†ã€è¯„ä¼°ã€éªŒè¯ |
| `detach` | å•ä¸ªèŠ‚ç‚¹ | æˆªæ–­ç‰¹å®šè·¯å¾„çš„æ¢¯åº¦æµ | å±€éƒ¨è·¯å¾„ | GANã€Actor-Criticã€Target Network |
| `retain_graph` | backward è°ƒç”¨ | ä¿ç•™è®¡ç®—å›¾ä¾›å¤šæ¬¡åå‘ä¼ æ’­ | è®¡ç®—å›¾ç”Ÿå‘½å‘¨æœŸ | å¤š Lossã€é«˜é˜¶å¯¼æ•°ã€TBPTT |

### ç›´è§‚å¯¹æ¯”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è®­ç»ƒæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰                          â”‚
â”‚  x â†’ A â†’ B â†’ C â†’ loss                                          â”‚
â”‚       â†‘   â†‘   â†‘                                                â”‚
â”‚      æ¢¯åº¦æ­£å¸¸æµåŠ¨                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        detach (å±€éƒ¨æˆªæ–­)                         â”‚
â”‚  x â†’ A â†’ B.detach() â†’ C â†’ loss                                 â”‚
â”‚       â†‘       â•³       â†‘                                        â”‚
â”‚      æ— æ¢¯åº¦  æˆªæ–­ç‚¹   æœ‰æ¢¯åº¦                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        no_grad (å…¨å±€ç¦ç”¨)                        â”‚
â”‚  x â†’ A â†’ B â†’ C â†’ output                                        â”‚
â”‚      (æ— è®¡ç®—å›¾æ„å»ºï¼Œçº¯å‰å‘è®¡ç®—)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     retain_graph (ä¿ç•™è®¡ç®—å›¾)                    â”‚
â”‚  x â†’ A â†’ B â†’ C â†’ loss1.backward(retain_graph=True)             â”‚
â”‚       â†‘   â†‘   â†‘                                                â”‚
â”‚      å›¾ä¿ç•™ï¼Œå¯å†æ¬¡ backward                                     â”‚
â”‚              â””â”€â”€â”€â†’ loss2.backward()                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. no_grad ä¸Šä¸‹æ–‡

### 1.1 è®¾è®¡ç›®æ ‡

- **å†…å­˜ä¼˜åŒ–**ï¼šæ¨ç†æ—¶ä¸éœ€è¦å­˜å‚¨ä¸­é—´å€¼ç”¨äºåå‘ä¼ æ’­
- **æ€§èƒ½æå‡**ï¼šè·³è¿‡æ¢¯åº¦è¿½è¸ªç›¸å…³çš„å¼€é”€
- **è¯­ä¹‰æ˜ç¡®**ï¼šæ˜ç¡®æ ‡è¯†"è¿™æ®µä»£ç ä¸éœ€è¦æ¢¯åº¦"

### 1.2 API è®¾è®¡

```rust
impl Graph {
    /// åœ¨ no_grad ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œé—­åŒ…
    /// åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ï¼Œå‰å‘ä¼ æ’­ä¸ä¼šä¸ºåå‘ä¼ æ’­ç¼“å­˜ä¸­é—´å€¼
    pub fn no_grad_scope<F, R>(&mut self, f: F) -> R
    where
        F: FnOnce(&mut Self) -> R,
    {
        let was_train = self.is_train_mode();
        self.set_eval_mode();
        let result = f(self);
        if was_train {
            self.set_train_mode();
        }
        result
    }

    /// æ£€æŸ¥æ˜¯å¦åœ¨ no_grad æ¨¡å¼
    pub fn is_grad_enabled(&self) -> bool {
        self.is_train_mode()
    }
}
```

### 1.3 ä½¿ç”¨ç¤ºä¾‹

```rust
// è®­ç»ƒå¾ªç¯
for epoch in 0..epochs {
    // è®­ç»ƒé˜¶æ®µ
    graph.set_train_mode();
    for batch in train_loader {
        graph.forward_node(loss)?;
        graph.backward_nodes(&[w, b], loss)?;
        optimizer.step(&mut graph)?;
        graph.clear_jacobi()?;
    }

    // éªŒè¯é˜¶æ®µï¼ˆno_gradï¼‰
    graph.no_grad_scope(|g| {
        let mut total_loss = 0.0;
        for batch in val_loader {
            g.forward_node(loss)?;
            total_loss += g.get_node_value(loss)?.unwrap().data()[0];
        }
        println!("Validation loss: {}", total_loss / val_loader.len());
        Ok(())
    })?;
}
```

### 1.4 å®ç°è¦ç‚¹

- ä¸ç°æœ‰ `is_train_mode()` / `set_eval_mode()` é›†æˆ
- `eval_mode` ä¸‹çš„ `forward_node` å¯è·³è¿‡ä¸º backward ç¼“å­˜çš„ä¸­é—´å€¼
- æŸäº›å±‚ï¼ˆå¦‚æœªæ¥çš„ Dropoutã€BatchNormï¼‰åœ¨ eval æ¨¡å¼ä¸‹è¡Œä¸ºä¸åŒ

### 1.5 ä¸ PyTorch/tch-rs çš„å¯¹æ¯”

| æ¡†æ¶ | API | è¡Œä¸º |
|------|-----|------|
| PyTorch | `with torch.no_grad():` | ä¸Šä¸‹æ–‡ç®¡ç†å™¨ |
| tch-rs | `tch::no_grad(\|\| { ... })` | é—­åŒ…é£æ ¼ |
| tch-rs | `tch::no_grad_guard()` | Guard é£æ ¼ |
| only_torch | `graph.no_grad_scope(\|g\| { ... })` | é—­åŒ…é£æ ¼ |

### 1.6 ä¸ºä½•æš‚ä¸å¼•å…¥ `no_grad_guard` å½¢å¼

tch-rs æä¾›äº†ä¸¤ç§ APIï¼šé—­åŒ…å½¢å¼å’Œ Guard å½¢å¼ã€‚æˆ‘ä»¬ç›®å‰åªå®ç°é—­åŒ…å½¢å¼ï¼ŒåŸå› å¦‚ä¸‹ï¼š

#### æ¶æ„å·®å¼‚

| æ¡†æ¶ | çŠ¶æ€ç®¡ç† | Guard å¯è¡Œæ€§ |
|------|----------|--------------|
| PyTorch/tch-rs | **å…¨å±€/çº¿ç¨‹å±€éƒ¨çŠ¶æ€** | âœ… Guard è‡ªç„¶é€‚é… |
| only_torch | **å›¾ç»‘å®šçŠ¶æ€** | âš ï¸ Guard ä¼šå¯¼è‡´å€Ÿç”¨å†²çª |

```rust
// PyTorch/tch-rs é£æ ¼ï¼šå…¨å±€çŠ¶æ€
let _guard = tch::no_grad_guard();  // ä¿®æ”¹å…¨å±€çŠ¶æ€
let output = model.forward(&input); // tensor æ“ä½œæ£€æŸ¥å…¨å±€çŠ¶æ€

// only_torch è‹¥å®ç° Guard ä¼šé‡åˆ°é—®é¢˜
let _guard = graph.no_grad_guard();  // å€Ÿç”¨äº† &mut graph
graph.forward_node(output)?;         // âŒ æ— æ³•å†å€Ÿç”¨ graphï¼
```

#### é—­åŒ…å½¢å¼çš„ä¼˜åŠ¿

| æ–¹é¢ | é—­åŒ…å½¢å¼ | Guard å½¢å¼ |
|------|----------|------------|
| ä½œç”¨åŸŸæ§åˆ¶ | âœ… è‡ªåŠ¨ã€æ˜ç¡® | âš ï¸ ä¾èµ–å˜é‡ç”Ÿå‘½å‘¨æœŸ |
| çŠ¶æ€æ¢å¤ | âœ… ä¿è¯æ¢å¤ | âš ï¸ éœ€æ­£ç¡®æŒæœ‰ guard |
| Rust é£æ ¼ | âœ… æ›´ç¬¦åˆ RAII | âš ï¸ éœ€é¢å¤–æ³¨æ„ |
| å€Ÿç”¨å®‰å…¨ | âœ… é—­åŒ…å†… `&mut` æ¸…æ™° | âŒ ä¸å›¾ç»‘å®šæ¶æ„å†²çª |

#### ä½•æ—¶è€ƒè™‘å¼•å…¥ Guard å½¢å¼

å½“æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ä¹‹ä¸€æ—¶ï¼Œå¯è€ƒè™‘å¼•å…¥ï¼š

1. **æ¶æ„æ¼”è¿›ä¸ºå…¨å±€çŠ¶æ€æ¨¡å¼**ï¼šå¦‚æœæœªæ¥é¡¹ç›®é‡‡ç”¨ç±»ä¼¼ PyTorch çš„å…¨å±€/çº¿ç¨‹å±€éƒ¨çŠ¶æ€ç®¡ç†æ¢¯åº¦å¼€å…³ï¼ˆè€Œéç»‘å®šåˆ° `Graph` å®ä¾‹ï¼‰ï¼ŒGuard å½¢å¼å°†è‡ªç„¶é€‚é…

2. **å¤šå›¾ååŒåœºæ™¯**ï¼šè‹¥éœ€è¦è·¨å¤šä¸ª `Graph` å®ä¾‹ç»Ÿä¸€ç¦ç”¨æ¢¯åº¦ï¼Œå…¨å±€ Guard ä¼šæ¯”é€ä¸ªè°ƒç”¨ `no_grad_scope` æ›´ä¾¿æ·

3. **ä¸å¤–éƒ¨ FFI é›†æˆ**ï¼šè‹¥éœ€è¦åœ¨ C/FFI è¾¹ç•Œæ§åˆ¶æ¢¯åº¦çŠ¶æ€ï¼ŒGuard æ¨¡å¼å¯èƒ½æ›´é€‚åˆ

#### å½“å‰ç»“è®º

**é—­åŒ…å½¢å¼ `no_grad_scope` å·²è¶³å¤Ÿæ»¡è¶³éœ€æ±‚**ï¼Œä¸”æ›´ç¬¦åˆ Rust çš„å€Ÿç”¨è§„åˆ™å’Œ RAII åŸåˆ™ã€‚åœ¨å½“å‰å›¾ç»‘å®šæ¶æ„ä¸‹ï¼Œè¿™æ˜¯æ›´å®‰å…¨ã€æ›´è‡ªç„¶çš„é€‰æ‹©ã€‚

### 1.7 no_grad ä¸­è°ƒç”¨ backward çš„è­¦å‘Šæœºåˆ¶

#### ä¸ PyTorch çš„è¡Œä¸ºå·®å¼‚

| æ¡†æ¶ | no_grad å†…è°ƒç”¨ backward | åŸå›  |
|------|------------------------|------|
| PyTorch | âŒ **è¿è¡Œæ—¶é”™è¯¯** | åŠ¨æ€å›¾ï¼šno_grad å†…åˆ›å»ºçš„å¼ é‡æ—  `grad_fn`ï¼Œæ— æ³•å›æº¯ |
| only_torch | âš ï¸ **è­¦å‘Šä½†å…è®¸** | é™æ€å›¾ï¼šå›¾åœ¨èŠ‚ç‚¹åˆ›å»ºæ—¶å·²æ„å»ºï¼Œbackward æŠ€æœ¯ä¸Šå¯è¡Œ |

#### ä¸ºä½•ä¸é˜»æ­¢è€Œæ˜¯è­¦å‘Š

1. **æ¶æ„æœ¬è´¨ä¸åŒ**ï¼šPyTorch çš„é”™è¯¯æ˜¯åŠ¨æ€å›¾çš„è‡ªç„¶ç»“æœï¼Œè€Œéæ˜¾å¼æ£€æŸ¥ã€‚only_torch è‹¥è¦é˜»æ­¢éœ€äººä¸ºæ·»åŠ é™åˆ¶ã€‚

2. **å­˜åœ¨åˆæ³•ç”¨ä¾‹**ï¼ˆçº¦ 20%ï¼‰ï¼š
   ```rust
   // è°ƒè¯•åœºæ™¯ï¼šåœ¨è¯„ä¼°æ—¶æŸ¥çœ‹æ¢¯åº¦ä¿¡æ¯
   graph.no_grad_scope(|g| {
       g.forward_node(output)?;
       g.backward_nodes(&[w], output)?;
       println!("Debug grad: {:?}", g.get_node_jacobi(w));
       Ok(())
   });
   ```

3. **å¤§å¤šæ•°æƒ…å†µæ˜¯è¯¯ç”¨**ï¼ˆçº¦ 80%ï¼‰ï¼šç”¨æˆ·å¯èƒ½å¿˜è®°åœ¨è®­ç»ƒæ¨¡å¼ä¸‹è°ƒç”¨ backwardã€‚

#### å®ç°

åœ¨ `backward_nodes_ex` å’Œ `backward_batch` å¼€å¤´æ·»åŠ è­¦å‘Šï¼š

```rust
if !self.is_train_mode() {
    eprintln!(
        "[only_torch è­¦å‘Š] åœ¨ no_grad/eval æ¨¡å¼ä¸‹è°ƒç”¨ backwardï¼Œè¿™é€šå¸¸æ˜¯è¯¯ç”¨ã€‚\
        å¦‚ç¡®éœ€æ­¤è¡Œä¸ºï¼Œè¯·å¿½ç•¥æ­¤è­¦å‘Šã€‚"
    );
}
```

#### å¯¹ç…§æµ‹è¯•

- Rust æµ‹è¯•: `test_no_grad_scope_backward_still_works`
- PyTorch å¯¹ç…§: `tests/calc_jacobi_by_pytorch/no_grad_scope_behavior.py`

---

## 2. detach æœºåˆ¶

### 2.1 è®¾è®¡ç›®æ ‡

- **é€‰æ‹©æ€§æ¢¯åº¦æˆªæ–­**ï¼šåªé˜»æ­¢ç‰¹å®šè·¯å¾„çš„æ¢¯åº¦æµï¼Œå…¶ä»–è·¯å¾„æ­£å¸¸
- **æ”¯æŒé«˜çº§è®­ç»ƒæ¨¡å¼**ï¼šGANã€Actor-Critic ç­‰éœ€è¦ç²¾ç»†æ§åˆ¶æ¢¯åº¦æµå‘

### 2.2 API è®¾è®¡

```rust
impl Graph {
    /// å°†èŠ‚ç‚¹æ ‡è®°ä¸º detachedï¼Œé˜»æ­¢æ¢¯åº¦å›æµåˆ°å…¶çˆ¶èŠ‚ç‚¹
    pub fn detach_node(&mut self, node_id: NodeId) -> Result<(), GraphError> {
        self.get_node_mut(node_id)?.set_detached(true);
        Ok(())
    }

    /// å–æ¶ˆ detach çŠ¶æ€
    pub fn attach_node(&mut self, node_id: NodeId) -> Result<(), GraphError> {
        self.get_node_mut(node_id)?.set_detached(false);
        Ok(())
    }

    /// æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦è¢« detach
    pub fn is_node_detached(&self, node_id: NodeId) -> Result<bool, GraphError> {
        Ok(self.get_node(node_id)?.is_detached())
    }
}

// NodeHandle æ‰©å±•
impl NodeHandle {
    pub fn is_detached(&self) -> bool {
        self.is_detached
    }

    pub fn set_detached(&mut self, detached: bool) {
        self.is_detached = detached;
    }
}
```

### 2.3 å®ç°æ–¹æ¡ˆ

åœ¨ç°æœ‰ `pass_id` æœºåˆ¶ä¸‹å®ç°ï¼Œä¿®æ”¹ `backward_node_internal`ï¼š

```rust
fn backward_node_internal(
    &mut self,
    target_node_id: NodeId,
    result_node_id: NodeId,
) -> Result<(), GraphError> {
    let target_node = self.get_node(target_node_id)?;

    // ğŸ†• æ£€æŸ¥ detach çŠ¶æ€
    if target_node.is_detached() {
        // è§†ä¸ºå¶å­èŠ‚ç‚¹ï¼Œä¸å‘çˆ¶èŠ‚ç‚¹ä¼ æ’­æ¢¯åº¦
        // jacobi ä¸è®¾ç½®ï¼ˆä¿æŒ Noneï¼‰
        return Ok(());
    }

    // åŸæœ‰é€»è¾‘ä¿æŒä¸å˜...
    let parents_ids = self.get_node_parents(target_node_id)?;
    for parent_id in &parents_ids {
        self.backward_node_internal(*parent_id, result_node_id)?;
    }
    // ...
}
```

### 2.4 PyTorch è¯­ä¹‰å…¼å®¹æ€§

**å…³é”®è¡Œä¸º**ï¼šå½“èŠ‚ç‚¹è¢« detach åï¼Œå…¶ä¸Šæ¸¸å‚æ•°èŠ‚ç‚¹çš„ jacobi åº”ä¸º `None`ï¼Œè€Œéé›¶å¼ é‡ã€‚

```
ç½‘ç»œ: x â†’ w1 â†’ h(detached) â†’ w2 â†’ y

backward(y) å:
- w2.jacobi = Some(æ­£å¸¸æ¢¯åº¦)
- h.jacobi = None (è¢« detach)
- w1.jacobi = None (æ¢¯åº¦è¢« h é˜»æ–­ï¼Œç¬¦åˆ PyTorch è¯­ä¹‰)
```

å®ç°ç»†èŠ‚ï¼š
- è‹¥ç›®æ ‡èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹éƒ½æ—  jacobiï¼ˆå›  detach å¯¼è‡´ï¼‰ï¼Œåˆ™æ¸…é™¤è¯¥èŠ‚ç‚¹çš„ jacobi
- è¿™ç¡®ä¿äº†è¢« detach é˜»æ–­çš„ä¸Šæ¸¸èŠ‚ç‚¹ä¸ä¼šæ®‹ç•™é›¶æ¢¯åº¦
```

### 2.4 ä½¿ç”¨ç¤ºä¾‹

#### GAN è®­ç»ƒ

```rust
// è®­ç»ƒåˆ¤åˆ«å™¨
let fake = graph.forward_node(generator_output)?;
graph.detach_node(fake)?;  // é˜²æ­¢ D çš„ loss æ›´æ–° G
let d_fake = graph.forward_node(discriminator_on_fake)?;
graph.backward_nodes(&[d_weights], d_loss)?;

// è®­ç»ƒç”Ÿæˆå™¨
graph.attach_node(fake)?;  // æ¢å¤æ¢¯åº¦æµ
graph.backward_nodes(&[g_weights], g_loss)?;
```

#### Actor-Critic (å¼ºåŒ–å­¦ä¹ )

```rust
// Critic çš„ value ä¼°è®¡ä¼ ç»™ Actor æ—¶éœ€è¦ detach
let value = graph.forward_node(critic_output)?;
graph.detach_node(value)?;  // Actor çš„ loss ä¸åº”æ›´æ–° Critic
let advantage = compute_advantage(reward, value);
// ... è®¡ç®— actor_loss ...
graph.backward_nodes(&[actor_weights], actor_loss)?;
```

### 2.5 ä¸ `value_version` æœºåˆ¶çš„å…³ç³»

å½’æ¡£æ–‡æ¡£ `graph_execution_refactor.md` æè®®ç”¨ `value_version` æ›¿ä»£ `pass_id`ï¼Œå¹¶å£°ç§°å¯¹ `detach` æ›´å‹å¥½ã€‚

**ç»“è®º**ï¼š`detach` åœ¨å½“å‰ `pass_id` æœºåˆ¶ä¸‹**å®Œå…¨å¯å®ç°**ï¼Œä¸¤ç§æœºåˆ¶åœ¨åŠŸèƒ½ä¸Šç­‰ä»·ï¼š

| å®ç°æ–¹å¼ | detach å¤„ç† |
|----------|-------------|
| `pass_id` + é€’å½’ | é€’å½’æ—¶æ£€æŸ¥ `is_detached` flagï¼Œé‡åˆ°åˆ™åœæ­¢ |
| `value_version` + æ‹“æ‰‘æ’åº | æ„å»ºåå‘å­å›¾æ—¶æ’é™¤ detached åˆ†æ”¯ |

---

## 3. retain_graph æœºåˆ¶

### 3.1 è®¾è®¡ç›®æ ‡

- **æ”¯æŒå¤šæ¬¡åå‘ä¼ æ’­**ï¼šå¤šä¸ª Loss å…±äº«è®¡ç®—è·¯å¾„æ—¶å¿…éœ€
- **æ”¯æŒé«˜é˜¶å¯¼æ•°**ï¼šè®¡ç®—æ¢¯åº¦çš„æ¢¯åº¦éœ€è¦ä¿ç•™è®¡ç®—å›¾
- **å†…å­˜æ§åˆ¶**ï¼šé»˜è®¤é‡Šæ”¾ä»¥èŠ‚çœå†…å­˜ï¼Œéœ€è¦æ—¶æ˜¾å¼ä¿ç•™

### 3.2 API è®¾è®¡

```rust
impl Graph {
    /// åå‘ä¼ æ’­ï¼ˆæ‰©å±•ç‰ˆæœ¬ï¼‰
    pub fn backward_nodes_ex(
        &mut self,
        target_nodes: &[NodeId],
        result_node_id: NodeId,
        retain_graph: bool,
    ) -> Result<(), GraphError> {
        // æ‰§è¡Œåå‘ä¼ æ’­...
        self.backward_nodes_internal(target_nodes, result_node_id)?;

        if !retain_graph {
            // é‡Šæ”¾ä¸­é—´è®¡ç®—å€¼ä»¥èŠ‚çœå†…å­˜
            // ä¿ç•™å¶å­èŠ‚ç‚¹ï¼ˆInput/Parameterï¼‰çš„å€¼
            self.release_intermediate_values()?;
        }
        Ok(())
    }

    /// ç®€åŒ–ç‰ˆæœ¬ï¼Œé»˜è®¤ retain_graph = falseï¼ˆä¸ PyTorch ä¸€è‡´ï¼‰
    pub fn backward_nodes(
        &mut self,
        target_nodes: &[NodeId],
        result_node_id: NodeId,
    ) -> Result<(), GraphError> {
        self.backward_nodes_ex(target_nodes, result_node_id, false)
    }
}
```

### 3.3 å¿…é¡»ä½¿ç”¨ retain_graph çš„åœºæ™¯

#### åœºæ™¯ 1ï¼šå¤š Loss å…±äº«è®¡ç®—è·¯å¾„

```rust
// å¤šä»»åŠ¡å­¦ä¹ 
let features = graph.forward_node(backbone_output)?;
let cls_loss = graph.forward_node(classification_loss)?;
let reg_loss = graph.forward_node(regression_loss)?;

// ç¬¬ä¸€ä¸ª loss backwardï¼Œä¿ç•™å›¾
graph.backward_nodes_ex(&[cls_weights], cls_loss, true)?;
// ç¬¬äºŒä¸ª loss backward
graph.backward_nodes_ex(&[reg_weights], reg_loss, false)?;
```

#### åœºæ™¯ 2ï¼šå¼ºåŒ–å­¦ä¹ å¤šè¾“å‡ºæ¨¡å‹ï¼ˆActor-Criticï¼‰

> **æ³¨æ„**ï¼šActor-Critic æœ¬è´¨ä¸Šæ˜¯å¤šä»»åŠ¡å­¦ä¹ çš„ä¸€ç§å½¢å¼ï¼Œç»“æ„ä¸åœºæ™¯ 1 ç›¸åŒã€‚

```rust
// Actor-Critic å…±äº« backboneï¼ˆä¸å¤šä»»åŠ¡å­¦ä¹ ç»“æ„ç›¸åŒï¼‰
let (actor_out, critic_out) = forward_shared_model(&mut graph)?;

let actor_loss = compute_actor_loss(actor_out, actions, advantages);
let critic_loss = compute_critic_loss(critic_out, returns);

// ä¸¤ä¸ª loss éƒ½éœ€è¦ backward
graph.backward_nodes_ex(&[actor_params], actor_loss, true)?;
graph.backward_nodes_ex(&[critic_params], critic_loss, false)?;
```

#### åœºæ™¯ 3ï¼šé«˜é˜¶å¯¼æ•°

```rust
// è®¡ç®— Hessianï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰
// éœ€è¦ä¿ç•™ä¸€é˜¶æ¢¯åº¦çš„è®¡ç®—å›¾
```

### 3.4 å†…å­˜è€ƒè™‘

| retain_graph | è¡Œä¸º | å†…å­˜ |
|--------------|------|------|
| `false`ï¼ˆé»˜è®¤ï¼‰ | backward åé‡Šæ”¾ä¸­é—´å€¼ | ä½ |
| `true` | ä¿ç•™æ‰€æœ‰ä¸­é—´å€¼ | é«˜ |

---

## 4. ç»„åˆä½¿ç”¨æ¨¡å¼

### 4.1 GAN è®­ç»ƒå®Œæ•´ç¤ºä¾‹

```rust
for epoch in 0..epochs {
    // === è®­ç»ƒåˆ¤åˆ«å™¨ ===
    // çœŸå®æ ·æœ¬
    let d_real = graph.forward_node(discriminator_on_real)?;

    // ç”Ÿæˆæ ·æœ¬ï¼ˆdetach é˜²æ­¢æ›´æ–°ç”Ÿæˆå™¨ï¼‰
    let fake = graph.forward_node(generator_output)?;
    graph.detach_node(fake)?;
    let d_fake = graph.forward_node(discriminator_on_fake)?;

    let d_loss = compute_d_loss(d_real, d_fake);
    graph.backward_nodes(&[d_weights], d_loss)?;
    d_optimizer.step(&mut graph)?;
    graph.clear_jacobi()?;

    // === è®­ç»ƒç”Ÿæˆå™¨ ===
    graph.attach_node(fake)?;  // æ¢å¤æ¢¯åº¦æµ
    let g_loss = compute_g_loss(d_fake);
    graph.backward_nodes(&[g_weights], g_loss)?;
    g_optimizer.step(&mut graph)?;
    graph.clear_jacobi()?;
}
```

### 4.2 Actor-Critic (PPO é£æ ¼)

```rust
for epoch in 0..epochs {
    // æ”¶é›†ç»éªŒæ—¶ä½¿ç”¨ no_grad
    let trajectories = graph.no_grad_scope(|g| {
        collect_trajectories(g, env)
    })?;

    // è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆCritic è¾“å‡º detachï¼‰
    let values = graph.forward_node(critic_output)?;
    graph.detach_node(values)?;
    let advantages = compute_gae(rewards, values);

    // å¤šæ¬¡ PPO æ›´æ–°
    for _ in 0..ppo_epochs {
        let actor_loss = compute_ppo_loss(actions, advantages);
        let critic_loss = compute_value_loss(values, returns);

        // ä¸¤ä¸ª loss å…±äº« backboneï¼Œéœ€è¦ retain_graph
        graph.backward_nodes_ex(&[actor_params], actor_loss, true)?;
        graph.backward_nodes_ex(&[critic_params], critic_loss, false)?;

        optimizer.step(&mut graph)?;
        graph.clear_jacobi()?;
    }
}
```

### 4.3 å¤šä»»åŠ¡å­¦ä¹ 

```rust
// å…±äº« backbone çš„å¤šä»»åŠ¡æ¨¡å‹
let features = graph.forward_node(shared_backbone)?;

// ä»»åŠ¡ 1ï¼šåˆ†ç±»
let cls_out = graph.forward_node(classification_head)?;
let cls_loss = graph.forward_node(ce_loss)?;

// ä»»åŠ¡ 2ï¼šæ£€æµ‹
let det_out = graph.forward_node(detection_head)?;
let det_loss = graph.forward_node(detection_loss)?;

// åå‘ä¼ æ’­ï¼ˆæ³¨æ„ retain_graphï¼‰
graph.backward_nodes_ex(&[backbone, cls_head], cls_loss, true)?;
graph.backward_nodes_ex(&[backbone, det_head], det_loss, false)?;

optimizer.step(&mut graph)?;
graph.clear_jacobi()?;
```

---

## 5. å®ç°ä¼˜å…ˆçº§

| åŠŸèƒ½ | ä¼˜å…ˆçº§ | ä¾èµ– | è§¦å‘æ¡ä»¶ |
|------|--------|------|----------|
| `no_grad` / eval mode å¢å¼º | é«˜ | ç°æœ‰ `is_train_mode` | æ¨ç†/è¯„ä¼°éœ€æ±‚ |
| `detach` | ä¸­ | `pass_id` æœºåˆ¶ | GAN/RL ç¤ºä¾‹ |
| `retain_graph` | ä¸­ | backward å®ç° | å¤š Loss åœºæ™¯ |

---

## 6. ä¸å…¶ä»–æ–‡æ¡£çš„å…³ç³»

| æ–‡æ¡£ | å…³æ³¨ç‚¹ |
|------|--------|
| **æœ¬æ–‡æ¡£** | ç”¨æˆ·çº§æ¢¯åº¦æµæ§åˆ¶ API |
| `gradient_clear_and_accumulation_design.md` | è®­ç»ƒå¾ªç¯ä¸­çš„æ¢¯åº¦ç´¯ç§¯å’Œæ¸…é™¤æ—¶æœº |
| `graph_execution_refactor.md`ï¼ˆå·²å½’æ¡£ï¼‰ | åº•å±‚æ‰§è¡Œæœºåˆ¶ï¼ˆpass_id vs value_versionï¼‰ |

---

## 7. å®ç°æ³¨æ„äº‹é¡¹

### 7.1 å¤šæ¬¡ forward åçš„ backward

åœ¨å¤šä»»åŠ¡å­¦ä¹ åœºæ™¯ä¸­ï¼Œå¯èƒ½éœ€è¦å¤šæ¬¡è°ƒç”¨ `forward_node`ï¼š

```rust
graph.forward_node(out1)?;  // forward_pass_id = 1
graph.forward_node(out2)?;  // forward_pass_id = 2
```

**å…³é”®å®ç°ç»†èŠ‚**ï¼šåœ¨ backward æ—¶ï¼Œä¸åº”ä¸¥æ ¼æ£€æŸ¥èŠ‚ç‚¹çš„ `forward_pass_id` æ˜¯å¦ç­‰äºå›¾çš„å½“å‰ `last_forward_pass_id`ã€‚è¿™ä¼šå¯¼è‡´åœ¨å¤šæ¬¡ forward åï¼Œæ—©æœŸ forward çš„èŠ‚ç‚¹è¢«é”™è¯¯è·³è¿‡ã€‚

æ­£ç¡®åšæ³•ï¼šåªè·³è¿‡**ä»æœª forward è¿‡**çš„èŠ‚ç‚¹ï¼ˆ`forward_pass_id == 0`ï¼‰ï¼Œè€Œé id ä¸åŒ¹é…çš„èŠ‚ç‚¹ã€‚

### 7.2 æ¢¯åº¦ç´¯ç§¯è¯­ä¹‰ï¼ˆPyTorch å…¼å®¹ï¼‰

å¤šæ¬¡ backward æ—¶ï¼Œæ¢¯åº¦ç´¯ç§¯éµå¾ª PyTorch è¯­ä¹‰ï¼š

| èŠ‚ç‚¹ç±»å‹ | è¡Œä¸º | è¯´æ˜ |
|----------|------|------|
| **å‚æ•°èŠ‚ç‚¹** | jacobi **ç´¯ç§¯** | æ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼ˆå¦‚å¤šä»»åŠ¡å­¦ä¹ ã€å¤§ batch æ¨¡æ‹Ÿï¼‰ |
| **ä¸­é—´èŠ‚ç‚¹** | jacobi **é‡æ–°è®¡ç®—** | æ¯æ¬¡ backward ç‹¬ç«‹è®¡ç®—ï¼Œä¸ç´¯ç§¯ |

#### æ ¸å¿ƒæœºåˆ¶ï¼šä¼ æ’­ä¿¡å· vs ç´¯åŠ å™¨

ç†è§£å¤šæ¬¡ backward çš„å…³é”®æ˜¯åŒºåˆ†ä¸¤ç§ä¸åŒç”¨é€”çš„æ¢¯åº¦ï¼š

| æ¦‚å¿µ | ç”¨é€” | æ˜¯å¦è·¨ backward ç´¯ç§¯ |
|------|------|---------------------|
| **ä¼ æ’­ä¿¡å·**ï¼ˆupstream gradï¼‰ | é“¾å¼æ³•åˆ™å‘ä¸Šä¼ é€’ | âŒ å¿…é¡»æ˜¯æœ¬æ¬¡ backward æ–°ç®—çš„ |
| **å‚æ•°ç´¯åŠ å™¨**ï¼ˆparam.jacobiï¼‰ | ä¼˜åŒ–å™¨æ›´æ–°ç”¨ | âœ… è·¨ backward ç´¯ç§¯ |

**å…³é”®è§„åˆ™**ï¼š
1. æ¯æ¬¡ backward éƒ½ä» scratch è®¡ç®—ä¸€æ¡"æœ¬æ¬¡æ¢¯åº¦æµ"ï¼ˆä¼ æ’­ä¿¡å·åªç”¨æœ¬æ¬¡çš„ï¼‰
2. å‚æ•°èŠ‚ç‚¹ç»´æŠ¤ä¸€ä¸ªè·¨ backward çš„ç´¯åŠ å™¨ï¼ˆç”¨äºæœ€ç»ˆæ›´æ–°ï¼‰
3. éå‚æ•°èŠ‚ç‚¹ä¸ç»´æŠ¤è·¨ backward çš„ç´¯åŠ å™¨ï¼ˆé»˜è®¤ï¼‰ï¼Œå› ä¸ºå®ƒä¸æ˜¯è¦æ›´æ–°çš„çŠ¶æ€
4. âš ï¸ **é“¾å¼æ³•åˆ™ä¼ æ’­å¿…é¡»ä½¿ç”¨"æœ¬æ¬¡æ–°ç®—çš„æ¢¯åº¦"ï¼Œè€Œéä»»ä½•ç´¯ç§¯åçš„å€¼**ï¼ˆå¦åˆ™ä¼š double countï¼‰

**è§„åˆ™ 4 çš„é‡è¦è¡¥å……**ï¼šå³ä½¿ä¸‹æ¸¸èŠ‚ç‚¹ä¹Ÿæ˜¯éœ€è¦ç´¯ç§¯æ¢¯åº¦çš„å‚æ•°èŠ‚ç‚¹ï¼Œåœ¨è®¡ç®—ä¸Šæ¸¸èŠ‚ç‚¹çš„æ¢¯åº¦æ—¶ï¼Œä¹Ÿå¿…é¡»ä½¿ç”¨ä¸‹æ¸¸èŠ‚ç‚¹**æœ¬æ¬¡ backward æ–°ç®—çš„è´¡çŒ®**ï¼Œè€Œéå…¶ç´¯åŠ å™¨ä¸­çš„ç´¯ç§¯å€¼ã€‚

```
å‡è®¾å­˜åœ¨æ‹“æ‰‘ï¼šu(param) â†’ w(param) â†’ out

ç¬¬ 1 æ¬¡ backward:
  w.jacobi = âˆ‚L1/âˆ‚w
  u.jacobi = âˆ‚L1/âˆ‚w Ã— âˆ‚w/âˆ‚u  â† ä½¿ç”¨æœ¬æ¬¡æ–°ç®—çš„ âˆ‚L1/âˆ‚w

ç¬¬ 2 æ¬¡ backward:
  w.jacobi += âˆ‚L2/âˆ‚w  â†’ ç´¯ç§¯å = âˆ‚L1/âˆ‚w + âˆ‚L2/âˆ‚w
  u.jacobi += âˆ‚L2/âˆ‚w Ã— âˆ‚w/âˆ‚u  â† å¿…é¡»ä½¿ç”¨æœ¬æ¬¡æ–°ç®—çš„ âˆ‚L2/âˆ‚wï¼Œä¸èƒ½ç”¨ç´¯ç§¯åçš„ï¼

æ­£ç¡®ç»“æœï¼šu.jacobi = (âˆ‚L1/âˆ‚w + âˆ‚L2/âˆ‚w) Ã— âˆ‚w/âˆ‚u = âˆ‚(L1+L2)/âˆ‚u âœ“
é”™è¯¯ç»“æœï¼ˆè‹¥ç”¨ç´¯ç§¯å€¼ï¼‰ï¼šu.jacobi = âˆ‚L1/âˆ‚wÃ—âˆ‚w/âˆ‚u + (âˆ‚L1/âˆ‚w+âˆ‚L2/âˆ‚w)Ã—âˆ‚w/âˆ‚u
                                = 2Ã—âˆ‚L1/âˆ‚wÃ—âˆ‚w/âˆ‚u + âˆ‚L2/âˆ‚wÃ—âˆ‚w/âˆ‚u âœ— (L1 è¢«ç®—äº†ä¸¤æ¬¡)
```

#### ä¸ºä»€ä¹ˆä¸­é—´èŠ‚ç‚¹ä¸ç´¯ç§¯ä¸å½±å“å‚æ•°çš„æ­£ç¡®æ€§ï¼Ÿ

```
å¤šä»»åŠ¡å­¦ä¹ ç¤ºä¾‹ï¼š
  x â†’ w_shared â†’ features â†’ w1 â†’ out1 (Loss1)
                    â””â”€â”€â”€â”€â†’ w2 â†’ out2 (Loss2)
```

æ•°å­¦ä¸Šï¼Œæ¯æ¬¡ backward è®¡ç®—çš„æ˜¯**ç‹¬ç«‹çš„æ¢¯åº¦æµ**ï¼š

```
ç¬¬ 1 æ¬¡ backward(out1):
  features.jacobi = âˆ‚L1/âˆ‚features  â† æœ¬æ¬¡æ–°ç®—
  w_shared.jacobi = âˆ‚L1/âˆ‚w_shared  â† ä½¿ç”¨ä¸Šé¢çš„ features.jacobi

ç¬¬ 2 æ¬¡ backward(out2):
  features.jacobi = âˆ‚L2/âˆ‚features  â† æœ¬æ¬¡æ–°ç®—ï¼ˆä¸ä¾èµ–ç¬¬ 1 æ¬¡çš„å€¼ï¼ï¼‰
  w_shared.jacobi += âˆ‚L2/âˆ‚w_shared â† ç´¯ç§¯åˆ°å‚æ•°
```

**å…³é”®æ´å¯Ÿ**ï¼šè®¡ç®— `w_shared` çš„æ¢¯åº¦æ—¶ï¼Œåªéœ€è¦**å½“å‰è¿™æ¬¡ backward** ç®—å‡ºæ¥çš„ `âˆ‚L/âˆ‚features`ï¼Œä¸éœ€è¦ä¸Šä¸€æ¬¡ backward ç•™ä¸‹æ¥çš„å€¼ã€‚æ‰€ä»¥æ¸…é™¤ä¸­é—´èŠ‚ç‚¹çš„ jacobi ä¸ä¼šå½±å“å‚æ•°çš„ç´¯ç§¯æ­£ç¡®æ€§ã€‚

ä»"è´£ä»»"çš„è§’åº¦ç†è§£ï¼š
- **å‚æ•°èŠ‚ç‚¹**ï¼šéœ€è¦çŸ¥é“"æˆ‘å¯¹æ‰€æœ‰ loss è´Ÿå¤šå°‘è´£ä»»" â†’ ç´¯ç§¯
- **ä¸­é—´èŠ‚ç‚¹**ï¼šåªæ˜¯ä¼ é€’æ¢¯åº¦çš„"ç®¡é“"ï¼Œæ¯æ¬¡ backward å¯è§†ä¸ºæ¦‚å¿µä¸Šä¸åŒçš„è·¯å¾„ â†’ ä¸ç´¯ç§¯

#### ç¤ºä¾‹

```
backward(out1, retain_graph=True):
  - w_shared.jacobi = [1,2,3,4,...]  âœ“ ä¿ç•™ï¼ˆç´¯åŠ å™¨ï¼‰
  - features.jacobi = [[1],[1]]      æœ¬æ¬¡ä¼ æ’­ä¿¡å·

backward(out2):
  - w_shared.jacobi = [2,4,6,8,...]  ç´¯ç§¯ = task1 + task2
  - features.jacobi = [[1],[1]]      æœ¬æ¬¡ä¼ æ’­ä¿¡å·ï¼ˆé‡æ–°è®¡ç®—ï¼Œä¸æ˜¯ç´¯ç§¯ï¼ï¼‰
```

#### å®ç°ç»†èŠ‚

**backward å¼€å§‹æ—¶**ï¼šè°ƒç”¨ `reset_intermediate_jacobi()` æ¸…é™¤ä¸­é—´èŠ‚ç‚¹çš„ jacobiï¼Œåªä¿ç•™å‚æ•°èŠ‚ç‚¹çš„ jacobiã€‚è¿™ç¡®ä¿ï¼š
1. ä¼ æ’­ä¿¡å·å§‹ç»ˆæ˜¯"æœ¬æ¬¡æ–°ç®—çš„"
2. å‚æ•°ç´¯åŠ å™¨æ­£ç¡®ç´¯ç§¯å¤šæ¬¡ backward çš„è´¡çŒ®

**backward ç»“æŸæ—¶ï¼ˆ`retain_graph=false`ï¼‰**ï¼šè°ƒç”¨ `release_intermediate_results()` åŒæ—¶é‡Šæ”¾ä¸­é—´èŠ‚ç‚¹çš„**å€¼å’Œæ¢¯åº¦**ï¼š
- å€¼è¢«é‡Šæ”¾ï¼šéœ€è¦é‡æ–° forward æ‰èƒ½å†æ¬¡ backward
- æ¢¯åº¦ä¹Ÿè¢«é‡Šæ”¾ï¼šä¿æŒä¸€è‡´æ€§ï¼Œé¿å…ç”¨æˆ·è¯¯ä»¥ä¸ºä¸­é—´èŠ‚ç‚¹çš„æ¢¯åº¦æ˜¯ç´¯ç§¯çš„

è¿™æ›´æ¥è¿‘ PyTorch çš„è¯­ä¹‰ï¼šä¸­é—´èŠ‚ç‚¹çš„æ¢¯åº¦é»˜è®¤ä¸ä¿ç•™ï¼ˆé™¤éæ˜¾å¼è°ƒç”¨ `retain_grad()`ï¼‰ã€‚

è‹¥éœ€è¦é˜»æ­¢å‚æ•°èŠ‚ç‚¹çš„æ¢¯åº¦ç´¯ç§¯ï¼Œåº”åœ¨ backward ä¹‹é—´è°ƒç”¨ `clear_jacobi()`ã€‚

### 7.3 ä¸ºä½•ä¸å¼•å…¥ `retain_grad` åŠŸèƒ½

PyTorch æä¾›äº† `retain_grad()` æ–¹æ³•ï¼Œå…è®¸ä¸­é—´èŠ‚ç‚¹ï¼ˆéå¶å­èŠ‚ç‚¹ï¼‰åœ¨å¤šæ¬¡ backward æ—¶ç´¯ç§¯æ¢¯åº¦ã€‚ç»è¿‡å¯¹ä¸»æµæ¡†æ¶çš„è°ƒç ”ï¼Œæˆ‘ä»¬å†³å®š**æš‚ä¸å¼•å…¥**æ­¤åŠŸèƒ½ã€‚

#### å„æ¡†æ¶å¯¹ä¸­é—´èŠ‚ç‚¹æ¢¯åº¦çš„å¤„ç†

| æ¡†æ¶ | è®¾è®¡æ¨¡å¼ | ä¸­é—´èŠ‚ç‚¹æ¢¯åº¦ | ç±»ä¼¼ `retain_grad`? |
|------|----------|--------------|---------------------|
| **PyTorch** | åŠ¨æ€å›¾ + å¶å­èŠ‚ç‚¹åŒºåˆ† | é»˜è®¤ä¸ä¿ç•™ï¼Œéœ€æ˜¾å¼ `retain_grad()` | âœ… æœ‰ |
| **JAX** | çº¯å‡½æ•°å¼ | **æ ¹æœ¬ä¸æš´éœ²**ï¼ˆåªè¿”å›è¾“å…¥å‚æ•°çš„æ¢¯åº¦ï¼‰ | âŒ æ— æ­¤æ¦‚å¿µ |
| **TensorFlow/Keras** | GradientTape + watch | åªè®¡ç®—æ˜¾å¼ `watch()` çš„å˜é‡ | âŒ æ—  |
| **MXNet** | `attach_grad()` æ˜¾å¼å£°æ˜ | åªè®¡ç®— `attach_grad()` çš„å˜é‡ | âŒ æ—  |

#### ä¸å¼•å…¥çš„ç†ç”±

1. **å†…å­˜æ•ˆç‡**ï¼šä¸­é—´ç‰¹å¾ï¼ˆå¦‚ CNN çš„ feature mapï¼‰å¯èƒ½éå¸¸å¤§ï¼Œé»˜è®¤ä¿ç•™æ‰€æœ‰æ¢¯åº¦ä¼šæ˜¾è‘—å¢åŠ å†…å­˜å ç”¨
2. **å®ç”¨æ€§ä½**ï¼š99% çš„è®­ç»ƒåœºæ™¯åªéœ€è¦å‚æ•°æ¢¯åº¦ï¼Œ`retain_grad` ä¸»è¦ç”¨äºè°ƒè¯•å’Œç ”ç©¶
3. **å½“å‰èƒ½åŠ›å·²è¶³å¤Ÿ**ï¼šåœ¨ `backward(..., retain_graph=true)` åã€ä¸‹ä¸€æ¬¡ backward å‰ï¼Œä¸­é—´èŠ‚ç‚¹çš„ jacobi æ˜¯å¯ä»¥è®¿é—®çš„ï¼Œæ»¡è¶³å¤§å¤šæ•°è°ƒè¯•éœ€æ±‚
4. **API ç®€æ´æ€§**ï¼šé¿å…å¼•å…¥é¢å¤–æ¦‚å¿µï¼Œé™ä½ç”¨æˆ·å­¦ä¹ æˆæœ¬
5. **YAGNI åŸåˆ™**ï¼šåœ¨æ²¡æœ‰æ˜ç¡®éœ€æ±‚å‰ï¼Œä¸è¿‡æ—©å¼•å…¥å¤æ‚åŠŸèƒ½

#### å½“å‰çš„è°ƒè¯•æ–¹å¼

```rust
// ç¬¬ä¸€æ¬¡ backward åï¼Œå¯ä»¥ç«‹å³è®¿é—®ä¸­é—´èŠ‚ç‚¹çš„ jacobi
graph.backward_nodes_ex(&[w], output, true)?;

// è¿™ä¸ªæ—¶é—´çª—å£å†…ï¼Œä¸­é—´èŠ‚ç‚¹çš„ jacobi æ˜¯å¯è®¿é—®çš„
let features_jacobi = graph.get_node(features_id)?.jacobi();
println!("ä¸­é—´ç‰¹å¾çš„æ¢¯åº¦: {:?}", features_jacobi);

// ä¸‹ä¸€æ¬¡ backward ä¼šé‡ç½®ä¸­é—´èŠ‚ç‚¹çš„ jacobi
graph.backward_nodes_ex(&[w], output2, false)?;
```

#### æœªæ¥æ‰©å±•

å½“å‰è®¾è®¡ä¸é˜»ç¢æœªæ¥æ·»åŠ  `retain_grad` åŠŸèƒ½ã€‚å¦‚æœç¡®æœ‰éœ€æ±‚ï¼Œå¯ä»¥ï¼š
1. åœ¨èŠ‚ç‚¹ä¸Šæ·»åŠ  `retains_grad` æ ‡å¿—
2. ä¿®æ”¹ `reset_intermediate_jacobi()` è·³è¿‡æ ‡è®°ä¸º `retains_grad` çš„èŠ‚ç‚¹

---

## 8. å‚è€ƒèµ„æ–™

- [PyTorch Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)
- [JAX Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)

### é¡¹ç›®å†…å¯¹ç…§æµ‹è¯•

| Rust æµ‹è¯• | PyTorch å¯¹ç…§è„šæœ¬ |
|-----------|------------------|
| `test_retain_graph_multi_task_learning` | `tests/calc_jacobi_by_pytorch/multi_task_learning_retain_graph.py` |
