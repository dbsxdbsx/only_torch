# 计算图执行机制重构方案

## 1. 背景与动机

当前计算图的执行（前向与反向传播）严重依赖`forward_pass_id`和`backward_pass_id`。这种机制通过为每次传播分配一个唯一的、递增的ID，并将其标记在已计算的节点上，来避免重复计算。（注：`forward_pass_id`和`backward_pass_id`在`Graph`结构体中，用于记录上一次前向/反向传播的ID，以便于在下一次传播时进行检查，避免重复计算。）

虽然此方法简单直观，但存在以下核心问题：

-   **效率低下**：即使计算图中只有一小部分输入发生改变，`pass_id`机制也需要遍历整个计算图，对每个节点进行ID检查。大部分节点的计算虽然能通过ID匹配命中缓存，但遍历和检查本身的开销是巨大的，尤其是在图规模变大时。
-   **可扩展性差**：像“部分前向传播”（只计算图的一个子集）或`detach`（在反向传播中剪掉某个分支）这样的高级功能，在`pass_id`模型下难以优雅地实现。逻辑会变得复杂且容易出错。
-   **耦合度高**：计算的“时机”控制逻辑（`pass_id`检查）与节点的“计算”逻辑本身耦合在了一起，违反了关注点分离原则，使得代码更难维护和扩展。

为了构建一个更现代化、高效且灵活的计算图框架，我们提议移除`pass_id`机制，转向一个基于“反应式更新”和“外部调度”的新架构。

## 2. 核心设计思想

新架构的核心思想是：**从指令式遍览转变为反应式更新**，并通过**版本控制**和**拓扑排序**来实现高效的缓存利用。

-   **反应式更新**：节点的计算不再由外部的`pass_id`强制驱动，而是由其上游输入节点的状态变化来“触发”。如果一个节点的**所有相关输入**都没有变化，它就不需要重新计算。
-   **外部调度**：计算的顺序不再由节点间的递归调用决定，而是由一个中心的`Graph`执行器通过对图进行**拓扑排序**来统一调度。

## 3. 具体实现方案

### 3.1. 前向传播：基于版本号的“脏检查”机制

为了实现高效的反应式更新，我们将为每个节点引入一套版本控制状态，以替代全局的`pass_id`。

#### 3.1.1. 节点的核心状态

每个节点将维护以下三个核心状态：
1.  **`value`**: 节点的缓存值（`Tensor`）---可能是经由父节点计算的（如Add点），也可能是直接被赋值的（如：Input和Parameter节点）。
2.  **`value_version`**: 一个u64整数，代表节点`value`的版本。它**只在**节点成功完成一次重新计算后递增，初始时为0。
3.  **`parents_values_versions`**: 一个字典（或类似结构），精确记录了【上一次计算本节点时，所使用的每一个父节点的`value_version`】。对于从未计算过的节点，此记录为无效状态(即为None)。

#### 3.1.2. 核心流程：递归的“请求-返回”模型

整个过程由一次深度优先的递归调用完成，该调用包含两个阶段：

**阶段一：请求下溯 (Request Downstream / 刨根溯源)**
-   当外部调用 `graph.forward_node(Y)` 时，一个“对值的请求”被创建。
-   这个请求会以深度优先的方式，从 `Y` 递归地传递给它的父节点，再到父节点的父节点，一路“下溯”直到所有相关的祖先节点都被访问。
-   这个下溯过程的**唯一目的**是确保在任何节点开始决策之前，它所依赖的所有上游节点都已经被请求过。

**阶段二：决策与值返回 (Decide and Return Upstream)**
-   当递归的请求到达一个**叶子节点**（如Input或Parameter），或者一个已经完成了对其所有父节点的请求的中间节点时，该节点的返回阶段开始。
-   一个节点 `N` **只有在收到了其所有父节点的返回值之后**，才能开始自己的决策和返回流程。
-   **本地决策**：节点 `N` 会比较它收到的父节点们的当前 `value_version` 和自己 `parents_values_versions` 中记录的旧版本。
    -   如果**全部匹配**，节点 `N` 判定自己是“干净的”。
    -   如果**任何一个不匹配**，节点 `N` 判定自己是“脏的”。
-   **向上返回值**：
    -   如果 `N` 是“干净的”，它**直接将其缓存的 `value`** 和未改变的 `value_version` 返回给调用它的子节点。
    -   如果 `N` 是“脏的”，它会先**执行计算**，更新自己的状态（`value`, `value_version`, `parents_values_versions`），然后**将新计算出的 `value`** 和新的 `value_version` 返回给调用它的子节点。

这个“请求下溯，值返回”的过程确保了在任何节点做决策或计算之前，它所依赖的所有上游信息都已准备就绪，从而以一种优雅的方式实现了拓扑排序和高效缓存。

#### 3.1.3. 示例：`y = wx + b` 的执行过程

为了更清晰地理解，我们以 `y = wx + b` 为例。该表达式在图中被分解为：
-   `Mul_Node = w * x`
-   `Add_Node(y) = Mul_Node + b`

**场景：只有输入 `x` 的值发生了改变。**

1.  **请求 `y`**: `graph.forward_node(y)` 被调用，即向 `Add_Node` 请求其值。

2.  **`Add_Node` 的请求**: `Add_Node` 向其父节点 `Mul_Node` 和 `b` 发出请求。
    -   **请求 `b`**: `b` 是一个参数节点，它的值没有变。它检查自己（没有父节点），发现自己是“干净的”，于是直接从缓存返回值和它的`value_version`。
    -   **请求 `Mul_Node`**: `Add_Node` 向 `Mul_Node` 请求其值。

3.  **`Mul_Node` 的请求**: `Mul_Node` 为了响应请求，向其父节点 `w` 和 `x` 发出请求。
    -   **请求 `w`**: `w` 是参数，值没变，直接返回缓存值和版本号。
    -   **请求 `x`**: `x` 是输入，其值被改变了，`value_version` 已递增。`x` 返回新值和新版本号。

4.  **`Mul_Node` 的决策与计算**:
    -   `Mul_Node` 收到了 `w` 和 `x` 的响应。它进行本地检查，发现 `w` 的版本号匹配，但 `x` 的版本号**不匹配**。
    -   `Mul_Node` 判定自己是“脏的”，必须重新计算。
    -   它使用从 `w`（缓存）和 `x`（新值）获取的值，执行乘法运算。
    -   计算完成后，`Mul_Node` 更新自己的 `value`、`value_version` 和 `parents_values_versions`，然后将**新计算出的值和新版本号**返回给 `Add_Node`。

5.  **`Add_Node` 的决策与计算**:
    -   `Add_Node` 此时收齐了 `b` 和 `Mul_Node` 的响应。
    -   它进行本地检查，发现 `b` 的版本号匹配，但 `Mul_Node` 的版本号**不匹配**。
    -   `Add_Node` 判定自己也是“脏的”，必须重新计算。
    -   它使用从 `b`（缓存）和 `Mul_Node`（新值）获取的值，执行加法运算，得到最终结果 `y`。

这个例子清晰地展示了“检查并返回节点值”的流程如何精准地将计算限制在从 `x` 开始的、受影响的路径上，而 `b` 和 `w` 的昂贵计算（如果是复杂节点）则被完全跳过。

### 3.2. 反向传播：基于拓扑排序的梯度累加

反向传播的逻辑保持不变，但其执行将更加健壮。

1.  **构建拓扑序**:
    -   在启动反向传播之前，`Graph`执行器会对从`result_node`到所有需要计算梯度的`target_node`所构成的子图进行**拓扑排序**。

2.  **检查前向状态**: 在反向传播一个节点之前，可以利用版本号机制检查其`value`是否为最新。如果不是，可以报错或自动执行一次前向计算。

3.  **执行反向传播**:
    -   将`result_node`的梯度初始化为1或单位矩阵。
    -   **逆序**遍历拓扑排序列表。对于每个节点：
        -   此时能确保它已从所有下游子节点接收并累加了全部梯度。
        -   利用总梯度，计算对各父节点的梯度贡献。
        -   将梯度贡献传递给父节点进行累加。

### 3.3. Graph结构体变更

-   移除 `last_forward_pass_id` 和 `last_backward_pass_id` 字段。
-   `forward_node` 和 `backward_nodes` 方法的实现将完全重写，以采用拓扑排序和版本控制的逻辑。

## 4. 新架构的优势

-   **高效的局部更新**: 当图中只有少量输入变化时，只有受影响的“脏”路径会被重新计算，极大提升了效率。
-   **天然支持高级功能**:
    -   **部分前向传播**: 只需更新关心的输入，然后请求关心的输出，计算会自动且最小化地在相关子图上发生。
    -   **`detach`**: 在构建反向传播的拓扑序时，只需在被`detach`的节点处停止回溯即可，实现非常简单自然。
-   **代码解耦与可维护性**: 节点的职责被简化为“如何计算”和“如何求导”。而“何时计算”的复杂调度逻辑被统一移至`Graph`执行器中，使得代码结构更清晰，更易于维护和扩展。

这是一个具有长远价值的架构升级，将为项目未来的发展奠定坚实的基础。
