//! # California Housing æˆ¿ä»·å›å½’ç¤ºä¾‹ï¼ˆPyTorch é£æ ¼ï¼‰
//!
//! å±•ç¤º MSE æŸå¤±åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å›å½’ä»»åŠ¡ï¼š
//! - ä½¿ç”¨ California Housing æ•°æ®é›†
//! - ä½¿ç”¨ Linear å±‚ + Softplus æ¿€æ´»
//! - ä½¿ç”¨ `MseLoss`ï¼ˆ`PyTorch` é£æ ¼ï¼‰
//! - ä½¿ç”¨ `DataLoader` æ‰¹å¤„ç†
//!
//! ## è¿è¡Œ
//! ```bash
//! cargo run --example california_housing
//! ```
//!
//! ## ç›®æ ‡
//! RÂ² â‰¥ 70%ï¼ˆè§£é‡Š 70% çš„ç›®æ ‡å˜é‡æ–¹å·®ï¼‰

mod model;

use model::CaliforniaHousingMLP;
use only_torch::data::{CaliforniaHousingDataset, DataLoader, TensorDataset};
use only_torch::nn::{Adam, Graph, GraphError, Module, MseLoss, Optimizer};
use only_torch::tensor::Tensor;
use std::time::Instant;

fn main() -> Result<(), GraphError> {
    println!("=== California Housing æˆ¿ä»·å›å½’ï¼ˆPyTorch é£æ ¼ï¼‰===\n");

    let start_time = Instant::now();

    // ========== 1. åŠ è½½æ•°æ® ==========
    println!("[1/4] åŠ è½½æ•°æ®é›†...");

    let dataset = CaliforniaHousingDataset::load_default()
        .expect("åŠ è½½ California Housing æ•°æ®é›†å¤±è´¥")
        .standardize();

    let (train_data, test_data) = dataset
        .train_test_split(0.2, Some(42))
        .expect("åˆ’åˆ†æ•°æ®é›†å¤±è´¥");

    println!(
        "  è®­ç»ƒé›†: {} æ ·æœ¬, æµ‹è¯•é›†: {} æ ·æœ¬",
        train_data.len(),
        test_data.len()
    );

    // è½¬æ¢ä¸º TensorDataset
    let (train_x, train_y) = to_tensor_dataset(&train_data);
    let (test_x, test_y) = to_tensor_dataset(&test_data);

    let train_dataset = TensorDataset::new(train_x, train_y);
    let test_dataset = TensorDataset::new(test_x, test_y);

    // ========== 2. è®­ç»ƒé…ç½® ==========
    let batch_size = 256;
    let max_epochs = 30;
    let learning_rate = 0.01;
    let target_r2 = 0.70;

    println!("\n[2/4] è®­ç»ƒé…ç½®:");
    println!("  Batch Size: {batch_size}");
    println!("  Max Epochs: {max_epochs}");
    println!("  å­¦ä¹ ç‡: {learning_rate}");
    println!("  ç›®æ ‡ RÂ²: {:.0}%", target_r2 * 100.0);

    // ========== 3. æ„å»ºæ¨¡å‹ ==========
    println!("\n[3/4] æ„å»ºæ¨¡å‹: 8 -> 128 -> 64 -> 32 -> 1");

    let graph = Graph::new_with_seed(42);
    let model = CaliforniaHousingMLP::new(&graph)?;

    println!("  å‚æ•°æ•°é‡: {} ä¸ª Var", model.parameters().len());

    // æŸå¤±å‡½æ•°ï¼ˆPyTorch é£æ ¼ï¼‰
    let criterion = MseLoss::new();

    // ä¼˜åŒ–å™¨
    let mut optimizer = Adam::new(&graph, &model.parameters(), learning_rate);

    // DataLoader
    let train_loader = DataLoader::new(train_dataset, batch_size)
        .shuffle(true)
        .drop_last(true);

    let test_loader = DataLoader::new(test_dataset, batch_size)
        .shuffle(false)
        .drop_last(true);

    // ========== 4. è®­ç»ƒå¾ªç¯ ==========
    println!("\n[4/4] å¼€å§‹è®­ç»ƒ...\n");

    let mut best_r2 = f32::NEG_INFINITY;

    for epoch in 0..max_epochs {
        let epoch_start = Instant::now();
        let mut epoch_loss_sum = 0.0;
        let mut batch_count = 0;

        // è®­ç»ƒ
        for (x_batch, y_batch) in train_loader.iter() {
            let output = model.forward(&x_batch)?;
            let loss = criterion.forward(&output, &y_batch)?;

            optimizer.zero_grad()?;
            let loss_val = loss.backward()?;
            optimizer.step()?;

            epoch_loss_sum += loss_val;
            batch_count += 1;
        }

        let epoch_avg_loss = epoch_loss_sum / batch_count as f32;

        // æµ‹è¯•é›†è¯„ä¼°ï¼ˆè®¡ç®— RÂ²ï¼‰
        let r2_score = evaluate_r2(&model, &test_loader)?;
        best_r2 = best_r2.max(r2_score);

        println!(
            "Epoch {:2}/{}: loss = {:.4}, RÂ² = {:.2}% ({:.4}), è€—æ—¶ {:.2}s",
            epoch + 1,
            max_epochs,
            epoch_avg_loss,
            r2_score * 100.0,
            r2_score,
            epoch_start.elapsed().as_secs_f32()
        );

        // æå‰ç»“æŸ
        if r2_score >= target_r2 {
            println!("\nğŸ‰ è¾¾åˆ°ç›®æ ‡ RÂ² â‰¥ {:.0}%ï¼", target_r2 * 100.0);
            break;
        }
    }

    // ========== ä¿å­˜å¯è§†åŒ– ==========
    let vis_result =
        graph.save_visualization("examples/california_housing/california_housing", None)?;
    println!("\nè®¡ç®—å›¾å·²ä¿å­˜: {}", vis_result.dot_path.display());
    if let Some(img_path) = &vis_result.image_path {
        println!("å¯è§†åŒ–å›¾åƒ: {}", img_path.display());
    }

    // ========== ç»“æœæ€»ç»“ ==========
    let total_duration = start_time.elapsed();
    println!("\næ€»è€—æ—¶: {:.2}s", total_duration.as_secs_f32());
    println!("æœ€ä½³ RÂ²: {:.4} ({:.1}%)", best_r2, best_r2 * 100.0);

    if best_r2 >= target_r2 {
        println!("\nâœ… California Housing å›å½’æˆåŠŸï¼");
        println!("   æ¨¡å‹è§£é‡Šäº† {:.1}% çš„ç›®æ ‡å˜é‡æ–¹å·®", best_r2 * 100.0);
        Ok(())
    } else {
        println!("\nâŒ æœªè¾¾åˆ°ç›®æ ‡ RÂ² (å®é™…: {:.1}%)", best_r2 * 100.0);
        Err(GraphError::ComputationError(format!(
            "RÂ² åˆ†æ•° {:.2}% < ç›®æ ‡ {:.0}%",
            best_r2 * 100.0,
            target_r2 * 100.0
        )))
    }
}

/// å°† `CaliforniaHousingDataset` è½¬æ¢ä¸º (Tensor, Tensor)
fn to_tensor_dataset(data: &CaliforniaHousingDataset) -> (Tensor, Tensor) {
    let n = data.len();
    let mut x_data = Vec::with_capacity(n * 8);
    let mut y_data = Vec::with_capacity(n);

    for i in 0..n {
        let (features, target) = data.get(i).unwrap();
        x_data.extend(features.flatten_view().iter().copied());
        y_data.push(target[[0]]);
    }

    (Tensor::new(&x_data, &[n, 8]), Tensor::new(&y_data, &[n, 1]))
}

/// åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®— RÂ² åˆ†æ•°
fn evaluate_r2(model: &CaliforniaHousingMLP, loader: &DataLoader) -> Result<f32, GraphError> {
    let mut predictions = Vec::new();
    let mut actuals = Vec::new();

    for (x_batch, y_batch) in loader.iter() {
        let output = model.forward(&x_batch)?;
        let pred = output.value()?.unwrap();

        for i in 0..pred.shape()[0] {
            predictions.push(pred[[i, 0]]);
            actuals.push(y_batch[[i, 0]]);
        }
    }

    Ok(compute_r2(&predictions, &actuals))
}

/// è®¡ç®— RÂ² åˆ†æ•°
fn compute_r2(predictions: &[f32], actuals: &[f32]) -> f32 {
    let mean_actual: f32 = actuals.iter().sum::<f32>() / actuals.len() as f32;

    let ss_res: f32 = predictions
        .iter()
        .zip(actuals.iter())
        .map(|(pred, actual)| (actual - pred).powi(2))
        .sum();

    let ss_tot: f32 = actuals
        .iter()
        .map(|actual| (actual - mean_actual).powi(2))
        .sum();

    1.0 - (ss_res / ss_tot)
}
